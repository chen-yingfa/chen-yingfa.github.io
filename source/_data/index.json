{"meta":{"version":1,"warehouse":"4.0.2"},"models":{"Asset":[{"_id":"themes/fengye/source/favicon-32x32.png","path":"favicon-32x32.png","modified":0,"renderable":1},{"_id":"themes/fengye/source/favicon.ico","path":"favicon.ico","modified":0,"renderable":1},{"_id":"themes/fengye/source/css/highlight.styl","path":"css/highlight.styl","modified":0,"renderable":1},{"_id":"themes/fengye/source/css/search.styl","path":"css/search.styl","modified":0,"renderable":1},{"_id":"themes/fengye/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/fengye/source/js/main.js","path":"js/main.js","modified":0,"renderable":1},{"_id":"themes/fengye/source/js/search.js","path":"js/search.js","modified":0,"renderable":1},{"_id":"themes/fengye/source/lib/clipboard.min.js","path":"lib/clipboard.min.js","modified":0,"renderable":1},{"_id":"themes/fengye/source/lib/iconify-icon.min.js","path":"lib/iconify-icon.min.js","modified":0,"renderable":1},{"_id":"themes/fengye/source/lib/jquery.min.js","path":"lib/jquery.min.js","modified":0,"renderable":1},{"_id":"themes/fengye/source/images/Fengye.png","path":"images/Fengye.png","modified":0,"renderable":1},{"_id":"themes/fengye/source/images/logo.svg","path":"images/logo.svg","modified":0,"renderable":1},{"_id":"themes/fengye/source/lib/fancybox/fancybox.min.css","path":"lib/fancybox/fancybox.min.css","modified":0,"renderable":1},{"_id":"themes/fengye/source/lib/fancybox/fancybox.umd.min.js","path":"lib/fancybox/fancybox.umd.min.js","modified":0,"renderable":1},{"_id":"themes/fengye/source/lib/tocbot/tocbot.min.css","path":"lib/tocbot/tocbot.min.css","modified":0,"renderable":1},{"_id":"themes/fengye/source/lib/tocbot/tocbot.min.js","path":"lib/tocbot/tocbot.min.js","modified":0,"renderable":1},{"_id":"source/images/favicon-32x32.png","path":"images/favicon-32x32.png","modified":0,"renderable":0},{"_id":"source/images/favicon.ico","path":"images/favicon.ico","modified":0,"renderable":0},{"_id":"source/images/portrait.png","path":"images/portrait.png","modified":0,"renderable":0},{"_id":"source/images/portrait.jpg","path":"images/portrait.jpg","modified":0,"renderable":0},{"_id":"source/pdf/cv.pdf","path":"pdf/cv.pdf","modified":0,"renderable":0}],"Cache":[{"_id":"source/.DS_Store","hash":"3f0c25e5b4975cb37e74b893f0af73abb89883db","modified":1706814084843},{"_id":"source/about.md","hash":"d26e3110cf867bfb287e64f88b3073513a8ee8f6","modified":1696064779412},{"_id":"source/publications.md","hash":"487fd6a28ef51b6e7797d508fbfeae6d915fdee6","modified":1704945062707},{"_id":"source/indexx.md","hash":"e7a89c4563ef74f1b1c9439db3e82c4abe6468ff","modified":1695454296827},{"_id":"source/projects.md","hash":"d529b2396a9daa08a77068d2bbb8b2edc9089196","modified":1695454114813},{"_id":"source/_posts/2023中秋.md","hash":"76c05e3a7a67a3d2927d3a524366fc7104aebd66","modified":1704951873394},{"_id":"source/_posts/actadd.md","hash":"788d9eb175022a2d888a911e5afd36d44ecc31d1","modified":1704951722734},{"_id":"source/_posts/.DS_Store","hash":"1ff83699470dc1649bdaf0815bb4f4256acf5fdd","modified":1709464743338},{"_id":"source/_posts/infinitebench.md","hash":"2f887f8a28cd3dda186387f287278b71677358ba","modified":1708916824673},{"_id":"source/_posts/some_binary_search.md","hash":"bd37ac2fa1894f5e306b0b32f1319d01cc5efaad","modified":1704949168107},{"_id":"source/_posts/CFDBench.md","hash":"635cbc1a9ba5c73579d153764e1d6964d4cf6e1d","modified":1704944740727},{"_id":"source/_posts/临近2023暑假.md","hash":"7be9d2213d8cea4982502db7f8d883866019e675","modified":1705901904811},{"_id":"source/_posts/第一个帖子，瞎写点东西.md","hash":"da89f515329cdebf2bbcf5af703130a595b547a7","modified":1708916181456},{"_id":"source/_posts/更新个人主页.md","hash":"1324f3678164635831cc6b148ce9232c05d26ef4","modified":1706009572626},{"_id":"source/friends/index.md","hash":"640ab2bae07bedc4c163f679a746f7ab7fb5d1fa","modified":1694796577338},{"_id":"source/_posts/llm-safety-and-ethics.md","hash":"7b8071ecca76b5473a3144cba5a513dc584eae6e","modified":1704871496515},{"_id":"source/_drafts/EREN.md","hash":"a90bfd52bb6bc40244623575fd8de871162b217d","modified":1706008157490},{"_id":"source/_posts/第一篇.md","hash":"87d255ec553fa1f4a38ec9f9e3174efaf93550f4","modified":1704871496516},{"_id":"source/_posts/interpreting-a-maze-solving-network.md","hash":"af6564de1ec2bf91229a46e9c1ccf833c0a87e28","modified":1704951709088},{"_id":"source/_drafts/语言模型的进化.md","hash":"074cc63ea66033f9d28fb9a509cb886165679b3c","modified":1708915998088},{"_id":"source/home/index.md","hash":"76e8103ebc508f55471a9676e8b22fb52f12d03c","modified":1694851730129},{"_id":"source/categories/index.md","hash":"4c4f89189728ad20934104d3d598fc3414bf10cc","modified":1710046158292},{"_id":"source/images/favicon.ico","hash":"8ede8a7e8edefbd78bb978d781a502f696c32be9","modified":1694861604941},{"_id":"source/images/favicon-32x32.png","hash":"ebaaa72b28fc97b0e4c419837ac1f2c54110d070","modified":1695511244000},{"_id":"source/index/index.md","hash":"263e77b32aef07196b099dccb348d39e217fe3ad","modified":1706008323884},{"_id":"source/tags/index.md","hash":"3ecc8ff83978866b006b59fb6b8d1977e45ae09e","modified":1710046123903},{"_id":"source/_drafts/语言模型的进化/rnn.png","hash":"aac5c34e5dd58a5aef3986971fbdb86995e55fc8","modified":1706807686007},{"_id":"source/_posts/actadd/alg.png","hash":"dc702db52c81311391f17e05666a5a5c9e082ed2","modified":1696759929479},{"_id":"source/pdf/cv.pdf","hash":"84479a0480b08fc28ed34e856c8a8dcad815dbc4","modified":1693635547998},{"_id":"source/_drafts/语言模型的进化/self-attention.png","hash":"077705a17f32431f3ebe5191671ca05b7bc656e3","modified":1706809422310},{"_id":"themes/fengye/package.json","hash":"2fd1608c7164b35dfbc325e9d3a1293f92ea672d","modified":1694946750845},{"_id":"themes/fengye/.gitignore","hash":"2a191a03434ba584ea5e70d36ab4eb2a1b6324da","modified":1694839598097},{"_id":"themes/fengye/.editorconfig","hash":"b16f01a7b04ad512e15ebb32c5786d432a536779","modified":1694798100057},{"_id":"themes/fengye/LICENSE","hash":"632b916dd7e4f5c11790ab808388cda6610210ed","modified":1694798100058},{"_id":"themes/fengye/languages/default.yml","hash":"da38f00bb45a318f118db0d74df24a137351777e","modified":1694798100058},{"_id":"themes/fengye/.DS_Store","hash":"ee7a7465a1b0a4a13207339a23f29012f8b7b5b4","modified":1705901982879},{"_id":"themes/fengye/_config.yml","hash":"fff2b123cd2d603014193592dcc1598e8f83c7bb","modified":1710051530939},{"_id":"themes/fengye/languages/en.yml","hash":"72066419d6682a017c97910921ade125f21b04cd","modified":1694798100058},{"_id":"themes/fengye/languages/ja.yml","hash":"3e2fedca096678c0c234ebffa4637828979296fa","modified":1694798100059},{"_id":"themes/fengye/scripts/echarts.js","hash":"2c5a1439a12b3c3ab3fd51b3572748a0ccd8667a","modified":1694798100064},{"_id":"themes/fengye/scripts/mermaid.js","hash":"778f3daf90edab9bd5c007cfb131e69e3e65709f","modified":1694798100064},{"_id":"themes/fengye/scripts/wordcount.js","hash":"d1c45c18bc63c144f02f3400c21b179534ae99ce","modified":1694798100064},{"_id":"themes/fengye/package-lock.json","hash":"f963312fc6344c02b731e735fd14818a813ce98b","modified":1694946891794},{"_id":"themes/fengye/languages/ko.yml","hash":"11330316e3c1262474a2b496e40dbc29f93fe01b","modified":1694798100059},{"_id":"themes/fengye/README.md","hash":"f2aec2b29e9844daa7d7f9c6a282ed605734e209","modified":1709517996130},{"_id":"themes/fengye/languages/no.yml","hash":"182bd9ea76313ec9dc769b5dd2845c0d1c56e3a0","modified":1694947626048},{"_id":"themes/fengye/source/favicon-32x32.png","hash":"ebaaa72b28fc97b0e4c419837ac1f2c54110d070","modified":1695511244000},{"_id":"themes/fengye/languages/de.yml","hash":"d45cea36c5c83d7d09afcd1c26fff4a4c513c25b","modified":1694798100058},{"_id":"themes/fengye/source/favicon.ico","hash":"8ede8a7e8edefbd78bb978d781a502f696c32be9","modified":1694798100065},{"_id":"themes/fengye/layout/archive.ejs","hash":"bed6fea1a69ef1eb13b8d4e6616adab7282a7ebf","modified":1704874448957},{"_id":"themes/fengye/source/.DS_Store","hash":"43be40210f45a8cca5810daf05ae8d117010cfc1","modified":1705901982878},{"_id":"themes/fengye/layout/categories.ejs","hash":"50c3fc1c0fe4a619f4504722a5ae693c482ded7d","modified":1695974049298},{"_id":"themes/fengye/layout/layout.ejs","hash":"6bcddd59ee27ad6b8f94d54a08e80903627c885e","modified":1696507318605},{"_id":"themes/fengye/languages/zh-CN.yml","hash":"e41d1e0e3a9e15c30b7142491bed39dc50371e96","modified":1694947135799},{"_id":"themes/fengye/layout/tags.ejs","hash":"f4cfea6489b5c25253cb80df3de8603591399f02","modified":1704951509685},{"_id":"themes/fengye/source/css/highlight.styl","hash":"92aa8c7a03febcebe91e58cabdda94086407a4a3","modified":1695633123647},{"_id":"themes/fengye/layout/index.ejs","hash":"0c3a330a3b3bb90f7f0ea161c78d097d5f199a69","modified":1709518106495},{"_id":"themes/fengye/layout/post.ejs","hash":"cde0234b96a98b0af0d067f518dbce18639ef84b","modified":1704950797619},{"_id":"themes/fengye/layout/.DS_Store","hash":"503f8df87987f2567c18b293ef8add5a56e23abb","modified":1705901977825},{"_id":"themes/fengye/source/js/main.js","hash":"954fb2295daaa1d918d4afe13163f4282567b624","modified":1695998347849},{"_id":"themes/fengye/source/css/search.styl","hash":"c2707e9c4ab6ea6a2f0c5445639ed6debabd4fe3","modified":1698947373651},{"_id":"themes/fengye/source/lib/clipboard.min.js","hash":"91d8fe48e42d7d985918d4f244f6e7e3cc5adc2d","modified":1694798100082},{"_id":"themes/fengye/source/lib/iconify-icon.min.js","hash":"014e6de8308a10051823d39ba49834ff18e4dba7","modified":1694798100083},{"_id":"themes/fengye/source/css/main.styl","hash":"a111232eb79526f8a3321a681920d596ae59b129","modified":1710046986661},{"_id":"themes/fengye/layout/_partial/after-footer.ejs","hash":"e0441196b4db18962ec563128de25fcf6b2547ac","modified":1694947748330},{"_id":"themes/fengye/layout/_partial/head.ejs","hash":"d798e3af9099197a2e8080034defa2e152a406cb","modified":1704860574098},{"_id":"themes/fengye/source/js/search.js","hash":"9b14f8e4a76d8da293d538d73d340dfd5b5e5402","modified":1710051538026},{"_id":"themes/fengye/source/images/Fengye.png","hash":"bd5ebaf9ffd792aa83701e7701ad5130e1b4df4c","modified":1709517996137},{"_id":"themes/fengye/layout/_partial/header.ejs","hash":"65fb66831696f323f7cf6a8c28078c6094c80f58","modified":1710051584458},{"_id":"themes/fengye/source/images/logo.svg","hash":"1b7a73d948e593dcec7549d63e5ac60ad6db6a8f","modified":1694798100073},{"_id":"themes/fengye/layout/_partial/post-list-item.ejs","hash":"d5bb4e6c0c736e09612c8b581320952dd780e8a5","modified":1709517996133},{"_id":"themes/fengye/layout/_partial/paginator.ejs","hash":"76f929c87797cb6b81c1003846001915c98ed336","modified":1709517996132},{"_id":"themes/fengye/layout/_partial/tag-list.ejs","hash":"3e5ad39f0866e7eab2768a93a9778ba418c9d741","modified":1709517996135},{"_id":"themes/fengye/layout/_partial/toc.ejs","hash":"b2d43f20570cac0c58e3b0578791ed79ab3321a1","modified":1704944938535},{"_id":"themes/fengye/layout/_plugins/baidu-analytics.ejs","hash":"5d651a50ab521334b3cc0bff34ce3de3e76ac750","modified":1694798100061},{"_id":"themes/fengye/layout/_partial/post-list.ejs","hash":"1303877ae7ad5ba759694385e079d427b3b3c594","modified":1709517996134},{"_id":"themes/fengye/layout/_partial/footer.ejs","hash":"f07cae693f3b78401168edd73db30e96ca9eb35a","modified":1704949293627},{"_id":"themes/fengye/layout/_partial/social-list.ejs","hash":"71bb8e8ad61c4ad836ab335aa24e67731a6b162e","modified":1696074432629},{"_id":"themes/fengye/layout/_plugins/disqusjs.ejs","hash":"5f9b22b65eb828e3af178379c3d5be56d93a7aaa","modified":1694798100061},{"_id":"themes/fengye/layout/_plugins/giscus.ejs","hash":"d6f97ceb3ccfcedd61ac7890c5ca46f252f73802","modified":1694798100061},{"_id":"themes/fengye/layout/_plugins/google-analytics.ejs","hash":"a8a6e445d6016bfd8ae657d33c213ec828612184","modified":1694798100061},{"_id":"themes/fengye/layout/_plugins/mathjax.ejs","hash":"c358356a7595b5e1fe1d65aee10616eec8eb00e7","modified":1694798100062},{"_id":"themes/fengye/layout/_plugins/mermaid.ejs","hash":"804c36256f9232a1fe6f308e06ae89642fe0eb1b","modified":1694798100062},{"_id":"themes/fengye/layout/_plugins/fancybox.ejs","hash":"d2d0858a34f550c80d00fb705f94e08b33150eab","modified":1704865863839},{"_id":"themes/fengye/layout/_plugins/busuanzi.ejs","hash":"7bd277fb195c6a499e36f4870a80fd7073c61052","modified":1695467772525},{"_id":"themes/fengye/source/lib/fancybox/fancybox.min.css","hash":"faa1beb3cde9b3abf714bf1b8410eb71199e798d","modified":1694798100082},{"_id":"themes/fengye/layout/_plugins/tocbot.ejs","hash":"e17e921d40c71405a37ee3c782b7584f8b339653","modified":1694973231694},{"_id":"themes/fengye/layout/_plugins/tailwindcss.ejs","hash":"3bc366434255a72d0447c88af03abc1c34c128a7","modified":1695461959702},{"_id":"themes/fengye/layout/_partial/search.ejs","hash":"cdc5676141e436a61c279213d412f1f37cb48e8a","modified":1710051320561},{"_id":"themes/fengye/layout/_plugins/theme.ejs","hash":"af6f16cf670f50731bc89a31f9f208a65776ccf4","modified":1695019010493},{"_id":"themes/fengye/source/lib/tocbot/tocbot.min.js","hash":"30be41afb1f21ae821a143d74beab47094bc87a3","modified":1694798100085},{"_id":"themes/fengye/source/lib/tocbot/tocbot.min.css","hash":"fb0f0f1d2ceff6621f4e20191a103330c03ea2f6","modified":1695547118134},{"_id":"source/_posts/第一个帖子，瞎写点东西/lillesand0.jpg","hash":"ea04c507fe1c7859b5e2c13265649e7ee5e10ca9","modified":1704866075031},{"_id":"source/_posts/第一个帖子，瞎写点东西/lillesand1.jpg","hash":"208e8d0ed9178a7935a930082b48e0a7371e5f62","modified":1704866075036},{"_id":"source/_posts/actadd/method.png","hash":"a8b682bfe5ab4dbe09b14b083055f6690dd032c1","modified":1696674147963},{"_id":"source/_posts/infinitebench/data-stat-pie.png","hash":"3384e825a982ec9038295ef68d0856b1b04a1d07","modified":1704855164330},{"_id":"source/_posts/actadd/result.png","hash":"9e94dccb92d8f3d25345b515322d23cf9ba24def","modified":1696814224211},{"_id":"themes/fengye/source/lib/fancybox/fancybox.umd.min.js","hash":"91f17b14286c64506ed5e214d7e31209d34940c0","modified":1694798100083},{"_id":"themes/fengye/source/lib/jquery.min.js","hash":"f694238d616f579a0690001f37984af430c19963","modified":1694798100084},{"_id":"source/_posts/infinitebench/results.png","hash":"5b36c793d61b0c070f426e02db6223526d51e102","modified":1704856068843},{"_id":"source/_posts/更新个人主页/中国护照.jpg","hash":"ff10a197a19b140980e804180b0503216ef1d863","modified":1694882171768},{"_id":"source/images/portrait.jpg","hash":"c910db58974a63b1c7d8d256b99b0c6ee4901a50","modified":1695527695106},{"_id":"source/_posts/更新个人主页/申请签证.jpg","hash":"7df3acbc98aa10dca03efdb285fb40f2d4525f52","modified":1694882064635},{"_id":"source/_posts/更新个人主页/吃澳门菜.jpg","hash":"92855a153bb0c0644b327b55f433ebe661a78fa2","modified":1694882171854},{"_id":"source/images/portrait.png","hash":"049faf25dabe7878afe4181dc7355845504ce90c","modified":1695528095993},{"_id":"source/_posts/2023中秋/武汉欢乐谷.png","hash":"977e23f8ad8150689b0b34f3df1a9bbed2909460","modified":1704865384673},{"_id":"source/_posts/2023中秋/武商梦时代.png","hash":"9dec428cc6c918f9eaf6d47dd1a36f797678f72b","modified":1704865384690},{"_id":"source/_posts/2023中秋/新天地-霸王茶姬.png","hash":"cd9609e61378cea89811ef3d1cf348ec59822036","modified":1704865384714},{"_id":"source/_posts/2023中秋/解放公园中间.png","hash":"359aaf1437e94f494e6dd419ff6b7001a6e0b362","modified":1704865384660},{"_id":"public/search.xml","hash":"e4fc0f4b011b1b7393704d9dc4b64aa84d178bc3","modified":1710051804435},{"_id":"public/about.html","hash":"4f7e7faf107fd8bbfeae0c71a0572ea164aaadf2","modified":1710051856894},{"_id":"public/friends/index.html","hash":"9d8d752e00a84fee6e0974eab9ce149e414a0d68","modified":1710051856894},{"_id":"public/publications.html","hash":"92f402b3724a13271603c7cc0b00cec70a6cb5c3","modified":1710051856894},{"_id":"public/categories/index.html","hash":"a4f720f1fd8b504911de5efc466d7246b8a20355","modified":1710051856894},{"_id":"public/home/index.html","hash":"c63a8f71c4dc64b399af7083675cdd306e4e7f16","modified":1710051856894},{"_id":"public/index/index.html","hash":"9ee9caf3e61998eda92f8fd8805806c750cae13c","modified":1710051856894},{"_id":"public/tags/index.html","hash":"d52260dd9b2d630e88977d8d9982b186074353f6","modified":1710051856894},{"_id":"public/projects.html","hash":"ac5ed097c420487468276d27c11625c9f8d6fc8a","modified":1710051856894},{"_id":"public/2024/01/10/infinitebench/index.html","hash":"f6660316052b5a6693e6211dae548f16eb2f48ab","modified":1710051856894},{"_id":"public/2023/10/07/interpreting-a-maze-solving-network/index.html","hash":"f1a724cb535e0987c5cd811a64c494458dc08f01","modified":1710051856894},{"_id":"public/indexx.html","hash":"2af3d7b2b051b65d09148cd7d930ae0a99c178e4","modified":1710051856894},{"_id":"public/2023/10/07/actadd/index.html","hash":"3318217ed50fefb837eb298ef382d26aafb889e8","modified":1710051856894},{"_id":"public/2023/10/05/2023中秋/index.html","hash":"8d77714c554757185d7c6cc820c57aa02a6059ae","modified":1710051856894},{"_id":"public/2023/09/19/llm-safety-and-ethics/index.html","hash":"d9a5133875fb383a7d4ab3dc4b1a820f419deb7b","modified":1710051856894},{"_id":"public/2023/09/16/更新个人主页/index.html","hash":"961fa5601a59e744f8ac4003b51a74f28c5614c2","modified":1710051856894},{"_id":"public/2023/09/16/CFDBench/index.html","hash":"c7a663f183b0abb1a2b6197c5437387ca051a810","modified":1710051856894},{"_id":"public/2023/09/14/some_binary_search/index.html","hash":"904509f9513a9ee8d2e18638a73355d53e0f8a21","modified":1710051856894},{"_id":"public/2023/05/18/临近2023暑假/index.html","hash":"81fc53bce31072fffd14c5b3375bd02c57462616","modified":1710051856894},{"_id":"public/2023/05/17/第一个帖子，瞎写点东西/index.html","hash":"85c47bafdb0114f27a8c0b36cc1c5f402e5bf43b","modified":1710051856894},{"_id":"public/2022/10/27/第一篇/index.html","hash":"16b2c1e0300f87b8c4366aa11ade3adf45e645aa","modified":1710051856894},{"_id":"public/page/2/index.html","hash":"15287dccb41674bdb19eab5561e37e0d77e0fe35","modified":1710051856894},{"_id":"public/index.html","hash":"15287dccb41674bdb19eab5561e37e0d77e0fe35","modified":1710051856894},{"_id":"public/archives/index.html","hash":"c2a012d2d68375cbe5318b78008ae62f374221f0","modified":1710051856894},{"_id":"public/archives/2022/index.html","hash":"596ff995968927346bada90207050a88f2b02e5f","modified":1710051856894},{"_id":"public/archives/page/2/index.html","hash":"ee8219d248de8d817366f742b4b6d481fda95811","modified":1710051856894},{"_id":"public/archives/2022/10/index.html","hash":"74550b9c1603dea4004fd5050ee4c6af63404451","modified":1710051856894},{"_id":"public/archives/2023/index.html","hash":"9ae1dacbcd27b0b95ba4629086d282da3065bdb7","modified":1710051856894},{"_id":"public/archives/2023/05/index.html","hash":"b0b490e1fefdca4cf8b02ce634068a00914899f3","modified":1710051856894},{"_id":"public/archives/2023/09/index.html","hash":"5eafc4f5665ebf80837777725d308b46de9bdd5d","modified":1710051856894},{"_id":"public/archives/2023/10/index.html","hash":"b38108df1d7a7335110a004eb970e13079a43b3a","modified":1710051856894},{"_id":"public/archives/2024/01/index.html","hash":"75a9991ae85dd7bf95296c4ff5ae53e971c10b9c","modified":1710051856894},{"_id":"public/archives/2024/index.html","hash":"3426471d17335599696175a7120eeaa67ce0fec6","modified":1710051856894},{"_id":"public/tags/life/index.html","hash":"be78e0cee91fdecea094e5bedd4581e830d68d8c","modified":1710051856894},{"_id":"public/tags/中秋/index.html","hash":"fcabb1a9d8f03f13d6863b85681d1506623cfd3f","modified":1710051856894},{"_id":"public/tags/中文/index.html","hash":"934cb35fcd4e662d258f51b1ce7cf34b8e4fbdf7","modified":1710051856894},{"_id":"public/tags/00/index.html","hash":"18c45825a985dbae33baf4248f8092f05f98923a","modified":1710051856894},{"_id":"public/tags/wedding/index.html","hash":"0d9085fefece08857987477ef3f0480a41d59ee1","modified":1710051856894},{"_id":"public/tags/中秋-middle-autumn/index.html","hash":"3ec8d00f2c80893b24ffe9977f8fffef55b05aab","modified":1710051856894},{"_id":"public/tags/国庆-national-day/index.html","hash":"35d17e097cba6486970ed6f09709e2eb0427c656","modified":1710051856894},{"_id":"public/tags/应城/index.html","hash":"528b6c51de3080d79db21cefb0a6249d99da21c2","modified":1710051856894},{"_id":"public/tags/武汉-wuhan/index.html","hash":"da00365d197dffd34df549612ce9fd144d88cd73","modified":1710051856894},{"_id":"public/tags/paper/index.html","hash":"a5a7a39215e0ea466f330142a1b3bb54738bba57","modified":1710051856894},{"_id":"public/tags/research/index.html","hash":"2d0dcaff812d30c8f4413625f62c17a712a83c82","modified":1710051856894},{"_id":"public/tags/cfd/index.html","hash":"eaaafe8d30c0cd4ba8b25f64a8efdc3a145c9c6b","modified":1710051856894},{"_id":"public/tags/dataset/index.html","hash":"978f0fa353f6767a08c87ae252390222376c343f","modified":1710051856894},{"_id":"public/tags/english/index.html","hash":"74352b307f1f1f84a1ac03f0fce64b13af26fb6d","modified":1710051856894},{"_id":"public/tags/pinn/index.html","hash":"a19b9a7b950df43ff7f749002ab832faca1f48fa","modified":1710051856894},{"_id":"public/tags/fno/index.html","hash":"b9080fffb029e096a604c540a2ce1f419d16ebaa","modified":1710051856894},{"_id":"public/tags/physics/index.html","hash":"de37c0e7e6775a026691d30144f60683dd3deb4f","modified":1710051856894},{"_id":"public/tags/machine-learning/index.html","hash":"b17886600722ab132d69cb61439e91c2f3bd7aa4","modified":1710051856894},{"_id":"public/tags/deep-learning/index.html","hash":"0d920eb547256bcdc155d8a33ce27e79ead034a4","modified":1710051856894},{"_id":"public/tags/deeponet/index.html","hash":"cf21511ace0aa467bfb078d2b9c9ad64ede037c2","modified":1710051856894},{"_id":"public/tags/ai4science/index.html","hash":"7cdf6ca72ac7f98f97ebbd6029528076090624dd","modified":1710051856894},{"_id":"public/tags/ai-alignment/index.html","hash":"b6ab247709ae4e8f09d020cca74716e6ddbb88a4","modified":1710051856894},{"_id":"public/tags/llm/index.html","hash":"33a27c5c9d09c32131f499137d400d71a69cef24","modified":1710051856894},{"_id":"public/tags/gpt/index.html","hash":"d032f1a248d74e613273715e2ba250ca8aef925e","modified":1710051856894},{"_id":"public/tags/activation-modification/index.html","hash":"67eab9bdc2abd68ccf4de3c5451fca7e8639dd3e","modified":1710051856894},{"_id":"public/tags/adaptation/index.html","hash":"fa66f7a0bc5a5bc214c10a9d3324247227f153ea","modified":1710051856894},{"_id":"public/tags/model-editing/index.html","hash":"5bfd83430afda698e923b96b3ba6d4cd5fadabd2","modified":1710051856894},{"_id":"public/tags/representation-engineering/index.html","hash":"fd18b488151d157303319b50dd728d76cb778244","modified":1710051856894},{"_id":"public/tags/fine-tuning/index.html","hash":"07fcd0842f37c13ee7e5a21c3c54b5b0cc73be18","modified":1710051856894},{"_id":"public/tags/parameter-efficient-tuning/index.html","hash":"2c34a155cb40bae3b7221d72fb5db87d414fa5ed","modified":1710051856894},{"_id":"public/tags/nlp/index.html","hash":"4a6435478b8058bc64bb19ed46ee7141b0baeceb","modified":1710051856894},{"_id":"public/tags/long-context/index.html","hash":"aa369fba6788a6d1804c93455339555111e7d278","modified":1710051856894},{"_id":"public/tags/benchmark/index.html","hash":"107e732264576d6efbacad9ad4d5cfbf768a57fe","modified":1710051856894},{"_id":"public/tags/recurrence/index.html","hash":"247f2f7e40a42d0cb60c2d9645a79736b7c72d3c","modified":1710051856894},{"_id":"public/tags/linear-attention/index.html","hash":"e0f9dc60c00c21bd45abe6d6188bb059c9d90125","modified":1710051856894},{"_id":"public/tags/transformer/index.html","hash":"cfd0f3e78f0c7d8e84e195fcca9cdc9c9333cffe","modified":1710051856894},{"_id":"public/tags/activation-engineering/index.html","hash":"5d1cc0335b58e149de5fdb05b9ffd53a62c5bca8","modified":1710051856894},{"_id":"public/tags/interpretability/index.html","hash":"eab1689816f59ca28556f6369a423cb5d466fc49","modified":1710051856894},{"_id":"public/tags/rl/index.html","hash":"a56930cd3fd9577b5212a1a4f01388269234c92b","modified":1710051856894},{"_id":"public/tags/alignment/index.html","hash":"1849d54af19722f0d8f252c7b9490346ce4b0d13","modified":1710051856894},{"_id":"public/tags/maze/index.html","hash":"a80b70e5896e5b5ef970cc81871cb6189bb2669a","modified":1710051856894},{"_id":"public/tags/ethics/index.html","hash":"c6595fafdb7a7a5f097e69f46eb999e52c92ac4f","modified":1710051856894},{"_id":"public/tags/safety/index.html","hash":"18b1069943b3ec7e53a7aa71d8380cabb60b935b","modified":1710051856894},{"_id":"public/tags/面壁智能/index.html","hash":"8b4b55ca95193637bdd2b822d8f4083b4c120e42","modified":1710051856894},{"_id":"public/tags/tutorial/index.html","hash":"ee8f58254709b38a99c528d72f13ccf293aae29c","modified":1710051856894},{"_id":"public/tags/eren/index.html","hash":"f569ee0e2dcc9c7d6e9199467c7449dc4fae6c3e","modified":1710051856894},{"_id":"public/tags/chatgpt/index.html","hash":"571673ec151dab7dfa7776854cab70ef89ad221a","modified":1710051856894},{"_id":"public/tags/claude/index.html","hash":"45a61d6096014666a2971db3e8d9f5efd6063b2a","modified":1710051856894},{"_id":"public/tags/三体/index.html","hash":"6aa3c63f76117b9a065d3daa5c043479a05dc856","modified":1710051856894},{"_id":"public/tags/modelbest/index.html","hash":"a0bd1ba12b85fd867bc5ca3b146a78c24fd02b5b","modified":1710051856894},{"_id":"public/tags/binary-search/index.html","hash":"31a713640ff4400624118736979be5b43eba190c","modified":1710051856894},{"_id":"public/tags/rust/index.html","hash":"072e60ed71e94b1c040b96d54a5b3ed01c0971a7","modified":1710051856894},{"_id":"public/tags/algorithm/index.html","hash":"daae322134de513af94b4a706ef75460673ca53b","modified":1710051856894},{"_id":"public/tags/python/index.html","hash":"41409248ae5fe7f99c7328d4f95553d469e64e4c","modified":1710051856894},{"_id":"public/tags/c/index.html","hash":"12f259cdae59a3d1f8255b89f69453ba468bd3b2","modified":1710051856894},{"_id":"public/tags/test/index.html","hash":"4698113d962f4feee6d2a34bf30cec5c4987dffe","modified":1710051856894},{"_id":"public/tags/code/index.html","hash":"30581f6861d6f08a92a6bdd66956873c0fe1d4ca","modified":1710051856894},{"_id":"public/tags/school/index.html","hash":"8c3467255dc8e9c4a8de7d22d5ebc9e20ff16271","modified":1710051856894},{"_id":"public/tags/graduation/index.html","hash":"61c5a262ff1b7fc53f300fc00d4fd644484fd23e","modified":1710051856894},{"_id":"public/tags/blog/index.html","hash":"4816aee3f6b42144b6438f8986d344b9b27619d8","modified":1710051856894},{"_id":"public/tags/签证/index.html","hash":"6e10a7bc2a26227b2ea91d7bd2bc57c9b5ca04da","modified":1710051856894},{"_id":"public/tags/hugo/index.html","hash":"bb23c038867493a0ae751b7a6d96772dfcb72e37","modified":1710051856894},{"_id":"public/tags/jekyll/index.html","hash":"62b5df3857fb516b26f9529c2eff5b2a64c2ec0a","modified":1710051856894},{"_id":"public/tags/hexo/index.html","hash":"2f01eba6ae78d987b35672caca1977f5a81bb6fd","modified":1710051856894},{"_id":"public/tags/static-site-generator/index.html","hash":"8665150fc63932ea9bd7b1eb91c352908e2c9c13","modified":1710051856894},{"_id":"public/tags/羽毛球/index.html","hash":"c9958cbf86f7fb15299dc1acc3b1402bba289095","modified":1710051856894},{"_id":"public/tags/work/index.html","hash":"e141a71429cc6a97597914f5853c6fc42a91edc5","modified":1710051856894},{"_id":"public/tags/枫叶/index.html","hash":"1b8fc50bd24e908f504c5284406aebba4967f963","modified":1710051856894},{"_id":"public/tags/中国/index.html","hash":"73c4f266773ce4a200504e91a0ab5f1f915dc19f","modified":1710051856894},{"_id":"public/tags/挪威/index.html","hash":"bec17c6e394aac8d64bbe132b58a8b59ce43ddc5","modified":1710051856894},{"_id":"public/tags/markdown/index.html","hash":"eea5c0853d086d4d9dea166317188d7aaa65ceb3","modified":1710051856894},{"_id":"public/tags/宿舍/index.html","hash":"fc12ff3dd348b8edfe9dbb9229096abbffd33efd","modified":1710051856894},{"_id":"public/tags/孩子们/index.html","hash":"725ecf2a9f0e115f2403b2e5c977bc227acba80f","modified":1710051856894},{"_id":"public/tags/卧龙/index.html","hash":"8f5c0e21565fa9b7392b9b9417739abc49793955","modified":1710051856894},{"_id":"public/tags/凤雏/index.html","hash":"35ca836c7a244265cbb2c459e754f8f5c93429ac","modified":1710051856894},{"_id":"public/tags/骆雁/index.html","hash":"f8d5ad3dbc0eab55ab812cf1b30e0a8f1070ec0e","modified":1710051856894},{"_id":"public/tags/黄帝/index.html","hash":"fa56e69e1b22eb575e64d3aa840c68962f8ae2c9","modified":1710051856894},{"_id":"public/tags/小绿/index.html","hash":"273ba3bfd35c5f356d103c99589678802addef49","modified":1710051856894},{"_id":"public/tags/猫咪/index.html","hash":"384545e6f2ef66623a427b907355e74d9d2e2754","modified":1710051856894},{"_id":"public/tags/土鸡/index.html","hash":"68c68116e45e0ac01647384daecdee81ec2320c5","modified":1710051856894},{"_id":"public/tags/熊/index.html","hash":"09638a810e2f78afc366d683b9f02b25d56c8205","modified":1710051856894},{"_id":"public/tags/🐻/index.html","hash":"90d257a6d85c8a9d915bca10c602557fe07edb78","modified":1710051856894},{"_id":"public/tags/🐱/index.html","hash":"0a7c8c4014f67318ab94221db70bd8414fa8286d","modified":1710051856894},{"_id":"public/tags/🐇/index.html","hash":"ad235caba371ea929cc8bc127c5035831f1c1196","modified":1710051856894},{"_id":"public/tags/🐰/index.html","hash":"39fd9a2a4b328a9e6491b0b6a017787d68b8764d","modified":1710051856894},{"_id":"public/tags/🐊/index.html","hash":"16df46d5b9e07855e2b07057855c29e41373707c","modified":1710051856894},{"_id":"public/tags/emoren/index.html","hash":"ee2613c9d67a4b2ba499e48cf303877681472000","modified":1710051856894},{"_id":"public/tags/acl/index.html","hash":"eed2abb677f862a81bbefbca7e102f248f460d1d","modified":1710051856894},{"_id":"public/tags/lillesand/index.html","hash":"e6e1176c27bce87320266c4bbd86ed17df87bde9","modified":1710051856894},{"_id":"public/tags/加拿大/index.html","hash":"ba4b1f5d5e5aa0fbb0c7134dcb0366f0be92e67b","modified":1710051856894},{"_id":"public/tags/html/index.html","hash":"afb5af01d807b0e8d3c01f2bd3eafb05c137d220","modified":1710051856894},{"_id":"public/tags/liquid/index.html","hash":"70e342e4c7e20fecdb1d21ed2c55b10a9355f470","modified":1710051856894},{"_id":"public/categories/life/index.html","hash":"1ead14620c432a6b108674e77e030df9827775b5","modified":1710051856894},{"_id":"public/categories/research/index.html","hash":"c2fda38d93e9b77748d58bdf735f306e9469717e","modified":1710051856894},{"_id":"public/categories/paper-note/index.html","hash":"5e4ec529236f17d994f762e4b58868add0d3069e","modified":1710051856894},{"_id":"public/categories/thoughts/index.html","hash":"9e83c0dee42565b264d7342a834c510baaf304e4","modified":1710051856894},{"_id":"public/categories/test/index.html","hash":"e4b9586539af58c2af15db49ce405964a48f4b69","modified":1710051856894},{"_id":"public/favicon.ico","hash":"8ede8a7e8edefbd78bb978d781a502f696c32be9","modified":1710051804435},{"_id":"public/images/Fengye.png","hash":"bd5ebaf9ffd792aa83701e7701ad5130e1b4df4c","modified":1710051804435},{"_id":"public/images/logo.svg","hash":"1b7a73d948e593dcec7549d63e5ac60ad6db6a8f","modified":1710051804435},{"_id":"public/favicon-32x32.png","hash":"ebaaa72b28fc97b0e4c419837ac1f2c54110d070","modified":1710051804435},{"_id":"public/images/favicon.ico","hash":"8ede8a7e8edefbd78bb978d781a502f696c32be9","modified":1710051804435},{"_id":"public/images/favicon-32x32.png","hash":"ebaaa72b28fc97b0e4c419837ac1f2c54110d070","modified":1710051804435},{"_id":"public/css/search.css","hash":"0efc0555c7c89a83321f0f4795b90df950d356bd","modified":1710051804435},{"_id":"public/pdf/cv.pdf","hash":"84479a0480b08fc28ed34e856c8a8dcad815dbc4","modified":1710051804435},{"_id":"public/2023/10/07/actadd/alg.png","hash":"dc702db52c81311391f17e05666a5a5c9e082ed2","modified":1710051804435},{"_id":"public/css/highlight.css","hash":"2588d7853381a2f0d2dc3edf1b5e9e67951d626f","modified":1710051804435},{"_id":"public/js/search.js","hash":"9b14f8e4a76d8da293d538d73d340dfd5b5e5402","modified":1710051804435},{"_id":"public/js/main.js","hash":"78e3f9e09ff68955215e8f2b4fcfa5d32662f81a","modified":1710051804435},{"_id":"public/css/main.css","hash":"4cdb9c722f98ddfcfb43070378d612da096a920d","modified":1710051804435},{"_id":"public/lib/clipboard.min.js","hash":"f48e9bfeca83e5057cc751e8c44fc07e9d976c06","modified":1710051804435},{"_id":"public/lib/tocbot/tocbot.min.css","hash":"3a2c80c85bdba2b71c604e32b1273c5c387948cc","modified":1710051804435},{"_id":"public/lib/tocbot/tocbot.min.js","hash":"4f1b40a6818fe6e955f2ce7de3b79aec4dcd0a7c","modified":1710051804435},{"_id":"public/lib/fancybox/fancybox.min.css","hash":"1564bb6a6b930a61875610c05001c4f7bfe9939a","modified":1710051804435},{"_id":"public/lib/jquery.min.js","hash":"69bb69e25ca7d5ef0935317584e6153f3fd9a88c","modified":1710051804435},{"_id":"public/lib/iconify-icon.min.js","hash":"7526cf2b54b9e657f377083129cc00c5aa4dc110","modified":1710051804435},{"_id":"public/lib/fancybox/fancybox.umd.min.js","hash":"e766e468e4f017b51a643648f6b4f05187c41d6b","modified":1710051804435},{"_id":"public/2023/10/07/actadd/method.png","hash":"a8b682bfe5ab4dbe09b14b083055f6690dd032c1","modified":1710051804435},{"_id":"public/2023/05/17/第一个帖子，瞎写点东西/lillesand0.jpg","hash":"ea04c507fe1c7859b5e2c13265649e7ee5e10ca9","modified":1710051804435},{"_id":"public/2023/05/17/第一个帖子，瞎写点东西/lillesand1.jpg","hash":"208e8d0ed9178a7935a930082b48e0a7371e5f62","modified":1710051804435},{"_id":"public/2024/01/10/infinitebench/data-stat-pie.png","hash":"3384e825a982ec9038295ef68d0856b1b04a1d07","modified":1710051804435},{"_id":"public/2023/10/07/actadd/result.png","hash":"9e94dccb92d8f3d25345b515322d23cf9ba24def","modified":1710051804435},{"_id":"public/2024/01/10/infinitebench/results.png","hash":"5b36c793d61b0c070f426e02db6223526d51e102","modified":1710051804435},{"_id":"public/2023/09/16/更新个人主页/中国护照.jpg","hash":"ff10a197a19b140980e804180b0503216ef1d863","modified":1710051804435},{"_id":"public/images/portrait.jpg","hash":"c910db58974a63b1c7d8d256b99b0c6ee4901a50","modified":1710051804435},{"_id":"public/2023/09/16/更新个人主页/申请签证.jpg","hash":"7df3acbc98aa10dca03efdb285fb40f2d4525f52","modified":1710051804435},{"_id":"public/2023/09/16/更新个人主页/吃澳门菜.jpg","hash":"92855a153bb0c0644b327b55f433ebe661a78fa2","modified":1710051804435},{"_id":"public/images/portrait.png","hash":"049faf25dabe7878afe4181dc7355845504ce90c","modified":1710051804435},{"_id":"public/2023/10/05/2023中秋/武汉欢乐谷.png","hash":"977e23f8ad8150689b0b34f3df1a9bbed2909460","modified":1710051804435},{"_id":"public/2023/10/05/2023中秋/武商梦时代.png","hash":"9dec428cc6c918f9eaf6d47dd1a36f797678f72b","modified":1710051804435},{"_id":"public/2023/10/05/2023中秋/新天地-霸王茶姬.png","hash":"cd9609e61378cea89811ef3d1cf348ec59822036","modified":1710051804435},{"_id":"public/2023/10/05/2023中秋/解放公园中间.png","hash":"359aaf1437e94f494e6dd419ff6b7001a6e0b362","modified":1710051804435},{"_id":"public/source/_data/index.json","hash":"a5d3ef034188d579ebf1d8df46370f1e5070962e","modified":1710051856894}],"Category":[{"name":"Life","_id":"cltl4oxef0004xh7kdcffd02p"},{"name":"Research","_id":"cltl4oxei000cxh7k8w255zde"},{"name":"Paper Note","_id":"cltl4oxej000jxh7k86kp7os4"},{"name":"Thoughts","_id":"cltl4oxem000uxh7k7oa1duf3"},{"name":"Test","_id":"cltl4oxen0014xh7k50v9dsv5"},{"name":"Paper","_id":"cltl4oxen0019xh7k621mhbtp"}],"Data":[],"Page":[{"title":"About Me","date":"2023-09-14T10:18:06.000Z","type":"about","_content":"\n<img src=\"images/portrait.jpg\" alt=\"Portrait of Chen Yingfa having lunch in Beijing, taken by Luo Yining.\"/>\n\n<iconify-icon icon=\"mingcute:world-2-fill\"></iconify-icon> [中文](#Chinese-Version-中文版本)\n\n<iconify-icon icon=\"mingcute:link-fill\"></iconify-icon> Social links:\n\n- [Google Scholar](https://scholar.google.com/citations?user=IgPWvEQAAAAJ&hl=en)\n- [X (Twitter)](https://www.twitter.com/DonnyChan123)\n- [GitHub](https://www.github.com/chen-yingfa)\n- [知乎](https://www.zhihu.com/people/chen-ying-fa-34)\n- [B站](https://space.bilibili.com/474619698?spm_id_from=333.1007.0.0)\n\n<iconify-icon icon=\"mingcute:mail-fill\"></iconify-icon> Email: (either is ok) \n\n- chenyingfa1999@qq.com\n- donnychan1999@gmail.com\n\n---\n\n你好, hello, hei!\n\nMy name is Yingfa Chen (陈英发).\n\nI'm a 2nd year graduate student at the [Natural Language Processing lab at Tsinghua University](http://nlp.csai.tsinghua.edu.cn/), advised by Prof. Zhiyuan Liu. My research interests are about controlling the knowledge and behavior of large language models.\n\nI'm just getting started with academia, and will be applying for PhD in the same lab this year (the end of 2023), there is so much to learn!\n\n[:page_facing_up: My resume (last updated: December, 2022)](/pdf/cv.pdf)\n\n## Education\n\n---\n\n**M.S. in Computer Science and Technology (计算机科学与技术)**\n\n<div align=\"right\">Sep 2022 - Jul 2024 (expected)</div>\n\nTsinghua University, China\n\n*Research direction: natural language processing, large language models*\n\n*Advisor: Prof. Zhiyuan Liu*\n\n---\n\n**B.S. in Computer Science and Technology (计算机科学与技术)**\n\n<div align=\"right\">Aug 2018 - Jul 2022</div>\n\nTsinghua University, China \n\n---\n\n**General Studies in Natural Science (Studiespesialisering med realfag)**\n\n<div align=\"right\">\nAug 2014 - Jul 2018\n</div>\n\nMøglestu High School, Norway\n\n---\n\n## Personal Background\n\nI was born in March 13, 1999 in Arendal Norway. I grew up in Lillesand, a small coastal town in Aust-Agder, Norway. My parents are ethnically Chinese but born respectively in Vietnam and Cambodia, and they moved to Norway as refugees. My mother tongue is Cantonese Chinese.\n\nWhen I was 19, I went to Beijing for a Bachelor's degree in Computer Science at Tsinghua University.\n\nSince March of 2022, I am happily in a relationship with [Luo Yining](https://www.github.com/luo-yining/)^[The tag [#00](../../../tags/00) refers to my girlfriend.].\n\n## Interests\n\n🏸 Sports: Badminton, running, soccer.\n\n🎮 Entertainment: Video games, C-pop, anime, manga, Jackie Cheung, etc.\n\n---\n\n> Below is the Chinese version of this text, no need to read it if you understand the above text already.\n\n## Chinese Version 中文版本\n\n我叫陈英发。\n\n我是清华大学[自然语言处理与社会人文计算实验室](http://nlp.csai.tsinghua.edu.cn/)的二年级研究生，由刘知远副教授指导。我的研究兴趣是关于控制大型语言模型的知识和行为。\n\n我刚刚开始学术生涯，将在今年（2023年低）申请博士学位，还有很多要学习！\n\n[📃 我的简历（最后更新：2022年12月）](/pdf/cv.pdf)\n\n### 教育背景\n\n- **硕士：计算机科学与技术**\n\n    清华大学，中国\n\n    2022年9月 - 2024年7月（预计）\n\n    研究方向：自然语言处理，大型语言模型\n\n    导师：刘知远副教授\n\n\n- **本科：计算机科学与技术**\n\n    清华大学，中国\n\n    2018年8月 - 2022年7月\n\n\n- **高中：自然科学通识教育**\n\n    挪威，莫格勒斯图高中\n\n    2014年8月 - 2018年7月\n\n### 个人背景\n\n我于 1999 年 3 月 13 日在挪威 Arendal 出生。我长大在挪威 Aust-Agder 的一个沿海小镇 Lillesand。我的父母是华裔，分别出生在越南和柬埔寨，他们作为难民移民到挪威。我的母语是广东话。\n\n十九岁时，我前往北京，在清华大学攻读计算机科学学士学位。\n\n自 2022 年 3 月以来，我与我女友[骆怡宁](https://www.github.com/luo-yining/)相爱。\n\n### 兴趣爱好\n\n🏸 体育：羽毛球，跑步，足球。\n\n🎮 娱乐：电子游戏，C-pop，动漫，漫画，张学友等等。\n\n> Translated by ChatGPT, reviewed by me.\n\n---\n\n## Extra\n\nHere is a binary search in Python:\n\n```python\ndef bin_search(arr: list, target) -> int:\n    lo, hi = 0, len(arr)\n    while lo < hi:\n        m = (lo + hi) // 2\n        if arr[m] < target:\n            lo = m + 1\n        else:\n            hi = m\n    return lo\n```\n","source":"about.md","raw":"---\ntitle: About Me\ndate: 2023-09-14 18:18:06\ntype: \"about\"\n---\n\n<img src=\"images/portrait.jpg\" alt=\"Portrait of Chen Yingfa having lunch in Beijing, taken by Luo Yining.\"/>\n\n<iconify-icon icon=\"mingcute:world-2-fill\"></iconify-icon> [中文](#Chinese-Version-中文版本)\n\n<iconify-icon icon=\"mingcute:link-fill\"></iconify-icon> Social links:\n\n- [Google Scholar](https://scholar.google.com/citations?user=IgPWvEQAAAAJ&hl=en)\n- [X (Twitter)](https://www.twitter.com/DonnyChan123)\n- [GitHub](https://www.github.com/chen-yingfa)\n- [知乎](https://www.zhihu.com/people/chen-ying-fa-34)\n- [B站](https://space.bilibili.com/474619698?spm_id_from=333.1007.0.0)\n\n<iconify-icon icon=\"mingcute:mail-fill\"></iconify-icon> Email: (either is ok) \n\n- chenyingfa1999@qq.com\n- donnychan1999@gmail.com\n\n---\n\n你好, hello, hei!\n\nMy name is Yingfa Chen (陈英发).\n\nI'm a 2nd year graduate student at the [Natural Language Processing lab at Tsinghua University](http://nlp.csai.tsinghua.edu.cn/), advised by Prof. Zhiyuan Liu. My research interests are about controlling the knowledge and behavior of large language models.\n\nI'm just getting started with academia, and will be applying for PhD in the same lab this year (the end of 2023), there is so much to learn!\n\n[:page_facing_up: My resume (last updated: December, 2022)](/pdf/cv.pdf)\n\n## Education\n\n---\n\n**M.S. in Computer Science and Technology (计算机科学与技术)**\n\n<div align=\"right\">Sep 2022 - Jul 2024 (expected)</div>\n\nTsinghua University, China\n\n*Research direction: natural language processing, large language models*\n\n*Advisor: Prof. Zhiyuan Liu*\n\n---\n\n**B.S. in Computer Science and Technology (计算机科学与技术)**\n\n<div align=\"right\">Aug 2018 - Jul 2022</div>\n\nTsinghua University, China \n\n---\n\n**General Studies in Natural Science (Studiespesialisering med realfag)**\n\n<div align=\"right\">\nAug 2014 - Jul 2018\n</div>\n\nMøglestu High School, Norway\n\n---\n\n## Personal Background\n\nI was born in March 13, 1999 in Arendal Norway. I grew up in Lillesand, a small coastal town in Aust-Agder, Norway. My parents are ethnically Chinese but born respectively in Vietnam and Cambodia, and they moved to Norway as refugees. My mother tongue is Cantonese Chinese.\n\nWhen I was 19, I went to Beijing for a Bachelor's degree in Computer Science at Tsinghua University.\n\nSince March of 2022, I am happily in a relationship with [Luo Yining](https://www.github.com/luo-yining/)^[The tag [#00](../../../tags/00) refers to my girlfriend.].\n\n## Interests\n\n🏸 Sports: Badminton, running, soccer.\n\n🎮 Entertainment: Video games, C-pop, anime, manga, Jackie Cheung, etc.\n\n---\n\n> Below is the Chinese version of this text, no need to read it if you understand the above text already.\n\n## Chinese Version 中文版本\n\n我叫陈英发。\n\n我是清华大学[自然语言处理与社会人文计算实验室](http://nlp.csai.tsinghua.edu.cn/)的二年级研究生，由刘知远副教授指导。我的研究兴趣是关于控制大型语言模型的知识和行为。\n\n我刚刚开始学术生涯，将在今年（2023年低）申请博士学位，还有很多要学习！\n\n[📃 我的简历（最后更新：2022年12月）](/pdf/cv.pdf)\n\n### 教育背景\n\n- **硕士：计算机科学与技术**\n\n    清华大学，中国\n\n    2022年9月 - 2024年7月（预计）\n\n    研究方向：自然语言处理，大型语言模型\n\n    导师：刘知远副教授\n\n\n- **本科：计算机科学与技术**\n\n    清华大学，中国\n\n    2018年8月 - 2022年7月\n\n\n- **高中：自然科学通识教育**\n\n    挪威，莫格勒斯图高中\n\n    2014年8月 - 2018年7月\n\n### 个人背景\n\n我于 1999 年 3 月 13 日在挪威 Arendal 出生。我长大在挪威 Aust-Agder 的一个沿海小镇 Lillesand。我的父母是华裔，分别出生在越南和柬埔寨，他们作为难民移民到挪威。我的母语是广东话。\n\n十九岁时，我前往北京，在清华大学攻读计算机科学学士学位。\n\n自 2022 年 3 月以来，我与我女友[骆怡宁](https://www.github.com/luo-yining/)相爱。\n\n### 兴趣爱好\n\n🏸 体育：羽毛球，跑步，足球。\n\n🎮 娱乐：电子游戏，C-pop，动漫，漫画，张学友等等。\n\n> Translated by ChatGPT, reviewed by me.\n\n---\n\n## Extra\n\nHere is a binary search in Python:\n\n```python\ndef bin_search(arr: list, target) -> int:\n    lo, hi = 0, len(arr)\n    while lo < hi:\n        m = (lo + hi) // 2\n        if arr[m] < target:\n            lo = m + 1\n        else:\n            hi = m\n    return lo\n```\n","updated":"2023-09-30T09:06:19.412Z","path":"about.html","comments":1,"layout":"page","_id":"cltl4oxeb0000xh7k3dl59pfl","content":"<img src=\"images/portrait.jpg\" alt=\"Portrait of Chen Yingfa having lunch in Beijing, taken by Luo Yining.\">\n<p><iconify-icon icon=\"mingcute:world-2-fill\"></iconify-icon> <a href=\"#Chinese-Version-%E4%B8%AD%E6%96%87%E7%89%88%E6%9C%AC\">中文</a></p>\n<p><iconify-icon icon=\"mingcute:link-fill\"></iconify-icon> Social links:</p>\n<ul>\n<li><a href=\"https://scholar.google.com/citations?user=IgPWvEQAAAAJ&amp;hl=en\">Google Scholar</a></li>\n<li><a href=\"https://www.twitter.com/DonnyChan123\">X (Twitter)</a></li>\n<li><a href=\"https://www.github.com/chen-yingfa\">GitHub</a></li>\n<li><a href=\"https://www.zhihu.com/people/chen-ying-fa-34\">知乎</a></li>\n<li><a href=\"https://space.bilibili.com/474619698?spm_id_from=333.1007.0.0\">B站</a></li>\n</ul>\n<p><iconify-icon icon=\"mingcute:mail-fill\"></iconify-icon> Email: (either is ok)</p>\n<ul>\n<li><a href=\"mailto:chenyingfa1999@qq.com\">chenyingfa1999@qq.com</a></li>\n<li><a href=\"mailto:donnychan1999@gmail.com\">donnychan1999@gmail.com</a></li>\n</ul>\n<hr>\n<p>你好, hello, hei!</p>\n<p>My name is Yingfa Chen (陈英发).</p>\n<p>I'm a 2nd year graduate student at the <a href=\"http://nlp.csai.tsinghua.edu.cn/\">Natural Language Processing lab at Tsinghua University</a>, advised by Prof. Zhiyuan Liu. My research interests are about controlling the knowledge and behavior of large language models.</p>\n<p>I'm just getting started with academia, and will be applying for PhD in the same lab this year (the end of 2023), there is so much to learn!</p>\n<p><a href=\"/pdf/cv.pdf\"><span class=\"github-emoji\"><span>📄</span><img src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4c4.png?v8\" aria-hidden=\"true\" onerror=\"this.parent.classList.add('github-emoji-fallback')\"></span> My resume (last updated: December, 2022)</a></p>\n<h2 id=\"Education\">Education</h2>\n<hr>\n<p><strong>M.S. in Computer Science and Technology (计算机科学与技术)</strong></p>\n<div align=\"right\">Sep 2022 - Jul 2024 (expected)</div>\n<p>Tsinghua University, China</p>\n<p><em>Research direction: natural language processing, large language models</em></p>\n<p><em>Advisor: Prof. Zhiyuan Liu</em></p>\n<hr>\n<p><strong>B.S. in Computer Science and Technology (计算机科学与技术)</strong></p>\n<div align=\"right\">Aug 2018 - Jul 2022</div>\n<p>Tsinghua University, China</p>\n<hr>\n<p><strong>General Studies in Natural Science (Studiespesialisering med realfag)</strong></p>\n<div align=\"right\">\nAug 2014 - Jul 2018\n</div>\n<p>Møglestu High School, Norway</p>\n<hr>\n<h2 id=\"Personal-Background\">Personal Background</h2>\n<p>I was born in March 13, 1999 in Arendal Norway. I grew up in Lillesand, a small coastal town in Aust-Agder, Norway. My parents are ethnically Chinese but born respectively in Vietnam and Cambodia, and they moved to Norway as refugees. My mother tongue is Cantonese Chinese.</p>\n<p>When I was 19, I went to Beijing for a Bachelor's degree in Computer Science at Tsinghua University.</p>\n<p>Since March of 2022, I am happily in a relationship with <a href=\"https://www.github.com/luo-yining/\">Luo Yining</a><sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">[1]</a></sup>.</p>\n<h2 id=\"Interests\">Interests</h2>\n<p>🏸 Sports: Badminton, running, soccer.</p>\n<p>🎮 Entertainment: Video games, C-pop, anime, manga, Jackie Cheung, etc.</p>\n<hr>\n<blockquote>\n<p>Below is the Chinese version of this text, no need to read it if you understand the above text already.</p>\n</blockquote>\n<h2 id=\"Chinese-Version-中文版本\">Chinese Version 中文版本</h2>\n<p>我叫陈英发。</p>\n<p>我是清华大学<a href=\"http://nlp.csai.tsinghua.edu.cn/\">自然语言处理与社会人文计算实验室</a>的二年级研究生，由刘知远副教授指导。我的研究兴趣是关于控制大型语言模型的知识和行为。</p>\n<p>我刚刚开始学术生涯，将在今年（2023年低）申请博士学位，还有很多要学习！</p>\n<p><a href=\"/pdf/cv.pdf\">📃 我的简历（最后更新：2022年12月）</a></p>\n<h3 id=\"教育背景\">教育背景</h3>\n<ul>\n<li>\n<p><strong>硕士：计算机科学与技术</strong></p>\n<p>清华大学，中国</p>\n<p>2022年9月 - 2024年7月（预计）</p>\n<p>研究方向：自然语言处理，大型语言模型</p>\n<p>导师：刘知远副教授</p>\n</li>\n<li>\n<p><strong>本科：计算机科学与技术</strong></p>\n<p>清华大学，中国</p>\n<p>2018年8月 - 2022年7月</p>\n</li>\n<li>\n<p><strong>高中：自然科学通识教育</strong></p>\n<p>挪威，莫格勒斯图高中</p>\n<p>2014年8月 - 2018年7月</p>\n</li>\n</ul>\n<h3 id=\"个人背景\">个人背景</h3>\n<p>我于 1999 年 3 月 13 日在挪威 Arendal 出生。我长大在挪威 Aust-Agder 的一个沿海小镇 Lillesand。我的父母是华裔，分别出生在越南和柬埔寨，他们作为难民移民到挪威。我的母语是广东话。</p>\n<p>十九岁时，我前往北京，在清华大学攻读计算机科学学士学位。</p>\n<p>自 2022 年 3 月以来，我与我女友<a href=\"https://www.github.com/luo-yining/\">骆怡宁</a>相爱。</p>\n<h3 id=\"兴趣爱好\">兴趣爱好</h3>\n<p>🏸 体育：羽毛球，跑步，足球。</p>\n<p>🎮 娱乐：电子游戏，C-pop，动漫，漫画，张学友等等。</p>\n<blockquote>\n<p>Translated by ChatGPT, reviewed by me.</p>\n</blockquote>\n<hr>\n<h2 id=\"Extra\">Extra</h2>\n<p>Here is a binary search in Python:</p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">bin_search</span>(<span class=\"params\">arr: <span class=\"built_in\">list</span>, target</span>) -&gt; <span class=\"built_in\">int</span>:</span><br><span class=\"line\">    lo, hi = <span class=\"number\">0</span>, <span class=\"built_in\">len</span>(arr)</span><br><span class=\"line\">    <span class=\"keyword\">while</span> lo &lt; hi:</span><br><span class=\"line\">        m = (lo + hi) // <span class=\"number\">2</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> arr[m] &lt; target:</span><br><span class=\"line\">            lo = m + <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            hi = m</span><br><span class=\"line\">    <span class=\"keyword\">return</span> lo</span><br></pre></td></tr></tbody></table></figure>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn1\" class=\"footnote-item\"><p>The tag <a href=\"../../../tags/00\">#00</a> refers to my girlfriend. <a href=\"#fnref1\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n</ol>\n</section>\n","site":{"data":{}},"excerpt":"","more":"<img src=\"images/portrait.jpg\" alt=\"Portrait of Chen Yingfa having lunch in Beijing, taken by Luo Yining.\"/>\n<p><iconify-icon icon=\"mingcute:world-2-fill\"></iconify-icon> <a href=\"#Chinese-Version-%E4%B8%AD%E6%96%87%E7%89%88%E6%9C%AC\">中文</a></p>\n<p><iconify-icon icon=\"mingcute:link-fill\"></iconify-icon> Social links:</p>\n<ul>\n<li><a href=\"https://scholar.google.com/citations?user=IgPWvEQAAAAJ&amp;hl=en\">Google Scholar</a></li>\n<li><a href=\"https://www.twitter.com/DonnyChan123\">X (Twitter)</a></li>\n<li><a href=\"https://www.github.com/chen-yingfa\">GitHub</a></li>\n<li><a href=\"https://www.zhihu.com/people/chen-ying-fa-34\">知乎</a></li>\n<li><a href=\"https://space.bilibili.com/474619698?spm_id_from=333.1007.0.0\">B站</a></li>\n</ul>\n<p><iconify-icon icon=\"mingcute:mail-fill\"></iconify-icon> Email: (either is ok)</p>\n<ul>\n<li><a href=\"mailto:chenyingfa1999@qq.com\">chenyingfa1999@qq.com</a></li>\n<li><a href=\"mailto:donnychan1999@gmail.com\">donnychan1999@gmail.com</a></li>\n</ul>\n<hr>\n<p>你好, hello, hei!</p>\n<p>My name is Yingfa Chen (陈英发).</p>\n<p>I'm a 2nd year graduate student at the <a href=\"http://nlp.csai.tsinghua.edu.cn/\">Natural Language Processing lab at Tsinghua University</a>, advised by Prof. Zhiyuan Liu. My research interests are about controlling the knowledge and behavior of large language models.</p>\n<p>I'm just getting started with academia, and will be applying for PhD in the same lab this year (the end of 2023), there is so much to learn!</p>\n<p><a href=\"/pdf/cv.pdf\">:page_facing_up: My resume (last updated: December, 2022)</a></p>\n<h2 id=\"Education\">Education</h2>\n<hr>\n<p><strong>M.S. in Computer Science and Technology (计算机科学与技术)</strong></p>\n<div align=\"right\">Sep 2022 - Jul 2024 (expected)</div>\n<p>Tsinghua University, China</p>\n<p><em>Research direction: natural language processing, large language models</em></p>\n<p><em>Advisor: Prof. Zhiyuan Liu</em></p>\n<hr>\n<p><strong>B.S. in Computer Science and Technology (计算机科学与技术)</strong></p>\n<div align=\"right\">Aug 2018 - Jul 2022</div>\n<p>Tsinghua University, China</p>\n<hr>\n<p><strong>General Studies in Natural Science (Studiespesialisering med realfag)</strong></p>\n<div align=\"right\">\nAug 2014 - Jul 2018\n</div>\n<p>Møglestu High School, Norway</p>\n<hr>\n<h2 id=\"Personal-Background\">Personal Background</h2>\n<p>I was born in March 13, 1999 in Arendal Norway. I grew up in Lillesand, a small coastal town in Aust-Agder, Norway. My parents are ethnically Chinese but born respectively in Vietnam and Cambodia, and they moved to Norway as refugees. My mother tongue is Cantonese Chinese.</p>\n<p>When I was 19, I went to Beijing for a Bachelor's degree in Computer Science at Tsinghua University.</p>\n<p>Since March of 2022, I am happily in a relationship with <a href=\"https://www.github.com/luo-yining/\">Luo Yining</a><sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">[1]</a></sup>.</p>\n<h2 id=\"Interests\">Interests</h2>\n<p>🏸 Sports: Badminton, running, soccer.</p>\n<p>🎮 Entertainment: Video games, C-pop, anime, manga, Jackie Cheung, etc.</p>\n<hr>\n<blockquote>\n<p>Below is the Chinese version of this text, no need to read it if you understand the above text already.</p>\n</blockquote>\n<h2 id=\"Chinese-Version-中文版本\">Chinese Version 中文版本</h2>\n<p>我叫陈英发。</p>\n<p>我是清华大学<a href=\"http://nlp.csai.tsinghua.edu.cn/\">自然语言处理与社会人文计算实验室</a>的二年级研究生，由刘知远副教授指导。我的研究兴趣是关于控制大型语言模型的知识和行为。</p>\n<p>我刚刚开始学术生涯，将在今年（2023年低）申请博士学位，还有很多要学习！</p>\n<p><a href=\"/pdf/cv.pdf\">📃 我的简历（最后更新：2022年12月）</a></p>\n<h3 id=\"教育背景\">教育背景</h3>\n<ul>\n<li>\n<p><strong>硕士：计算机科学与技术</strong></p>\n<p>清华大学，中国</p>\n<p>2022年9月 - 2024年7月（预计）</p>\n<p>研究方向：自然语言处理，大型语言模型</p>\n<p>导师：刘知远副教授</p>\n</li>\n<li>\n<p><strong>本科：计算机科学与技术</strong></p>\n<p>清华大学，中国</p>\n<p>2018年8月 - 2022年7月</p>\n</li>\n<li>\n<p><strong>高中：自然科学通识教育</strong></p>\n<p>挪威，莫格勒斯图高中</p>\n<p>2014年8月 - 2018年7月</p>\n</li>\n</ul>\n<h3 id=\"个人背景\">个人背景</h3>\n<p>我于 1999 年 3 月 13 日在挪威 Arendal 出生。我长大在挪威 Aust-Agder 的一个沿海小镇 Lillesand。我的父母是华裔，分别出生在越南和柬埔寨，他们作为难民移民到挪威。我的母语是广东话。</p>\n<p>十九岁时，我前往北京，在清华大学攻读计算机科学学士学位。</p>\n<p>自 2022 年 3 月以来，我与我女友<a href=\"https://www.github.com/luo-yining/\">骆怡宁</a>相爱。</p>\n<h3 id=\"兴趣爱好\">兴趣爱好</h3>\n<p>🏸 体育：羽毛球，跑步，足球。</p>\n<p>🎮 娱乐：电子游戏，C-pop，动漫，漫画，张学友等等。</p>\n<blockquote>\n<p>Translated by ChatGPT, reviewed by me.</p>\n</blockquote>\n<hr>\n<h2 id=\"Extra\">Extra</h2>\n<p>Here is a binary search in Python:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">bin_search</span>(<span class=\"params\">arr: <span class=\"built_in\">list</span>, target</span>) -&gt; <span class=\"built_in\">int</span>:</span><br><span class=\"line\">    lo, hi = <span class=\"number\">0</span>, <span class=\"built_in\">len</span>(arr)</span><br><span class=\"line\">    <span class=\"keyword\">while</span> lo &lt; hi:</span><br><span class=\"line\">        m = (lo + hi) // <span class=\"number\">2</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> arr[m] &lt; target:</span><br><span class=\"line\">            lo = m + <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            hi = m</span><br><span class=\"line\">    <span class=\"keyword\">return</span> lo</span><br></pre></td></tr></table></figure>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn1\" class=\"footnote-item\"><p>The tag <a href=\"../../../tags/00\">#00</a> refers to my girlfriend. <a href=\"#fnref1\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n</ol>\n</section>\n"},{"_content":"Test","source":"friends/index.md","raw":"Test","date":"2023-09-15T16:49:37.338Z","updated":"2023-09-15T16:49:37.338Z","path":"friends/index.html","title":"","comments":1,"layout":"page","_id":"cltl4oxee0002xh7k2d4wdwdx","content":"<p>Test</p>\n","site":{"data":{}},"excerpt":"","more":"<p>Test</p>\n"},{"title":"My Publications","date":"2023-09-16T11:13:19.000Z","type":"publications","_content":"<!-- # My Publications -->\n\nList of all my publications:\n\n**2023**:\n\n- [GitHub] [$\\infty$-Bench: Extending Long Context Evaluation to Over 100K](/2024/01/10/InfiniteBench/)\n    - [Code](http://www.github.com/OpenBMB/InfiniteBench)\n\n- [preprint] [CFDBench: A Comprehensive Benchmark for Machine Learning Methods in Fluid Dynamics](/2023/09/16/CFDBench/)\n    - [Code](https://www.github.com/luo-yining/CFDBench) | [知乎](https://zhuanlan.zhihu.com/p/656033757) | [Paper (preprints.org)](https://www.preprints.org/manuscript/202309.1550/v1)\n\n<!-- - [preprint] [Robust and Scalable Model Editing for Large Language Models](/2023/09/14/EREN/) -->\n- [ACL 2023] READIN: A Chinese Multi-Task Benchmark with Realistic and Diverse Input Noises\n    - [Code](https://www.github.com/THUNLP/READIN) | [Paper](https://aclanthology.org/2023.acl-long.460/)\n\n- [TACL 2023] Sub-Character Tokenization for Chinese Pretrained Language Models\n    - [Code](https://www.github.com/THUNLP/SubCharTokenization) | [Paper](https://aclanthology.org/2023.tacl-1.28/)\n\n**2022**:\n\n- [EMNLP 2022 Demo] BMCook: A Task-agnostic Compression Toolkit for Big Models\n    - [Code](https://www.github.com/OpenBMB/BMCook)\n","source":"publications.md","raw":"---\ntitle: My Publications\ndate: 2023-09-16 19:13:19\ntype: \"publications\"\n---\n<!-- # My Publications -->\n\nList of all my publications:\n\n**2023**:\n\n- [GitHub] [$\\infty$-Bench: Extending Long Context Evaluation to Over 100K](/2024/01/10/InfiniteBench/)\n    - [Code](http://www.github.com/OpenBMB/InfiniteBench)\n\n- [preprint] [CFDBench: A Comprehensive Benchmark for Machine Learning Methods in Fluid Dynamics](/2023/09/16/CFDBench/)\n    - [Code](https://www.github.com/luo-yining/CFDBench) | [知乎](https://zhuanlan.zhihu.com/p/656033757) | [Paper (preprints.org)](https://www.preprints.org/manuscript/202309.1550/v1)\n\n<!-- - [preprint] [Robust and Scalable Model Editing for Large Language Models](/2023/09/14/EREN/) -->\n- [ACL 2023] READIN: A Chinese Multi-Task Benchmark with Realistic and Diverse Input Noises\n    - [Code](https://www.github.com/THUNLP/READIN) | [Paper](https://aclanthology.org/2023.acl-long.460/)\n\n- [TACL 2023] Sub-Character Tokenization for Chinese Pretrained Language Models\n    - [Code](https://www.github.com/THUNLP/SubCharTokenization) | [Paper](https://aclanthology.org/2023.tacl-1.28/)\n\n**2022**:\n\n- [EMNLP 2022 Demo] BMCook: A Task-agnostic Compression Toolkit for Big Models\n    - [Code](https://www.github.com/OpenBMB/BMCook)\n","updated":"2024-01-11T03:51:02.707Z","path":"publications.html","comments":1,"layout":"page","_id":"cltl4oxeg0006xh7k3vum3lmo","content":"<p>List of all my publications:</p>\n<p><strong>2023</strong>:</p>\n<ul>\n<li>\n<p>[GitHub] <a href=\"/2024/01/10/InfiniteBench/\">$\\infty$-Bench: Extending Long Context Evaluation to Over 100K</a></p>\n<ul>\n<li><a href=\"http://www.github.com/OpenBMB/InfiniteBench\">Code</a></li>\n</ul>\n</li>\n<li>\n<p>[preprint] <a href=\"/2023/09/16/CFDBench/\">CFDBench: A Comprehensive Benchmark for Machine Learning Methods in Fluid Dynamics</a></p>\n<ul>\n<li><a href=\"https://www.github.com/luo-yining/CFDBench\">Code</a> | <a href=\"https://zhuanlan.zhihu.com/p/656033757\">知乎</a> | <a href=\"https://www.preprints.org/manuscript/202309.1550/v1\">Paper (preprints.org)</a></li>\n</ul>\n</li>\n</ul>\n<!-- - [preprint] [Robust and Scalable Model Editing for Large Language Models](/2023/09/14/EREN/) -->\n<ul>\n<li>\n<p>[ACL 2023] READIN: A Chinese Multi-Task Benchmark with Realistic and Diverse Input Noises</p>\n<ul>\n<li><a href=\"https://www.github.com/THUNLP/READIN\">Code</a> | <a href=\"https://aclanthology.org/2023.acl-long.460/\">Paper</a></li>\n</ul>\n</li>\n<li>\n<p>[TACL 2023] Sub-Character Tokenization for Chinese Pretrained Language Models</p>\n<ul>\n<li><a href=\"https://www.github.com/THUNLP/SubCharTokenization\">Code</a> | <a href=\"https://aclanthology.org/2023.tacl-1.28/\">Paper</a></li>\n</ul>\n</li>\n</ul>\n<p><strong>2022</strong>:</p>\n<ul>\n<li>[EMNLP 2022 Demo] BMCook: A Task-agnostic Compression Toolkit for Big Models\n<ul>\n<li><a href=\"https://www.github.com/OpenBMB/BMCook\">Code</a></li>\n</ul>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<!-- # My Publications -->\n<p>List of all my publications:</p>\n<p><strong>2023</strong>:</p>\n<ul>\n<li>\n<p>[GitHub] <a href=\"/2024/01/10/InfiniteBench/\">$\\infty$-Bench: Extending Long Context Evaluation to Over 100K</a></p>\n<ul>\n<li><a href=\"http://www.github.com/OpenBMB/InfiniteBench\">Code</a></li>\n</ul>\n</li>\n<li>\n<p>[preprint] <a href=\"/2023/09/16/CFDBench/\">CFDBench: A Comprehensive Benchmark for Machine Learning Methods in Fluid Dynamics</a></p>\n<ul>\n<li><a href=\"https://www.github.com/luo-yining/CFDBench\">Code</a> | <a href=\"https://zhuanlan.zhihu.com/p/656033757\">知乎</a> | <a href=\"https://www.preprints.org/manuscript/202309.1550/v1\">Paper (preprints.org)</a></li>\n</ul>\n</li>\n</ul>\n<!-- - [preprint] [Robust and Scalable Model Editing for Large Language Models](/2023/09/14/EREN/) -->\n<ul>\n<li>\n<p>[ACL 2023] READIN: A Chinese Multi-Task Benchmark with Realistic and Diverse Input Noises</p>\n<ul>\n<li><a href=\"https://www.github.com/THUNLP/READIN\">Code</a> | <a href=\"https://aclanthology.org/2023.acl-long.460/\">Paper</a></li>\n</ul>\n</li>\n<li>\n<p>[TACL 2023] Sub-Character Tokenization for Chinese Pretrained Language Models</p>\n<ul>\n<li><a href=\"https://www.github.com/THUNLP/SubCharTokenization\">Code</a> | <a href=\"https://aclanthology.org/2023.tacl-1.28/\">Paper</a></li>\n</ul>\n</li>\n</ul>\n<p><strong>2022</strong>:</p>\n<ul>\n<li>[EMNLP 2022 Demo] BMCook: A Task-agnostic Compression Toolkit for Big Models\n<ul>\n<li><a href=\"https://www.github.com/OpenBMB/BMCook\">Code</a></li>\n</ul>\n</li>\n</ul>\n"},{"title":"Categories 类别","date":"2023-09-15T16:50:43.000Z","type":"categories","layout":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: Categories 类别\ndate: 2023-09-16 00:50:43\ntype: \"categories\"\nlayout: categories\n---\n","updated":"2024-03-10T04:49:18.292Z","path":"categories/index.html","comments":1,"_id":"cltl4oxeh0008xh7kh4qe1bwf","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"home","date":"2023-09-16T08:08:44.000Z","_content":"\nHello","source":"home/index.md","raw":"---\ntitle: home\ndate: 2023-09-16 16:08:44\n---\n\nHello","updated":"2023-09-16T08:08:50.129Z","path":"home/index.html","comments":1,"layout":"page","_id":"cltl4oxeh000axh7k47lwczxn","content":"<p>Hello</p>\n","site":{"data":{}},"excerpt":"","more":"<p>Hello</p>\n"},{"title":"index","date":"2023-09-16T08:36:33.000Z","layout":"index","_content":"\nTest  \nThis is the content of index.md.\n\n这里是 index.md 的内容。\n\n> A comment\n\n","source":"index/index.md","raw":"---\ntitle: index\ndate: 2023-09-16 16:36:33\nlayout: index\n---\n\nTest  \nThis is the content of index.md.\n\n这里是 index.md 的内容。\n\n> A comment\n\n","updated":"2024-01-23T11:12:03.884Z","path":"index/index.html","comments":1,"_id":"cltl4oxei000exh7k543nfa96","content":"<p>Test<br>\nThis is the content of <a href=\"http://index.md\">index.md</a>.</p>\n<p>这里是 <a href=\"http://index.md\">index.md</a> 的内容。</p>\n<blockquote>\n<p>A comment</p>\n</blockquote>\n","site":{"data":{}},"excerpt":"","more":"<p>Test<br>\nThis is the content of <a href=\"http://index.md\">index.md</a>.</p>\n<p>这里是 <a href=\"http://index.md\">index.md</a> 的内容。</p>\n<blockquote>\n<p>A comment</p>\n</blockquote>\n"},{"title":"Tags 标签","date":"2023-09-15T16:00:51.000Z","type":"tags","layout":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: Tags 标签\ndate: 2023-09-16 00:00:51\ntype: \"tags\"\nlayout: tags\n---\n","updated":"2024-03-10T04:48:43.903Z","path":"tags/index.html","comments":1,"_id":"cltl4oxei000gxh7k60m1ckeh","content":"","site":{"data":{}},"excerpt":"","more":""},{"date":"2023-09-23T07:24:00.000Z","type":"projects","_content":"\n# Projects\n\n## 枫叶 Fengye\n\nA Hexo theme that is minimalistic, modern and beautiful.\n\n[Code](https://www.github.com/chen-yingfa/hexo-theme-fengye)\n\n## 轻书 Qingshu\n\nA modern and minimalistic Markdown editor with Manaco.\n\n[Code](https://www.github.com/chen-yingfa/qingshu)\n","source":"projects.md","raw":"---\ndate: 2023-09-23 15:24:00\ntype: \"projects\"\n---\n\n# Projects\n\n## 枫叶 Fengye\n\nA Hexo theme that is minimalistic, modern and beautiful.\n\n[Code](https://www.github.com/chen-yingfa/hexo-theme-fengye)\n\n## 轻书 Qingshu\n\nA modern and minimalistic Markdown editor with Manaco.\n\n[Code](https://www.github.com/chen-yingfa/qingshu)\n","updated":"2023-09-23T07:28:34.813Z","path":"projects.html","title":"","comments":1,"layout":"page","_id":"cltl4oxej000lxh7k2glv22nq","content":"<h1>Projects</h1>\n<h2 id=\"枫叶-Fengye\">枫叶 Fengye</h2>\n<p>A Hexo theme that is minimalistic, modern and beautiful.</p>\n<p><a href=\"https://www.github.com/chen-yingfa/hexo-theme-fengye\">Code</a></p>\n<h2 id=\"轻书-Qingshu\">轻书 Qingshu</h2>\n<p>A modern and minimalistic Markdown editor with Manaco.</p>\n<p><a href=\"https://www.github.com/chen-yingfa/qingshu\">Code</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h1>Projects</h1>\n<h2 id=\"枫叶-Fengye\">枫叶 Fengye</h2>\n<p>A Hexo theme that is minimalistic, modern and beautiful.</p>\n<p><a href=\"https://www.github.com/chen-yingfa/hexo-theme-fengye\">Code</a></p>\n<h2 id=\"轻书-Qingshu\">轻书 Qingshu</h2>\n<p>A modern and minimalistic Markdown editor with Manaco.</p>\n<p><a href=\"https://www.github.com/chen-yingfa/qingshu\">Code</a></p>\n"},{"_content":"Test\nThis is the content of index.md.\n\n这里是 index.md 的内容。\n\n> A comment\n","source":"indexx.md","raw":"Test\nThis is the content of index.md.\n\n这里是 index.md 的内容。\n\n> A comment\n","date":"2023-09-23T07:31:36.827Z","updated":"2023-09-23T07:31:36.827Z","path":"indexx.html","title":"","comments":1,"layout":"page","_id":"cltl4oxez0097xh7khum42pby","content":"<p>Test\nThis is the content of <a href=\"http://index.md\">index.md</a>.</p>\n<p>这里是 <a href=\"http://index.md\">index.md</a> 的内容。</p>\n<blockquote>\n<p>A comment</p>\n</blockquote>\n","site":{"data":{}},"excerpt":"","more":"<p>Test\nThis is the content of <a href=\"http://index.md\">index.md</a>.</p>\n<p>这里是 <a href=\"http://index.md\">index.md</a> 的内容。</p>\n<blockquote>\n<p>A comment</p>\n</blockquote>\n"}],"Post":[{"author":"陈英发 Yingfa Chen","title":"2023年中秋和国庆","date":"2023-10-05T05:55:02.000Z","thumbnail":"武商梦时代.png","_content":"\n今年国庆 🇨🇳 和中秋 🥮 一起放假，我跟 00 一起回来应城参加她堂姐和初中同学的婚礼^[27号是初中同学（魏陈）的婚礼，5号是堂姐（骆卓颖）的婚礼。]，\n住在她家里十个夜晚^[九月二十六日回来，十月七日走。坐高铁到北京，然后做火车到应城。]。第二次见家长，也算是挺顺利，但是每天都会见到陌生人，有点累，庆幸的是，感觉到 00 能接受跟我家人生活在一起。一号到三号我们去武汉玩了三天，超级开心，跟她在一起连逛商场都是开心的！\n\n\n## 小县城的氛围\n\n应城跟我想象中的小县城很像，也是很多远房亲戚，习俗也让人很烦。敬酒、随地扔垃圾、室内抽烟、八卦人家的私事、说话粗鄙、脏、说了不要还非要给人家……而且确实能明显感觉到，这里的人的素质的平均水平挺低的，尤其是上一辈。真的很讨厌吃席，00 也是，这些习俗的麻烦程度让 00 都不想结婚了……\n\n<!-- more -->\n\n但是无所谓了，之后能跟 00 在一起就好，除了回来过节应该也很少机会有联系。\n\n## 武汉\n\n一号到三号去了武汉旅游。早上五点多跟 00 的 ”二妈“（其实是婶，叔叔的老婆）坐车去武汉，坐了一个小时。他们这么早是因为要去谈婚礼的事情，然后害怕堵车。我们在酒店旁边下来，那时候“二妈”下地铁站上厕所，然后 00 非要给她买包子（为了礼貌），然后她最后还是拒绝了，导致我们得自己吃下包子。虽然包子没有不好吃，但是我就很讨厌这种明知人家不要还非要买的行为。\n\n之后我们去酒店的时候，还没有房子，我们寄存了行李就直接去新天地买了杯霸王茶姬的奶茶，然后去了古德寺。网上说不可以穿着暴露，但是感觉路人穿着还是很暴露。\n\n![在武汉新天地买霸王茶姬。](./2023中秋/新天地-霸王茶姬.png \"在武汉新天地买霸王茶姬。\")\n\n之后还去了解放公园和中山公园，都挺不错的。大城市就是好。里面看到了很好看的建筑物。在中山公园我们问了两个小孩借用羽毛球拍子来打了几下。之后在一个相亲角^[之前在上海都没找到。]旁边跟她的高中同学，彭双，会合，然后逛了一下相亲角。之后我们还坐了一下过山车（公园里面有过山车还是第一次见）。\n\n![解放公园中间的一个很多塔的地方。](./2023中秋/解放公园中间.png \"解放公园中间的一个很多塔的地方。\")\n\n晚上就去跟她的高中同学一起吃饭。\n\n第二天我们先在地铁站剪了头发，然后去宝通寺，晚上去武商梦时代。这个商场规格超级高，还挺好玩的。第一次看到索尼专卖店，还有 Pico 专卖店。里面还有滑雪的地方，但是太贵的。我们还去了优衣库，买了一些衣服，发现还挺便宜的。以前都会觉得逛街购物很无聊，但是跟她在一起连连逛街买衣服都是开心的。\n\n晚上我们跟她“大哥”（其实是堂哥）和他老婆一起吃饭，吃了魔宗烤肉，然后喝了茶颜悦色。总体来说也挺顺利的，感觉他们也不难相处。\n\n![武商梦时代里面的美食街买鲜虾汤包](./2023中秋/武商梦时代.png \"武商梦时代里面的美食街买鲜虾汤包\")\n\n第三天我们去了欢乐谷！是我们第一次一起去游乐场！玩了一个过山车，然后做了太阳飞车，00 就头晕想吐了，果然还是不行……但是没事，还是挺开心的。排队过程中还遇到了插队的人，好恶心！\n\n晚上我们跟一些人（共七个人）一起拼车回来应城，居然比火车还便宜，不错。回来已经11点了，然后回家放下行李箱之后又出去找她初中同学一起吃宵夜。\n\n![在武汉欢乐谷玩耍。](./2023中秋/武汉欢乐谷.png \"在武汉欢乐谷玩耍。\")\n\n\n## 公事\n\n这个假期有点长，感觉有很多活都没有干。每天都很多事情，感觉这里的人太闲了，应该让他们多上班哈哈哈。古文字翻译的工作还没有干完，目前感觉效果不是很好，我也不想干这个了，感觉很浪费我的时间……至于对齐神经元，貌似现有方法都无法用在自回归模型上面，但是对齐问题好像之后自回归模型才会出现。不知道是不是我没有找到，目前还没有找到一篇研究神经元对生成结果的影响的工作。[ROME](https://www.github.com/kmeng01/rome) 的 Causal Tracing 感觉可以用，这两天得赶紧做点东西出来。\n","source":"_posts/2023中秋.md","raw":"---\nauthor: 陈英发 Yingfa Chen\ntitle: 2023年中秋和国庆\ndate: 2023-10-05 13:55:02\ncategories: Life\nthumbnail: 武商梦时代.png\ntags:\n- life\n- 中秋\n- 中文\n- '00'\n- wedding\n- 中秋-middle-autumn\n- 国庆-national-day\n- 应城\n- 武汉-wuhan\n---\n\n今年国庆 🇨🇳 和中秋 🥮 一起放假，我跟 00 一起回来应城参加她堂姐和初中同学的婚礼^[27号是初中同学（魏陈）的婚礼，5号是堂姐（骆卓颖）的婚礼。]，\n住在她家里十个夜晚^[九月二十六日回来，十月七日走。坐高铁到北京，然后做火车到应城。]。第二次见家长，也算是挺顺利，但是每天都会见到陌生人，有点累，庆幸的是，感觉到 00 能接受跟我家人生活在一起。一号到三号我们去武汉玩了三天，超级开心，跟她在一起连逛商场都是开心的！\n\n\n## 小县城的氛围\n\n应城跟我想象中的小县城很像，也是很多远房亲戚，习俗也让人很烦。敬酒、随地扔垃圾、室内抽烟、八卦人家的私事、说话粗鄙、脏、说了不要还非要给人家……而且确实能明显感觉到，这里的人的素质的平均水平挺低的，尤其是上一辈。真的很讨厌吃席，00 也是，这些习俗的麻烦程度让 00 都不想结婚了……\n\n<!-- more -->\n\n但是无所谓了，之后能跟 00 在一起就好，除了回来过节应该也很少机会有联系。\n\n## 武汉\n\n一号到三号去了武汉旅游。早上五点多跟 00 的 ”二妈“（其实是婶，叔叔的老婆）坐车去武汉，坐了一个小时。他们这么早是因为要去谈婚礼的事情，然后害怕堵车。我们在酒店旁边下来，那时候“二妈”下地铁站上厕所，然后 00 非要给她买包子（为了礼貌），然后她最后还是拒绝了，导致我们得自己吃下包子。虽然包子没有不好吃，但是我就很讨厌这种明知人家不要还非要买的行为。\n\n之后我们去酒店的时候，还没有房子，我们寄存了行李就直接去新天地买了杯霸王茶姬的奶茶，然后去了古德寺。网上说不可以穿着暴露，但是感觉路人穿着还是很暴露。\n\n![在武汉新天地买霸王茶姬。](./2023中秋/新天地-霸王茶姬.png \"在武汉新天地买霸王茶姬。\")\n\n之后还去了解放公园和中山公园，都挺不错的。大城市就是好。里面看到了很好看的建筑物。在中山公园我们问了两个小孩借用羽毛球拍子来打了几下。之后在一个相亲角^[之前在上海都没找到。]旁边跟她的高中同学，彭双，会合，然后逛了一下相亲角。之后我们还坐了一下过山车（公园里面有过山车还是第一次见）。\n\n![解放公园中间的一个很多塔的地方。](./2023中秋/解放公园中间.png \"解放公园中间的一个很多塔的地方。\")\n\n晚上就去跟她的高中同学一起吃饭。\n\n第二天我们先在地铁站剪了头发，然后去宝通寺，晚上去武商梦时代。这个商场规格超级高，还挺好玩的。第一次看到索尼专卖店，还有 Pico 专卖店。里面还有滑雪的地方，但是太贵的。我们还去了优衣库，买了一些衣服，发现还挺便宜的。以前都会觉得逛街购物很无聊，但是跟她在一起连连逛街买衣服都是开心的。\n\n晚上我们跟她“大哥”（其实是堂哥）和他老婆一起吃饭，吃了魔宗烤肉，然后喝了茶颜悦色。总体来说也挺顺利的，感觉他们也不难相处。\n\n![武商梦时代里面的美食街买鲜虾汤包](./2023中秋/武商梦时代.png \"武商梦时代里面的美食街买鲜虾汤包\")\n\n第三天我们去了欢乐谷！是我们第一次一起去游乐场！玩了一个过山车，然后做了太阳飞车，00 就头晕想吐了，果然还是不行……但是没事，还是挺开心的。排队过程中还遇到了插队的人，好恶心！\n\n晚上我们跟一些人（共七个人）一起拼车回来应城，居然比火车还便宜，不错。回来已经11点了，然后回家放下行李箱之后又出去找她初中同学一起吃宵夜。\n\n![在武汉欢乐谷玩耍。](./2023中秋/武汉欢乐谷.png \"在武汉欢乐谷玩耍。\")\n\n\n## 公事\n\n这个假期有点长，感觉有很多活都没有干。每天都很多事情，感觉这里的人太闲了，应该让他们多上班哈哈哈。古文字翻译的工作还没有干完，目前感觉效果不是很好，我也不想干这个了，感觉很浪费我的时间……至于对齐神经元，貌似现有方法都无法用在自回归模型上面，但是对齐问题好像之后自回归模型才会出现。不知道是不是我没有找到，目前还没有找到一篇研究神经元对生成结果的影响的工作。[ROME](https://www.github.com/kmeng01/rome) 的 Causal Tracing 感觉可以用，这两天得赶紧做点东西出来。\n","slug":"2023中秋","published":1,"updated":"2024-01-11T05:44:33.394Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cltl4oxed0001xh7k0ksv1gpa","content":"<p>今年国庆 🇨🇳 和中秋 🥮 一起放假，我跟 00 一起回来应城参加她堂姐和初中同学的婚礼<sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">[1]</a></sup>，\n住在她家里十个夜晚<sup class=\"footnote-ref\"><a href=\"#fn2\" id=\"fnref2\">[2]</a></sup>。第二次见家长，也算是挺顺利，但是每天都会见到陌生人，有点累，庆幸的是，感觉到 00 能接受跟我家人生活在一起。一号到三号我们去武汉玩了三天，超级开心，跟她在一起连逛商场都是开心的！</p>\n<h2 id=\"小县城的氛围\">小县城的氛围</h2>\n<p>应城跟我想象中的小县城很像，也是很多远房亲戚，习俗也让人很烦。敬酒、随地扔垃圾、室内抽烟、八卦人家的私事、说话粗鄙、脏、说了不要还非要给人家……而且确实能明显感觉到，这里的人的素质的平均水平挺低的，尤其是上一辈。真的很讨厌吃席，00 也是，这些习俗的麻烦程度让 00 都不想结婚了……</p>\n<span id=\"more\"></span>\n<p>但是无所谓了，之后能跟 00 在一起就好，除了回来过节应该也很少机会有联系。</p>\n<h2 id=\"武汉\">武汉</h2>\n<p>一号到三号去了武汉旅游。早上五点多跟 00 的 ”二妈“（其实是婶，叔叔的老婆）坐车去武汉，坐了一个小时。他们这么早是因为要去谈婚礼的事情，然后害怕堵车。我们在酒店旁边下来，那时候“二妈”下地铁站上厕所，然后 00 非要给她买包子（为了礼貌），然后她最后还是拒绝了，导致我们得自己吃下包子。虽然包子没有不好吃，但是我就很讨厌这种明知人家不要还非要买的行为。</p>\n<p>之后我们去酒店的时候，还没有房子，我们寄存了行李就直接去新天地买了杯霸王茶姬的奶茶，然后去了古德寺。网上说不可以穿着暴露，但是感觉路人穿着还是很暴露。</p>\n<p><img src=\"/2023/10/05/2023%E4%B8%AD%E7%A7%8B/%E6%96%B0%E5%A4%A9%E5%9C%B0-%E9%9C%B8%E7%8E%8B%E8%8C%B6%E5%A7%AC.png\" alt=\"在武汉新天地买霸王茶姬。\" title=\"在武汉新天地买霸王茶姬。\"></p>\n<p>之后还去了解放公园和中山公园，都挺不错的。大城市就是好。里面看到了很好看的建筑物。在中山公园我们问了两个小孩借用羽毛球拍子来打了几下。之后在一个相亲角<sup class=\"footnote-ref\"><a href=\"#fn3\" id=\"fnref3\">[3]</a></sup>旁边跟她的高中同学，彭双，会合，然后逛了一下相亲角。之后我们还坐了一下过山车（公园里面有过山车还是第一次见）。</p>\n<p><img src=\"/2023/10/05/2023%E4%B8%AD%E7%A7%8B/%E8%A7%A3%E6%94%BE%E5%85%AC%E5%9B%AD%E4%B8%AD%E9%97%B4.png\" alt=\"解放公园中间的一个很多塔的地方。\" title=\"解放公园中间的一个很多塔的地方。\"></p>\n<p>晚上就去跟她的高中同学一起吃饭。</p>\n<p>第二天我们先在地铁站剪了头发，然后去宝通寺，晚上去武商梦时代。这个商场规格超级高，还挺好玩的。第一次看到索尼专卖店，还有 Pico 专卖店。里面还有滑雪的地方，但是太贵的。我们还去了优衣库，买了一些衣服，发现还挺便宜的。以前都会觉得逛街购物很无聊，但是跟她在一起连连逛街买衣服都是开心的。</p>\n<p>晚上我们跟她“大哥”（其实是堂哥）和他老婆一起吃饭，吃了魔宗烤肉，然后喝了茶颜悦色。总体来说也挺顺利的，感觉他们也不难相处。</p>\n<p><img src=\"/2023/10/05/2023%E4%B8%AD%E7%A7%8B/%E6%AD%A6%E5%95%86%E6%A2%A6%E6%97%B6%E4%BB%A3.png\" alt=\"武商梦时代里面的美食街买鲜虾汤包\" title=\"武商梦时代里面的美食街买鲜虾汤包\"></p>\n<p>第三天我们去了欢乐谷！是我们第一次一起去游乐场！玩了一个过山车，然后做了太阳飞车，00 就头晕想吐了，果然还是不行……但是没事，还是挺开心的。排队过程中还遇到了插队的人，好恶心！</p>\n<p>晚上我们跟一些人（共七个人）一起拼车回来应城，居然比火车还便宜，不错。回来已经11点了，然后回家放下行李箱之后又出去找她初中同学一起吃宵夜。</p>\n<p><img src=\"/2023/10/05/2023%E4%B8%AD%E7%A7%8B/%E6%AD%A6%E6%B1%89%E6%AC%A2%E4%B9%90%E8%B0%B7.png\" alt=\"在武汉欢乐谷玩耍。\" title=\"在武汉欢乐谷玩耍。\"></p>\n<h2 id=\"公事\">公事</h2>\n<p>这个假期有点长，感觉有很多活都没有干。每天都很多事情，感觉这里的人太闲了，应该让他们多上班哈哈哈。古文字翻译的工作还没有干完，目前感觉效果不是很好，我也不想干这个了，感觉很浪费我的时间……至于对齐神经元，貌似现有方法都无法用在自回归模型上面，但是对齐问题好像之后自回归模型才会出现。不知道是不是我没有找到，目前还没有找到一篇研究神经元对生成结果的影响的工作。<a href=\"https://www.github.com/kmeng01/rome\">ROME</a> 的 Causal Tracing 感觉可以用，这两天得赶紧做点东西出来。</p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn1\" class=\"footnote-item\"><p>27号是初中同学（魏陈）的婚礼，5号是堂姐（骆卓颖）的婚礼。 <a href=\"#fnref1\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n<li id=\"fn2\" class=\"footnote-item\"><p>九月二十六日回来，十月七日走。坐高铁到北京，然后做火车到应城。 <a href=\"#fnref2\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n<li id=\"fn3\" class=\"footnote-item\"><p>之前在上海都没找到。 <a href=\"#fnref3\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n</ol>\n</section>\n","site":{"data":{}},"excerpt":"<p>今年国庆 🇨🇳 和中秋 🥮 一起放假，我跟 00 一起回来应城参加她堂姐和初中同学的婚礼<sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">[1]</a></sup>，\n住在她家里十个夜晚<sup class=\"footnote-ref\"><a href=\"#fn2\" id=\"fnref2\">[2]</a></sup>。第二次见家长，也算是挺顺利，但是每天都会见到陌生人，有点累，庆幸的是，感觉到 00 能接受跟我家人生活在一起。一号到三号我们去武汉玩了三天，超级开心，跟她在一起连逛商场都是开心的！</p>\n<h2 id=\"小县城的氛围\">小县城的氛围</h2>\n<p>应城跟我想象中的小县城很像，也是很多远房亲戚，习俗也让人很烦。敬酒、随地扔垃圾、室内抽烟、八卦人家的私事、说话粗鄙、脏、说了不要还非要给人家……而且确实能明显感觉到，这里的人的素质的平均水平挺低的，尤其是上一辈。真的很讨厌吃席，00 也是，这些习俗的麻烦程度让 00 都不想结婚了……</p>","more":"<p>但是无所谓了，之后能跟 00 在一起就好，除了回来过节应该也很少机会有联系。</p>\n<h2 id=\"武汉\">武汉</h2>\n<p>一号到三号去了武汉旅游。早上五点多跟 00 的 ”二妈“（其实是婶，叔叔的老婆）坐车去武汉，坐了一个小时。他们这么早是因为要去谈婚礼的事情，然后害怕堵车。我们在酒店旁边下来，那时候“二妈”下地铁站上厕所，然后 00 非要给她买包子（为了礼貌），然后她最后还是拒绝了，导致我们得自己吃下包子。虽然包子没有不好吃，但是我就很讨厌这种明知人家不要还非要买的行为。</p>\n<p>之后我们去酒店的时候，还没有房子，我们寄存了行李就直接去新天地买了杯霸王茶姬的奶茶，然后去了古德寺。网上说不可以穿着暴露，但是感觉路人穿着还是很暴露。</p>\n<p><img src=\"/2023/10/05/2023%E4%B8%AD%E7%A7%8B/%E6%96%B0%E5%A4%A9%E5%9C%B0-%E9%9C%B8%E7%8E%8B%E8%8C%B6%E5%A7%AC.png\" alt=\"在武汉新天地买霸王茶姬。\" title=\"在武汉新天地买霸王茶姬。\"></p>\n<p>之后还去了解放公园和中山公园，都挺不错的。大城市就是好。里面看到了很好看的建筑物。在中山公园我们问了两个小孩借用羽毛球拍子来打了几下。之后在一个相亲角<sup class=\"footnote-ref\"><a href=\"#fn3\" id=\"fnref3\">[3]</a></sup>旁边跟她的高中同学，彭双，会合，然后逛了一下相亲角。之后我们还坐了一下过山车（公园里面有过山车还是第一次见）。</p>\n<p><img src=\"/2023/10/05/2023%E4%B8%AD%E7%A7%8B/%E8%A7%A3%E6%94%BE%E5%85%AC%E5%9B%AD%E4%B8%AD%E9%97%B4.png\" alt=\"解放公园中间的一个很多塔的地方。\" title=\"解放公园中间的一个很多塔的地方。\"></p>\n<p>晚上就去跟她的高中同学一起吃饭。</p>\n<p>第二天我们先在地铁站剪了头发，然后去宝通寺，晚上去武商梦时代。这个商场规格超级高，还挺好玩的。第一次看到索尼专卖店，还有 Pico 专卖店。里面还有滑雪的地方，但是太贵的。我们还去了优衣库，买了一些衣服，发现还挺便宜的。以前都会觉得逛街购物很无聊，但是跟她在一起连连逛街买衣服都是开心的。</p>\n<p>晚上我们跟她“大哥”（其实是堂哥）和他老婆一起吃饭，吃了魔宗烤肉，然后喝了茶颜悦色。总体来说也挺顺利的，感觉他们也不难相处。</p>\n<p><img src=\"/2023/10/05/2023%E4%B8%AD%E7%A7%8B/%E6%AD%A6%E5%95%86%E6%A2%A6%E6%97%B6%E4%BB%A3.png\" alt=\"武商梦时代里面的美食街买鲜虾汤包\" title=\"武商梦时代里面的美食街买鲜虾汤包\"></p>\n<p>第三天我们去了欢乐谷！是我们第一次一起去游乐场！玩了一个过山车，然后做了太阳飞车，00 就头晕想吐了，果然还是不行……但是没事，还是挺开心的。排队过程中还遇到了插队的人，好恶心！</p>\n<p>晚上我们跟一些人（共七个人）一起拼车回来应城，居然比火车还便宜，不错。回来已经11点了，然后回家放下行李箱之后又出去找她初中同学一起吃宵夜。</p>\n<p><img src=\"/2023/10/05/2023%E4%B8%AD%E7%A7%8B/%E6%AD%A6%E6%B1%89%E6%AC%A2%E4%B9%90%E8%B0%B7.png\" alt=\"在武汉欢乐谷玩耍。\" title=\"在武汉欢乐谷玩耍。\"></p>\n<h2 id=\"公事\">公事</h2>\n<p>这个假期有点长，感觉有很多活都没有干。每天都很多事情，感觉这里的人太闲了，应该让他们多上班哈哈哈。古文字翻译的工作还没有干完，目前感觉效果不是很好，我也不想干这个了，感觉很浪费我的时间……至于对齐神经元，貌似现有方法都无法用在自回归模型上面，但是对齐问题好像之后自回归模型才会出现。不知道是不是我没有找到，目前还没有找到一篇研究神经元对生成结果的影响的工作。<a href=\"https://www.github.com/kmeng01/rome\">ROME</a> 的 Causal Tracing 感觉可以用，这两天得赶紧做点东西出来。</p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn1\" class=\"footnote-item\"><p>27号是初中同学（魏陈）的婚礼，5号是堂姐（骆卓颖）的婚礼。 <a href=\"#fnref1\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n<li id=\"fn2\" class=\"footnote-item\"><p>九月二十六日回来，十月七日走。坐高铁到北京，然后做火车到应城。 <a href=\"#fnref2\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n<li id=\"fn3\" class=\"footnote-item\"><p>之前在上海都没找到。 <a href=\"#fnref3\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n</ol>\n</section>"},{"author":"陈英发 Yingfa Chen","title":"CFDBench: A Comprehensive Benchmark for Machine Learning Methods in Fluid Dynamics","date":"2023-09-16T11:47:17.000Z","featured":true,"_content":"\n[Code](https://www.github.com/luo-yining/CFDBench) | [Paper (on hold by ArXiv)](...) | [Paper (preprints.org)](https://www.preprints.org/manuscript/202309.1550/v1) | [知乎](https://zhuanlan.zhihu.com/p/656033757)\n\nI did this work with my girlfriend, whose research direction is computational fluid dynamics (CFD). We observed that there are numerous research works in applying deep learning (DL) to solve CFD problems. E.g., [Pangu-Weather](https://github.com/198808xc/Pangu-Weather) have shown that DL methods can not only be more accurate than the best numerical methods, but can also be multiple magnitudes faster.\n\n<!-- more -->\n\nHowever, there is no standard benchmark for evaluating the performance of different DL methods. Therefore, we constructed CFDBench.\n\n---\n\n## Abstract\n\nIn recent years, applying deep learning to solve physics problems has attracted much attention. Data-driven deep learning methods produce operators that can learn solutions to the whole system of partial differential equations. However, the existing methods are only evaluated on simple flow equations (e.g., Burger's equation), and only consider the generalization ability on different initial conditions. In this paper, we construct CFDBench, a benchmark with four classic problems in computational fluid dynamics (CFD): lid-driven cavity flow, laminar boundary layer flow in circular tubes, dam flows through the steps, and periodic Karman vortex street. Each flow problem includes data with different boundary conditions, fluid physical properties, and domain geometry. Compared to existing datasets, the advantages of CFDBench are (1) comprehensive. It contains common physical parameters such as velocity, pressure, and cavity fraction. (2) realistic. It is very suitable for deep learning solutions of fluid mechanics equations. (3) challenging. It has a certain learning difficulty, prompting to find models with strong learning ability. (4) standardized. CFDBench facilitates a comprehensive and fair comparison of different deep learning methods for CFD. We make appropriate modifications to popular deep neural networks to apply them to CFDBench and enable the accommodation of more changing inputs. The evaluation on CFDBench reveals some new shortcomings of existing works and we propose possible directions for solving such problems.\n","source":"_posts/CFDBench.md","raw":"---\nauthor: 陈英发 Yingfa Chen\ntitle: \"CFDBench: A Comprehensive Benchmark for Machine Learning Methods in Fluid Dynamics\"\ndate: 2023-09-16 19:47:17\ncategories: Research\ntags:\n- paper\n- research\n- cfd\n- dataset\n- \"00\"\n- english\n- pinn\n- fno\n- physics\n- machine-learning\n- deep-learning\n- deeponet\n- ai4science\nfeatured: true\n---\n\n[Code](https://www.github.com/luo-yining/CFDBench) | [Paper (on hold by ArXiv)](...) | [Paper (preprints.org)](https://www.preprints.org/manuscript/202309.1550/v1) | [知乎](https://zhuanlan.zhihu.com/p/656033757)\n\nI did this work with my girlfriend, whose research direction is computational fluid dynamics (CFD). We observed that there are numerous research works in applying deep learning (DL) to solve CFD problems. E.g., [Pangu-Weather](https://github.com/198808xc/Pangu-Weather) have shown that DL methods can not only be more accurate than the best numerical methods, but can also be multiple magnitudes faster.\n\n<!-- more -->\n\nHowever, there is no standard benchmark for evaluating the performance of different DL methods. Therefore, we constructed CFDBench.\n\n---\n\n## Abstract\n\nIn recent years, applying deep learning to solve physics problems has attracted much attention. Data-driven deep learning methods produce operators that can learn solutions to the whole system of partial differential equations. However, the existing methods are only evaluated on simple flow equations (e.g., Burger's equation), and only consider the generalization ability on different initial conditions. In this paper, we construct CFDBench, a benchmark with four classic problems in computational fluid dynamics (CFD): lid-driven cavity flow, laminar boundary layer flow in circular tubes, dam flows through the steps, and periodic Karman vortex street. Each flow problem includes data with different boundary conditions, fluid physical properties, and domain geometry. Compared to existing datasets, the advantages of CFDBench are (1) comprehensive. It contains common physical parameters such as velocity, pressure, and cavity fraction. (2) realistic. It is very suitable for deep learning solutions of fluid mechanics equations. (3) challenging. It has a certain learning difficulty, prompting to find models with strong learning ability. (4) standardized. CFDBench facilitates a comprehensive and fair comparison of different deep learning methods for CFD. We make appropriate modifications to popular deep neural networks to apply them to CFDBench and enable the accommodation of more changing inputs. The evaluation on CFDBench reveals some new shortcomings of existing works and we propose possible directions for solving such problems.\n","slug":"CFDBench","published":1,"updated":"2024-01-11T03:45:40.727Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cltl4oxee0003xh7k1kul88dt","content":"<p><a href=\"https://www.github.com/luo-yining/CFDBench\">Code</a> | <a href=\"...\">Paper (on hold by ArXiv)</a> | <a href=\"https://www.preprints.org/manuscript/202309.1550/v1\">Paper (preprints.org)</a> | <a href=\"https://zhuanlan.zhihu.com/p/656033757\">知乎</a></p>\n<p>I did this work with my girlfriend, whose research direction is computational fluid dynamics (CFD). We observed that there are numerous research works in applying deep learning (DL) to solve CFD problems. E.g., <a href=\"https://github.com/198808xc/Pangu-Weather\">Pangu-Weather</a> have shown that DL methods can not only be more accurate than the best numerical methods, but can also be multiple magnitudes faster.</p>\n<span id=\"more\"></span>\n<p>However, there is no standard benchmark for evaluating the performance of different DL methods. Therefore, we constructed CFDBench.</p>\n<hr>\n<h2 id=\"Abstract\">Abstract</h2>\n<p>In recent years, applying deep learning to solve physics problems has attracted much attention. Data-driven deep learning methods produce operators that can learn solutions to the whole system of partial differential equations. However, the existing methods are only evaluated on simple flow equations (e.g., Burger's equation), and only consider the generalization ability on different initial conditions. In this paper, we construct CFDBench, a benchmark with four classic problems in computational fluid dynamics (CFD): lid-driven cavity flow, laminar boundary layer flow in circular tubes, dam flows through the steps, and periodic Karman vortex street. Each flow problem includes data with different boundary conditions, fluid physical properties, and domain geometry. Compared to existing datasets, the advantages of CFDBench are (1) comprehensive. It contains common physical parameters such as velocity, pressure, and cavity fraction. (2) realistic. It is very suitable for deep learning solutions of fluid mechanics equations. (3) challenging. It has a certain learning difficulty, prompting to find models with strong learning ability. (4) standardized. CFDBench facilitates a comprehensive and fair comparison of different deep learning methods for CFD. We make appropriate modifications to popular deep neural networks to apply them to CFDBench and enable the accommodation of more changing inputs. The evaluation on CFDBench reveals some new shortcomings of existing works and we propose possible directions for solving such problems.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"https://www.github.com/luo-yining/CFDBench\">Code</a> | <a href=\"...\">Paper (on hold by ArXiv)</a> | <a href=\"https://www.preprints.org/manuscript/202309.1550/v1\">Paper (preprints.org)</a> | <a href=\"https://zhuanlan.zhihu.com/p/656033757\">知乎</a></p>\n<p>I did this work with my girlfriend, whose research direction is computational fluid dynamics (CFD). We observed that there are numerous research works in applying deep learning (DL) to solve CFD problems. E.g., <a href=\"https://github.com/198808xc/Pangu-Weather\">Pangu-Weather</a> have shown that DL methods can not only be more accurate than the best numerical methods, but can also be multiple magnitudes faster.</p>","more":"<p>However, there is no standard benchmark for evaluating the performance of different DL methods. Therefore, we constructed CFDBench.</p>\n<hr>\n<h2 id=\"Abstract\">Abstract</h2>\n<p>In recent years, applying deep learning to solve physics problems has attracted much attention. Data-driven deep learning methods produce operators that can learn solutions to the whole system of partial differential equations. However, the existing methods are only evaluated on simple flow equations (e.g., Burger's equation), and only consider the generalization ability on different initial conditions. In this paper, we construct CFDBench, a benchmark with four classic problems in computational fluid dynamics (CFD): lid-driven cavity flow, laminar boundary layer flow in circular tubes, dam flows through the steps, and periodic Karman vortex street. Each flow problem includes data with different boundary conditions, fluid physical properties, and domain geometry. Compared to existing datasets, the advantages of CFDBench are (1) comprehensive. It contains common physical parameters such as velocity, pressure, and cavity fraction. (2) realistic. It is very suitable for deep learning solutions of fluid mechanics equations. (3) challenging. It has a certain learning difficulty, prompting to find models with strong learning ability. (4) standardized. CFDBench facilitates a comprehensive and fair comparison of different deep learning methods for CFD. We make appropriate modifications to popular deep neural networks to apply them to CFDBench and enable the accommodation of more changing inputs. The evaluation on CFDBench reveals some new shortcomings of existing works and we propose possible directions for solving such problems.</p>"},{"author":"陈英发 Yingfa Chen","title":"Activation Addition (ActAdd)","date":"2023-10-07T09:55:33.000Z","_content":"\n[Paper](https://arxiv.org/abs/2308.10248)\n\nTLDR: Propose **ActAdd**, a method for controlling model behavior during inference by modifying activations with a bias term that is learned from a pair of prompt.\n\nSummary:\n\n- Propose **ActAdd**, a method for controlling model behavior by modifying activations at inference time.\n- Steering vectors are computed by taking the activation differences that result from pairs of prompts. The vectors are added as bias during inference.\n- ActAdd provides control over high-level properties of the output, and preserves off-target model performance, and requires little computational and implementational costs.\n\n<!-- more -->\n\n> The recently popular [representation engineering paper](https://arxiv.org/abs/2310.01405) (RepE) seems to be largely inspired by this work.\n\n## Background\n\nThe authors propose to compute steering vectors to steer the model's behavior. They call such methods *activation engineering*. They make the following contributions:\n\n- Find that combining forward passes works well in GPT-2, despite it was not trained for this.\n- The proposed method, **ActAdd**, is efficient, requiring no gradient descent or labeled data.\n\nThe difference between ActAdd and existing steering vector methods is that they find the vectors via one of the following.\n\n- Differences after fine-tuning\n- Per-query gradient-based search\n- Linear probes + differences in truthy attention heads\n\nIn contrast, ActAdd uses the difference between prompt pairs instead.\n\n## Method\n\nThe method is really, really simple. Simply manually contruct a pair of prompts, and compute the difference between the activations. Then, add the difference as a bias term to the activations during inference. The algorithm is as follows.\n\n![ActAdd method](./actadd/alg.png \"The algorithm of ActAdd\")\n\nAs shown, this method has two hyperparameters, the amount of drift $c$, and the modified layer $l$, and requires two manually constructed prompts $(p_+, p_-)$. How to more effectively construct these prompts is not discussed in this paper.\n\n## Result\n\n![Main result](./actadd/result.png \"Main results.\")\n\n## My Thoughts\n\nThe effectiveness of this method is a strong evidence that input and output features are represented as linear directions in representational space, but we still have no explanation for why such linearity arises naturally in LLMs. The fact that this actually works is very thought-provoking. However, ActAdd start to see degraded performance on off-target inputs when we drive the activations to far off, and the steering sometimes simply fails, this may indicate that the optimal steering path is not linear, which I believe is reasonable. This is somewhat similar to neuron attribution methods, many features/skills/knowledge cannot be attributed to single neurons (they are distributed across neurons), but existing neuron attribution methods still work well because, by change, some features are primarily determined by the activity or state of a single neuron (or a small set of neurons).\n\nWe can also draw a parallel with [BitFit](https://arxiv.org/abs/2106.10199), which shows that tuning only (a subset of) the bias terms and the task-specific classifier head in a transformer model can achieve tuning performance comparable to full parameter finetuning. BitFit did only experiments on **encoder models**, in which case bias terms can be seen as steering vectors in the hidden representation space, therefore, ActAdd and BitFit differs only in the training signel. ActAdd uses the difference between the representation of a pair of (positive and negative) prompts, while BitFit propagates the human annotation from the classification head. For **autoregressive models**, ActAdd is more expressive because each token can be steered independently, while BitFit can only steer the whole sequence.\n\nInterestingly, the fact that difference (or other arithmetics) in hidden representation are useful signal for some semantics has been shown in the era of learning word vectors, the fact that this can be used as a training signal is pretty neat.\n\nThe author also discussed the difference between activation engineering and adaptation methods, but the empirical results were not enough to show that ActAdd can be used as a substitute for adaptation. E.g., we cannot practically adapt the model into performing machine translation with ActAdd, because there is no negative prompt for translation. But perhaps upcoming works can apply this method to replace fine-tuning (or other adaptation methods). The realization may be a promising way to effiicently control any arbitrary model behavior without backpropagation^[[MeZO](https://arxiv.org/abs/2305.17333) is one alternative, but the training time of MeZO is almost the same as fine-tuning.].\n\nNevertheless, this method is extremely interesting, and I feel like many existing methods can be improved by taking inspirations from this work.\n","source":"_posts/actadd.md","raw":"---\nauthor: 陈英发 Yingfa Chen\ntitle: Activation Addition (ActAdd)\ndate: 2023-10-07 17:55:33\ncategories: Paper Note\ntags:\n- english\n- ai-alignment\n- llm\n- gpt\n- activation-modification\n- adaptation\n- model-editing\n- representation-engineering\n- fine-tuning\n- parameter-efficient-tuning\n---\n\n[Paper](https://arxiv.org/abs/2308.10248)\n\nTLDR: Propose **ActAdd**, a method for controlling model behavior during inference by modifying activations with a bias term that is learned from a pair of prompt.\n\nSummary:\n\n- Propose **ActAdd**, a method for controlling model behavior by modifying activations at inference time.\n- Steering vectors are computed by taking the activation differences that result from pairs of prompts. The vectors are added as bias during inference.\n- ActAdd provides control over high-level properties of the output, and preserves off-target model performance, and requires little computational and implementational costs.\n\n<!-- more -->\n\n> The recently popular [representation engineering paper](https://arxiv.org/abs/2310.01405) (RepE) seems to be largely inspired by this work.\n\n## Background\n\nThe authors propose to compute steering vectors to steer the model's behavior. They call such methods *activation engineering*. They make the following contributions:\n\n- Find that combining forward passes works well in GPT-2, despite it was not trained for this.\n- The proposed method, **ActAdd**, is efficient, requiring no gradient descent or labeled data.\n\nThe difference between ActAdd and existing steering vector methods is that they find the vectors via one of the following.\n\n- Differences after fine-tuning\n- Per-query gradient-based search\n- Linear probes + differences in truthy attention heads\n\nIn contrast, ActAdd uses the difference between prompt pairs instead.\n\n## Method\n\nThe method is really, really simple. Simply manually contruct a pair of prompts, and compute the difference between the activations. Then, add the difference as a bias term to the activations during inference. The algorithm is as follows.\n\n![ActAdd method](./actadd/alg.png \"The algorithm of ActAdd\")\n\nAs shown, this method has two hyperparameters, the amount of drift $c$, and the modified layer $l$, and requires two manually constructed prompts $(p_+, p_-)$. How to more effectively construct these prompts is not discussed in this paper.\n\n## Result\n\n![Main result](./actadd/result.png \"Main results.\")\n\n## My Thoughts\n\nThe effectiveness of this method is a strong evidence that input and output features are represented as linear directions in representational space, but we still have no explanation for why such linearity arises naturally in LLMs. The fact that this actually works is very thought-provoking. However, ActAdd start to see degraded performance on off-target inputs when we drive the activations to far off, and the steering sometimes simply fails, this may indicate that the optimal steering path is not linear, which I believe is reasonable. This is somewhat similar to neuron attribution methods, many features/skills/knowledge cannot be attributed to single neurons (they are distributed across neurons), but existing neuron attribution methods still work well because, by change, some features are primarily determined by the activity or state of a single neuron (or a small set of neurons).\n\nWe can also draw a parallel with [BitFit](https://arxiv.org/abs/2106.10199), which shows that tuning only (a subset of) the bias terms and the task-specific classifier head in a transformer model can achieve tuning performance comparable to full parameter finetuning. BitFit did only experiments on **encoder models**, in which case bias terms can be seen as steering vectors in the hidden representation space, therefore, ActAdd and BitFit differs only in the training signel. ActAdd uses the difference between the representation of a pair of (positive and negative) prompts, while BitFit propagates the human annotation from the classification head. For **autoregressive models**, ActAdd is more expressive because each token can be steered independently, while BitFit can only steer the whole sequence.\n\nInterestingly, the fact that difference (or other arithmetics) in hidden representation are useful signal for some semantics has been shown in the era of learning word vectors, the fact that this can be used as a training signal is pretty neat.\n\nThe author also discussed the difference between activation engineering and adaptation methods, but the empirical results were not enough to show that ActAdd can be used as a substitute for adaptation. E.g., we cannot practically adapt the model into performing machine translation with ActAdd, because there is no negative prompt for translation. But perhaps upcoming works can apply this method to replace fine-tuning (or other adaptation methods). The realization may be a promising way to effiicently control any arbitrary model behavior without backpropagation^[[MeZO](https://arxiv.org/abs/2305.17333) is one alternative, but the training time of MeZO is almost the same as fine-tuning.].\n\nNevertheless, this method is extremely interesting, and I feel like many existing methods can be improved by taking inspirations from this work.\n","slug":"actadd","published":1,"updated":"2024-01-11T05:42:02.734Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cltl4oxeg0007xh7k5ati3i3b","content":"<p><a href=\"https://arxiv.org/abs/2308.10248\">Paper</a></p>\n<p>TLDR: Propose <strong>ActAdd</strong>, a method for controlling model behavior during inference by modifying activations with a bias term that is learned from a pair of prompt.</p>\n<p>Summary:</p>\n<ul>\n<li>Propose <strong>ActAdd</strong>, a method for controlling model behavior by modifying activations at inference time.</li>\n<li>Steering vectors are computed by taking the activation differences that result from pairs of prompts. The vectors are added as bias during inference.</li>\n<li>ActAdd provides control over high-level properties of the output, and preserves off-target model performance, and requires little computational and implementational costs.</li>\n</ul>\n<span id=\"more\"></span>\n<blockquote>\n<p>The recently popular <a href=\"https://arxiv.org/abs/2310.01405\">representation engineering paper</a> (RepE) seems to be largely inspired by this work.</p>\n</blockquote>\n<h2 id=\"Background\">Background</h2>\n<p>The authors propose to compute steering vectors to steer the model's behavior. They call such methods <em>activation engineering</em>. They make the following contributions:</p>\n<ul>\n<li>Find that combining forward passes works well in GPT-2, despite it was not trained for this.</li>\n<li>The proposed method, <strong>ActAdd</strong>, is efficient, requiring no gradient descent or labeled data.</li>\n</ul>\n<p>The difference between ActAdd and existing steering vector methods is that they find the vectors via one of the following.</p>\n<ul>\n<li>Differences after fine-tuning</li>\n<li>Per-query gradient-based search</li>\n<li>Linear probes + differences in truthy attention heads</li>\n</ul>\n<p>In contrast, ActAdd uses the difference between prompt pairs instead.</p>\n<h2 id=\"Method\">Method</h2>\n<p>The method is really, really simple. Simply manually contruct a pair of prompts, and compute the difference between the activations. Then, add the difference as a bias term to the activations during inference. The algorithm is as follows.</p>\n<p><img src=\"/2023/10/07/actadd/alg.png\" alt=\"ActAdd method\" title=\"The algorithm of ActAdd\"></p>\n<p>As shown, this method has two hyperparameters, the amount of drift $c$, and the modified layer $l$, and requires two manually constructed prompts $(p_+, p_-)$. How to more effectively construct these prompts is not discussed in this paper.</p>\n<h2 id=\"Result\">Result</h2>\n<p><img src=\"/2023/10/07/actadd/result.png\" alt=\"Main result\" title=\"Main results.\"></p>\n<h2 id=\"My-Thoughts\">My Thoughts</h2>\n<p>The effectiveness of this method is a strong evidence that input and output features are represented as linear directions in representational space, but we still have no explanation for why such linearity arises naturally in LLMs. The fact that this actually works is very thought-provoking. However, ActAdd start to see degraded performance on off-target inputs when we drive the activations to far off, and the steering sometimes simply fails, this may indicate that the optimal steering path is not linear, which I believe is reasonable. This is somewhat similar to neuron attribution methods, many features/skills/knowledge cannot be attributed to single neurons (they are distributed across neurons), but existing neuron attribution methods still work well because, by change, some features are primarily determined by the activity or state of a single neuron (or a small set of neurons).</p>\n<p>We can also draw a parallel with <a href=\"https://arxiv.org/abs/2106.10199\">BitFit</a>, which shows that tuning only (a subset of) the bias terms and the task-specific classifier head in a transformer model can achieve tuning performance comparable to full parameter finetuning. BitFit did only experiments on <strong>encoder models</strong>, in which case bias terms can be seen as steering vectors in the hidden representation space, therefore, ActAdd and BitFit differs only in the training signel. ActAdd uses the difference between the representation of a pair of (positive and negative) prompts, while BitFit propagates the human annotation from the classification head. For <strong>autoregressive models</strong>, ActAdd is more expressive because each token can be steered independently, while BitFit can only steer the whole sequence.</p>\n<p>Interestingly, the fact that difference (or other arithmetics) in hidden representation are useful signal for some semantics has been shown in the era of learning word vectors, the fact that this can be used as a training signal is pretty neat.</p>\n<p>The author also discussed the difference between activation engineering and adaptation methods, but the empirical results were not enough to show that ActAdd can be used as a substitute for adaptation. E.g., we cannot practically adapt the model into performing machine translation with ActAdd, because there is no negative prompt for translation. But perhaps upcoming works can apply this method to replace fine-tuning (or other adaptation methods). The realization may be a promising way to effiicently control any arbitrary model behavior without backpropagation<sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">[1]</a></sup>.</p>\n<p>Nevertheless, this method is extremely interesting, and I feel like many existing methods can be improved by taking inspirations from this work.</p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn1\" class=\"footnote-item\"><p><a href=\"https://arxiv.org/abs/2305.17333\">MeZO</a> is one alternative, but the training time of MeZO is almost the same as fine-tuning. <a href=\"#fnref1\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n</ol>\n</section>\n","site":{"data":{}},"excerpt":"<p><a href=\"https://arxiv.org/abs/2308.10248\">Paper</a></p>\n<p>TLDR: Propose <strong>ActAdd</strong>, a method for controlling model behavior during inference by modifying activations with a bias term that is learned from a pair of prompt.</p>\n<p>Summary:</p>\n<ul>\n<li>Propose <strong>ActAdd</strong>, a method for controlling model behavior by modifying activations at inference time.</li>\n<li>Steering vectors are computed by taking the activation differences that result from pairs of prompts. The vectors are added as bias during inference.</li>\n<li>ActAdd provides control over high-level properties of the output, and preserves off-target model performance, and requires little computational and implementational costs.</li>\n</ul>","more":"<blockquote>\n<p>The recently popular <a href=\"https://arxiv.org/abs/2310.01405\">representation engineering paper</a> (RepE) seems to be largely inspired by this work.</p>\n</blockquote>\n<h2 id=\"Background\">Background</h2>\n<p>The authors propose to compute steering vectors to steer the model's behavior. They call such methods <em>activation engineering</em>. They make the following contributions:</p>\n<ul>\n<li>Find that combining forward passes works well in GPT-2, despite it was not trained for this.</li>\n<li>The proposed method, <strong>ActAdd</strong>, is efficient, requiring no gradient descent or labeled data.</li>\n</ul>\n<p>The difference between ActAdd and existing steering vector methods is that they find the vectors via one of the following.</p>\n<ul>\n<li>Differences after fine-tuning</li>\n<li>Per-query gradient-based search</li>\n<li>Linear probes + differences in truthy attention heads</li>\n</ul>\n<p>In contrast, ActAdd uses the difference between prompt pairs instead.</p>\n<h2 id=\"Method\">Method</h2>\n<p>The method is really, really simple. Simply manually contruct a pair of prompts, and compute the difference between the activations. Then, add the difference as a bias term to the activations during inference. The algorithm is as follows.</p>\n<p><img src=\"/2023/10/07/actadd/alg.png\" alt=\"ActAdd method\" title=\"The algorithm of ActAdd\"></p>\n<p>As shown, this method has two hyperparameters, the amount of drift $c$, and the modified layer $l$, and requires two manually constructed prompts $(p_+, p_-)$. How to more effectively construct these prompts is not discussed in this paper.</p>\n<h2 id=\"Result\">Result</h2>\n<p><img src=\"/2023/10/07/actadd/result.png\" alt=\"Main result\" title=\"Main results.\"></p>\n<h2 id=\"My-Thoughts\">My Thoughts</h2>\n<p>The effectiveness of this method is a strong evidence that input and output features are represented as linear directions in representational space, but we still have no explanation for why such linearity arises naturally in LLMs. The fact that this actually works is very thought-provoking. However, ActAdd start to see degraded performance on off-target inputs when we drive the activations to far off, and the steering sometimes simply fails, this may indicate that the optimal steering path is not linear, which I believe is reasonable. This is somewhat similar to neuron attribution methods, many features/skills/knowledge cannot be attributed to single neurons (they are distributed across neurons), but existing neuron attribution methods still work well because, by change, some features are primarily determined by the activity or state of a single neuron (or a small set of neurons).</p>\n<p>We can also draw a parallel with <a href=\"https://arxiv.org/abs/2106.10199\">BitFit</a>, which shows that tuning only (a subset of) the bias terms and the task-specific classifier head in a transformer model can achieve tuning performance comparable to full parameter finetuning. BitFit did only experiments on <strong>encoder models</strong>, in which case bias terms can be seen as steering vectors in the hidden representation space, therefore, ActAdd and BitFit differs only in the training signel. ActAdd uses the difference between the representation of a pair of (positive and negative) prompts, while BitFit propagates the human annotation from the classification head. For <strong>autoregressive models</strong>, ActAdd is more expressive because each token can be steered independently, while BitFit can only steer the whole sequence.</p>\n<p>Interestingly, the fact that difference (or other arithmetics) in hidden representation are useful signal for some semantics has been shown in the era of learning word vectors, the fact that this can be used as a training signal is pretty neat.</p>\n<p>The author also discussed the difference between activation engineering and adaptation methods, but the empirical results were not enough to show that ActAdd can be used as a substitute for adaptation. E.g., we cannot practically adapt the model into performing machine translation with ActAdd, because there is no negative prompt for translation. But perhaps upcoming works can apply this method to replace fine-tuning (or other adaptation methods). The realization may be a promising way to effiicently control any arbitrary model behavior without backpropagation<sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">[1]</a></sup>.</p>\n<p>Nevertheless, this method is extremely interesting, and I feel like many existing methods can be improved by taking inspirations from this work.</p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn1\" class=\"footnote-item\"><p><a href=\"https://arxiv.org/abs/2305.17333\">MeZO</a> is one alternative, but the training time of MeZO is almost the same as fine-tuning. <a href=\"#fnref1\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n</ol>\n</section>"},{"author":"陈英发 Yingfa Chen","title":"InfiniteBench: Extending Long Context Evaluation Beyond 100K Tokens","date":"2024-01-10T02:38:38.000Z","featured":true,"_content":"\n[Code](http://www.github.com/OpenBMB/InfiniteBench) | [Paper](https://arxiv.org/abs/2402.13718)\n\nThe first benchmark for evaluating the effectiveness of LLMs in handling more than 100k tokens!\n\n> In the paper, we name it $\\infty$-Bench, but I will sometimes use \"InfiniteBench\" in this blog post for better readability.\n\nFinally got some time to write this blog, been so busy lately! I have been in a fairly long duration of research hiatus, meanwhile the field of NLP has been revolutionized by an overwhelming number of new LLMs. Finally, I was able to arrive at some productive and meaningful work in this new era of research, as a second author. In this blog post, I will introduce this work that I have been working on recently.\n\n<!-- more -->\n\n## Background\n\nThe advent of LLMs have shown many promising results, but many practice applications (e.g., agents, document/webpage reading, long text summarization, etc.) are greatly limited by the context length constraint. Therefore, many works have strived to increase the length of the context that LLMs can accept. However, current \"long-sequence\" benchmarks all fall below 100k tokens, and is therefore not able to evaluate the effectiveness of many long-context LLMs. Our work, $\\infty$-Bench\n\n## The Data\n\nThe data consists of language tasks from diverse domains (math, code, novels), two languages (English and Chinese). Half of the tasks are automatically generated, which is desirable for optionally further scaling the context lengths to any arbitrary lengths.\n\nFollowing shows the statistics of the tasks in our benchmark.\n\n![Data statistics. The angle of segments is proportional to the number of examples, and the radius is proportional to the average example lengths (sum of input and output tokens).](infinitebench/data-stat-pie.png \"Data statistics. The angle of segments is proportional to the number of examples, and the radius is proportional to the average example lengths (sum of input and output tokens).\")\n\n## Results\n\nWe tested SOTA proprietary and open-source LLMs at the time of evaluation. The result is shown below. We can see that in most tasks, the performance is far from satisfactory in practical applications.\n\n![Results of some SOTA long-context LLMs on our InfiniteBench](infinitebench/results.png \"Results of some SOTA long-context LLMs on our InfiniteBench\")\n\n## Thoughts on the Future of Long-Context Research\n\nOur lab have been investing much efforts in long-context LLMs lately. Particularly, we are interested in developing LLMs that can accept infinite input lengths, or what some of my colleagues call streaming language models (i.e., models that operate on streaming inputs). I believe that the transformer architecture is inherently incapable of processing infinite-length inputs. This is the research direction that I have been focusing on.\n\n### Inherent Limitations of Transformers\n\nThe most obvious reason is the quadratic complexity. A large number of research papers have focused on reducing the computational cost of the self-attention mechanism. But most SOTA LLMs at the moment are still dense attention layers, and rely on using Flash-Attention to speedup the computation. However, through discussion with various researchers, I have found that many people now believe that the attention computation is fast enough for most practical applications. I strongly disagree with this view. Firstly, the computational cost translates to the operational cost and emission that results from the usage of LLMs, which is of great concern. Secondly, my experience with ChatGPT (especially when using GPT-4) is that it is often not fast enough. Especially when it tends to produce many irrelevant lead-up sentences, I often find that I can find the answer using search engines before ChatGPT gives me the answer. Thirdly, current LLMs typically only have less than 100k in context length, however, as we apply them on contexts with millions of tokens, the speed of processing these tokens becomes unacceptable in most applications. For instance, in our experiments with InfiniteBench, applying a 7B model on 128k tokens using one A100 GPU takes 8~11 minutes to simply read the input^[I know that this is much faster than humans, but we expect AI to be faster than humans, especially considering they cost so much power to run.]. \n\n> This is not the only drawback of the transformer architecture, but that is out of the scope of this discussion.\n\n### Possible Paths\n\nI do not believe making small tweaks to the self-attention mechanism will solve the problem. Yep, we need new model architectures. Two architectures that I find promising are **linear attention** and **state-space models** (SSMs). Since these architectures have been widely discussed in the research community, I will not describe them in detail here. Instead, I want to express my opinion on the future of these architectures.\n\nI like to think of different language model architectures from the perspective of compressing the history^[This perspective is not new and has been discussed in many papers.]. The way transformers work is that they feed the last $L$ tokens without any compression to the model in each step. This means that the model remembers everything perfectly up to $L$ previous tokens, and remembers **nothing** about the history before that.\n\nIn contrast, SSM and models with linear attention can learn to automatically choose what information retain about the past, which more closely resembles how humans memorize, and provides a smoother curve of forgettance (which is likely beneficial because I think that a blurry remembrance is much better than complete forgettal beyond $n$ tokens). I firmly believe that this is the right direction to go. We are very likely to see a surge of LLMs with $O(1)$ inference cost (for one token) in the upcoming five years, and this can drastically reduce the computational costs and increase their applicability in real-world applications.\n\nBy now, some poeple are urgent to say that \"but recurrent models are much weaker than transformers\". The thing is, most of such comparison are done in settings where the input does not exceed the context window of the transformer models. In other words, we only evaluate transformers on cases where it has perfect memory. In fact, I believe that within the context windows of a transformer model, it should be the upper bounds for the performance of recurrent models, which holds a lossful compression of the window. Moreover, I think that further research down the line can drastically improve the performance of recurrent models (actually any possible linear language models) over self-attention-based language models.\n\nAnother thing to note is that, I have noticed that people like to align the parameter count of different LLMs during comparison, but for models with different architecture, this is a bad practice. In practice, we likely care more about the cost of maintenance, the speed of inference or training, and memory usage, etc. For instance, [RetNet](https://arxiv.org/abs/2307.08621)'s training throughput is actually faster than a transformer with [Flash-Attention](https://github.com/Dao-AILab/flash-attention). Imagine how fast RetNet + Flash-Attention can be. For applications, if a model is 10x faster than ChatGPT, but just slightly underperforms it, it is very likely that I will choose that over ChatGPT.\n\n## Additional Notes\n\nI am currently working on a linear attention model, but the field is changing so fast. I expect that this project will end within the next three months, because if not, my ideas will likely become obselete due to new works being released. Stay tuned.\n","source":"_posts/infinitebench.md","raw":"---\nauthor: 陈英发 Yingfa Chen\ntitle: \"InfiniteBench: Extending Long Context Evaluation Beyond 100K Tokens\"\ndate: 2024-01-10 10:38:38\ncategories: Research\ntags:\n- research\n- llm\n- nlp\n- long-context\n- benchmark\n- recurrence\n- linear-attention\n- transformer\nfeatured: true\n---\n\n[Code](http://www.github.com/OpenBMB/InfiniteBench) | [Paper](https://arxiv.org/abs/2402.13718)\n\nThe first benchmark for evaluating the effectiveness of LLMs in handling more than 100k tokens!\n\n> In the paper, we name it $\\infty$-Bench, but I will sometimes use \"InfiniteBench\" in this blog post for better readability.\n\nFinally got some time to write this blog, been so busy lately! I have been in a fairly long duration of research hiatus, meanwhile the field of NLP has been revolutionized by an overwhelming number of new LLMs. Finally, I was able to arrive at some productive and meaningful work in this new era of research, as a second author. In this blog post, I will introduce this work that I have been working on recently.\n\n<!-- more -->\n\n## Background\n\nThe advent of LLMs have shown many promising results, but many practice applications (e.g., agents, document/webpage reading, long text summarization, etc.) are greatly limited by the context length constraint. Therefore, many works have strived to increase the length of the context that LLMs can accept. However, current \"long-sequence\" benchmarks all fall below 100k tokens, and is therefore not able to evaluate the effectiveness of many long-context LLMs. Our work, $\\infty$-Bench\n\n## The Data\n\nThe data consists of language tasks from diverse domains (math, code, novels), two languages (English and Chinese). Half of the tasks are automatically generated, which is desirable for optionally further scaling the context lengths to any arbitrary lengths.\n\nFollowing shows the statistics of the tasks in our benchmark.\n\n![Data statistics. The angle of segments is proportional to the number of examples, and the radius is proportional to the average example lengths (sum of input and output tokens).](infinitebench/data-stat-pie.png \"Data statistics. The angle of segments is proportional to the number of examples, and the radius is proportional to the average example lengths (sum of input and output tokens).\")\n\n## Results\n\nWe tested SOTA proprietary and open-source LLMs at the time of evaluation. The result is shown below. We can see that in most tasks, the performance is far from satisfactory in practical applications.\n\n![Results of some SOTA long-context LLMs on our InfiniteBench](infinitebench/results.png \"Results of some SOTA long-context LLMs on our InfiniteBench\")\n\n## Thoughts on the Future of Long-Context Research\n\nOur lab have been investing much efforts in long-context LLMs lately. Particularly, we are interested in developing LLMs that can accept infinite input lengths, or what some of my colleagues call streaming language models (i.e., models that operate on streaming inputs). I believe that the transformer architecture is inherently incapable of processing infinite-length inputs. This is the research direction that I have been focusing on.\n\n### Inherent Limitations of Transformers\n\nThe most obvious reason is the quadratic complexity. A large number of research papers have focused on reducing the computational cost of the self-attention mechanism. But most SOTA LLMs at the moment are still dense attention layers, and rely on using Flash-Attention to speedup the computation. However, through discussion with various researchers, I have found that many people now believe that the attention computation is fast enough for most practical applications. I strongly disagree with this view. Firstly, the computational cost translates to the operational cost and emission that results from the usage of LLMs, which is of great concern. Secondly, my experience with ChatGPT (especially when using GPT-4) is that it is often not fast enough. Especially when it tends to produce many irrelevant lead-up sentences, I often find that I can find the answer using search engines before ChatGPT gives me the answer. Thirdly, current LLMs typically only have less than 100k in context length, however, as we apply them on contexts with millions of tokens, the speed of processing these tokens becomes unacceptable in most applications. For instance, in our experiments with InfiniteBench, applying a 7B model on 128k tokens using one A100 GPU takes 8~11 minutes to simply read the input^[I know that this is much faster than humans, but we expect AI to be faster than humans, especially considering they cost so much power to run.]. \n\n> This is not the only drawback of the transformer architecture, but that is out of the scope of this discussion.\n\n### Possible Paths\n\nI do not believe making small tweaks to the self-attention mechanism will solve the problem. Yep, we need new model architectures. Two architectures that I find promising are **linear attention** and **state-space models** (SSMs). Since these architectures have been widely discussed in the research community, I will not describe them in detail here. Instead, I want to express my opinion on the future of these architectures.\n\nI like to think of different language model architectures from the perspective of compressing the history^[This perspective is not new and has been discussed in many papers.]. The way transformers work is that they feed the last $L$ tokens without any compression to the model in each step. This means that the model remembers everything perfectly up to $L$ previous tokens, and remembers **nothing** about the history before that.\n\nIn contrast, SSM and models with linear attention can learn to automatically choose what information retain about the past, which more closely resembles how humans memorize, and provides a smoother curve of forgettance (which is likely beneficial because I think that a blurry remembrance is much better than complete forgettal beyond $n$ tokens). I firmly believe that this is the right direction to go. We are very likely to see a surge of LLMs with $O(1)$ inference cost (for one token) in the upcoming five years, and this can drastically reduce the computational costs and increase their applicability in real-world applications.\n\nBy now, some poeple are urgent to say that \"but recurrent models are much weaker than transformers\". The thing is, most of such comparison are done in settings where the input does not exceed the context window of the transformer models. In other words, we only evaluate transformers on cases where it has perfect memory. In fact, I believe that within the context windows of a transformer model, it should be the upper bounds for the performance of recurrent models, which holds a lossful compression of the window. Moreover, I think that further research down the line can drastically improve the performance of recurrent models (actually any possible linear language models) over self-attention-based language models.\n\nAnother thing to note is that, I have noticed that people like to align the parameter count of different LLMs during comparison, but for models with different architecture, this is a bad practice. In practice, we likely care more about the cost of maintenance, the speed of inference or training, and memory usage, etc. For instance, [RetNet](https://arxiv.org/abs/2307.08621)'s training throughput is actually faster than a transformer with [Flash-Attention](https://github.com/Dao-AILab/flash-attention). Imagine how fast RetNet + Flash-Attention can be. For applications, if a model is 10x faster than ChatGPT, but just slightly underperforms it, it is very likely that I will choose that over ChatGPT.\n\n## Additional Notes\n\nI am currently working on a linear attention model, but the field is changing so fast. I expect that this project will end within the next three months, because if not, my ideas will likely become obselete due to new works being released. Stay tuned.\n","slug":"infinitebench","published":1,"updated":"2024-02-26T03:07:04.673Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cltl4oxeh0009xh7k6tym7gjd","content":"<p><a href=\"http://www.github.com/OpenBMB/InfiniteBench\">Code</a> | <a href=\"https://arxiv.org/abs/2402.13718\">Paper</a></p>\n<p>The first benchmark for evaluating the effectiveness of LLMs in handling more than 100k tokens!</p>\n<blockquote>\n<p>In the paper, we name it $\\infty$-Bench, but I will sometimes use \"InfiniteBench\" in this blog post for better readability.</p>\n</blockquote>\n<p>Finally got some time to write this blog, been so busy lately! I have been in a fairly long duration of research hiatus, meanwhile the field of NLP has been revolutionized by an overwhelming number of new LLMs. Finally, I was able to arrive at some productive and meaningful work in this new era of research, as a second author. In this blog post, I will introduce this work that I have been working on recently.</p>\n<span id=\"more\"></span>\n<h2 id=\"Background\">Background</h2>\n<p>The advent of LLMs have shown many promising results, but many practice applications (e.g., agents, document/webpage reading, long text summarization, etc.) are greatly limited by the context length constraint. Therefore, many works have strived to increase the length of the context that LLMs can accept. However, current \"long-sequence\" benchmarks all fall below 100k tokens, and is therefore not able to evaluate the effectiveness of many long-context LLMs. Our work, $\\infty$-Bench</p>\n<h2 id=\"The-Data\">The Data</h2>\n<p>The data consists of language tasks from diverse domains (math, code, novels), two languages (English and Chinese). Half of the tasks are automatically generated, which is desirable for optionally further scaling the context lengths to any arbitrary lengths.</p>\n<p>Following shows the statistics of the tasks in our benchmark.</p>\n<p><img src=\"/2024/01/10/infinitebench/data-stat-pie.png\" alt=\"Data statistics. The angle of segments is proportional to the number of examples, and the radius is proportional to the average example lengths (sum of input and output tokens).\" title=\"Data statistics. The angle of segments is proportional to the number of examples, and the radius is proportional to the average example lengths (sum of input and output tokens).\"></p>\n<h2 id=\"Results\">Results</h2>\n<p>We tested SOTA proprietary and open-source LLMs at the time of evaluation. The result is shown below. We can see that in most tasks, the performance is far from satisfactory in practical applications.</p>\n<p><img src=\"/2024/01/10/infinitebench/results.png\" alt=\"Results of some SOTA long-context LLMs on our InfiniteBench\" title=\"Results of some SOTA long-context LLMs on our InfiniteBench\"></p>\n<h2 id=\"Thoughts-on-the-Future-of-Long-Context-Research\">Thoughts on the Future of Long-Context Research</h2>\n<p>Our lab have been investing much efforts in long-context LLMs lately. Particularly, we are interested in developing LLMs that can accept infinite input lengths, or what some of my colleagues call streaming language models (i.e., models that operate on streaming inputs). I believe that the transformer architecture is inherently incapable of processing infinite-length inputs. This is the research direction that I have been focusing on.</p>\n<h3 id=\"Inherent-Limitations-of-Transformers\">Inherent Limitations of Transformers</h3>\n<p>The most obvious reason is the quadratic complexity. A large number of research papers have focused on reducing the computational cost of the self-attention mechanism. But most SOTA LLMs at the moment are still dense attention layers, and rely on using Flash-Attention to speedup the computation. However, through discussion with various researchers, I have found that many people now believe that the attention computation is fast enough for most practical applications. I strongly disagree with this view. Firstly, the computational cost translates to the operational cost and emission that results from the usage of LLMs, which is of great concern. Secondly, my experience with ChatGPT (especially when using GPT-4) is that it is often not fast enough. Especially when it tends to produce many irrelevant lead-up sentences, I often find that I can find the answer using search engines before ChatGPT gives me the answer. Thirdly, current LLMs typically only have less than 100k in context length, however, as we apply them on contexts with millions of tokens, the speed of processing these tokens becomes unacceptable in most applications. For instance, in our experiments with InfiniteBench, applying a 7B model on 128k tokens using one A100 GPU takes 8~11 minutes to simply read the input<sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">[1]</a></sup>.</p>\n<blockquote>\n<p>This is not the only drawback of the transformer architecture, but that is out of the scope of this discussion.</p>\n</blockquote>\n<h3 id=\"Possible-Paths\">Possible Paths</h3>\n<p>I do not believe making small tweaks to the self-attention mechanism will solve the problem. Yep, we need new model architectures. Two architectures that I find promising are <strong>linear attention</strong> and <strong>state-space models</strong> (SSMs). Since these architectures have been widely discussed in the research community, I will not describe them in detail here. Instead, I want to express my opinion on the future of these architectures.</p>\n<p>I like to think of different language model architectures from the perspective of compressing the history<sup class=\"footnote-ref\"><a href=\"#fn2\" id=\"fnref2\">[2]</a></sup>. The way transformers work is that they feed the last $L$ tokens without any compression to the model in each step. This means that the model remembers everything perfectly up to $L$ previous tokens, and remembers <strong>nothing</strong> about the history before that.</p>\n<p>In contrast, SSM and models with linear attention can learn to automatically choose what information retain about the past, which more closely resembles how humans memorize, and provides a smoother curve of forgettance (which is likely beneficial because I think that a blurry remembrance is much better than complete forgettal beyond $n$ tokens). I firmly believe that this is the right direction to go. We are very likely to see a surge of LLMs with $O(1)$ inference cost (for one token) in the upcoming five years, and this can drastically reduce the computational costs and increase their applicability in real-world applications.</p>\n<p>By now, some poeple are urgent to say that \"but recurrent models are much weaker than transformers\". The thing is, most of such comparison are done in settings where the input does not exceed the context window of the transformer models. In other words, we only evaluate transformers on cases where it has perfect memory. In fact, I believe that within the context windows of a transformer model, it should be the upper bounds for the performance of recurrent models, which holds a lossful compression of the window. Moreover, I think that further research down the line can drastically improve the performance of recurrent models (actually any possible linear language models) over self-attention-based language models.</p>\n<p>Another thing to note is that, I have noticed that people like to align the parameter count of different LLMs during comparison, but for models with different architecture, this is a bad practice. In practice, we likely care more about the cost of maintenance, the speed of inference or training, and memory usage, etc. For instance, <a href=\"https://arxiv.org/abs/2307.08621\">RetNet</a>'s training throughput is actually faster than a transformer with <a href=\"https://github.com/Dao-AILab/flash-attention\">Flash-Attention</a>. Imagine how fast RetNet + Flash-Attention can be. For applications, if a model is 10x faster than ChatGPT, but just slightly underperforms it, it is very likely that I will choose that over ChatGPT.</p>\n<h2 id=\"Additional-Notes\">Additional Notes</h2>\n<p>I am currently working on a linear attention model, but the field is changing so fast. I expect that this project will end within the next three months, because if not, my ideas will likely become obselete due to new works being released. Stay tuned.</p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn1\" class=\"footnote-item\"><p>I know that this is much faster than humans, but we expect AI to be faster than humans, especially considering they cost so much power to run. <a href=\"#fnref1\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n<li id=\"fn2\" class=\"footnote-item\"><p>This perspective is not new and has been discussed in many papers. <a href=\"#fnref2\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n</ol>\n</section>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://www.github.com/OpenBMB/InfiniteBench\">Code</a> | <a href=\"https://arxiv.org/abs/2402.13718\">Paper</a></p>\n<p>The first benchmark for evaluating the effectiveness of LLMs in handling more than 100k tokens!</p>\n<blockquote>\n<p>In the paper, we name it $\\infty$-Bench, but I will sometimes use \"InfiniteBench\" in this blog post for better readability.</p>\n</blockquote>\n<p>Finally got some time to write this blog, been so busy lately! I have been in a fairly long duration of research hiatus, meanwhile the field of NLP has been revolutionized by an overwhelming number of new LLMs. Finally, I was able to arrive at some productive and meaningful work in this new era of research, as a second author. In this blog post, I will introduce this work that I have been working on recently.</p>","more":"<h2 id=\"Background\">Background</h2>\n<p>The advent of LLMs have shown many promising results, but many practice applications (e.g., agents, document/webpage reading, long text summarization, etc.) are greatly limited by the context length constraint. Therefore, many works have strived to increase the length of the context that LLMs can accept. However, current &quot;long-sequence&quot; benchmarks all fall below 100k tokens, and is therefore not able to evaluate the effectiveness of many long-context LLMs. Our work, $\\infty$-Bench</p>\n<h2 id=\"The-Data\">The Data</h2>\n<p>The data consists of language tasks from diverse domains (math, code, novels), two languages (English and Chinese). Half of the tasks are automatically generated, which is desirable for optionally further scaling the context lengths to any arbitrary lengths.</p>\n<p>Following shows the statistics of the tasks in our benchmark.</p>\n<p><img src=\"/2024/01/10/infinitebench/data-stat-pie.png\" alt=\"Data statistics. The angle of segments is proportional to the number of examples, and the radius is proportional to the average example lengths (sum of input and output tokens).\" title=\"Data statistics. The angle of segments is proportional to the number of examples, and the radius is proportional to the average example lengths (sum of input and output tokens).\"></p>\n<h2 id=\"Results\">Results</h2>\n<p>We tested SOTA proprietary and open-source LLMs at the time of evaluation. The result is shown below. We can see that in most tasks, the performance is far from satisfactory in practical applications.</p>\n<p><img src=\"/2024/01/10/infinitebench/results.png\" alt=\"Results of some SOTA long-context LLMs on our InfiniteBench\" title=\"Results of some SOTA long-context LLMs on our InfiniteBench\"></p>\n<h2 id=\"Thoughts-on-the-Future-of-Long-Context-Research\">Thoughts on the Future of Long-Context Research</h2>\n<p>Our lab have been investing much efforts in long-context LLMs lately. Particularly, we are interested in developing LLMs that can accept infinite input lengths, or what some of my colleagues call streaming language models (i.e., models that operate on streaming inputs). I believe that the transformer architecture is inherently incapable of processing infinite-length inputs. This is the research direction that I have been focusing on.</p>\n<h3 id=\"Inherent-Limitations-of-Transformers\">Inherent Limitations of Transformers</h3>\n<p>The most obvious reason is the quadratic complexity. A large number of research papers have focused on reducing the computational cost of the self-attention mechanism. But most SOTA LLMs at the moment are still dense attention layers, and rely on using Flash-Attention to speedup the computation. However, through discussion with various researchers, I have found that many people now believe that the attention computation is fast enough for most practical applications. I strongly disagree with this view. Firstly, the computational cost translates to the operational cost and emission that results from the usage of LLMs, which is of great concern. Secondly, my experience with ChatGPT (especially when using GPT-4) is that it is often not fast enough. Especially when it tends to produce many irrelevant lead-up sentences, I often find that I can find the answer using search engines before ChatGPT gives me the answer. Thirdly, current LLMs typically only have less than 100k in context length, however, as we apply them on contexts with millions of tokens, the speed of processing these tokens becomes unacceptable in most applications. For instance, in our experiments with InfiniteBench, applying a 7B model on 128k tokens using one A100 GPU takes 8~11 minutes to simply read the input<sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">[1]</a></sup>.</p>\n<blockquote>\n<p>This is not the only drawback of the transformer architecture, but that is out of the scope of this discussion.</p>\n</blockquote>\n<h3 id=\"Possible-Paths\">Possible Paths</h3>\n<p>I do not believe making small tweaks to the self-attention mechanism will solve the problem. Yep, we need new model architectures. Two architectures that I find promising are <strong>linear attention</strong> and <strong>state-space models</strong> (SSMs). Since these architectures have been widely discussed in the research community, I will not describe them in detail here. Instead, I want to express my opinion on the future of these architectures.</p>\n<p>I like to think of different language model architectures from the perspective of compressing the history<sup class=\"footnote-ref\"><a href=\"#fn2\" id=\"fnref2\">[2]</a></sup>. The way transformers work is that they feed the last $L$ tokens without any compression to the model in each step. This means that the model remembers everything perfectly up to $L$ previous tokens, and remembers <strong>nothing</strong> about the history before that.</p>\n<p>In contrast, SSM and models with linear attention can learn to automatically choose what information retain about the past, which more closely resembles how humans memorize, and provides a smoother curve of forgettance (which is likely beneficial because I think that a blurry remembrance is much better than complete forgettal beyond $n$ tokens). I firmly believe that this is the right direction to go. We are very likely to see a surge of LLMs with $O(1)$ inference cost (for one token) in the upcoming five years, and this can drastically reduce the computational costs and increase their applicability in real-world applications.</p>\n<p>By now, some poeple are urgent to say that &quot;but recurrent models are much weaker than transformers&quot;. The thing is, most of such comparison are done in settings where the input does not exceed the context window of the transformer models. In other words, we only evaluate transformers on cases where it has perfect memory. In fact, I believe that within the context windows of a transformer model, it should be the upper bounds for the performance of recurrent models, which holds a lossful compression of the window. Moreover, I think that further research down the line can drastically improve the performance of recurrent models (actually any possible linear language models) over self-attention-based language models.</p>\n<p>Another thing to note is that, I have noticed that people like to align the parameter count of different LLMs during comparison, but for models with different architecture, this is a bad practice. In practice, we likely care more about the cost of maintenance, the speed of inference or training, and memory usage, etc. For instance, <a href=\"https://arxiv.org/abs/2307.08621\">RetNet</a>'s training throughput is actually faster than a transformer with <a href=\"https://github.com/Dao-AILab/flash-attention\">Flash-Attention</a>. Imagine how fast RetNet + Flash-Attention can be. For applications, if a model is 10x faster than ChatGPT, but just slightly underperforms it, it is very likely that I will choose that over ChatGPT.</p>\n<h2 id=\"Additional-Notes\">Additional Notes</h2>\n<p>I am currently working on a linear attention model, but the field is changing so fast. I expect that this project will end within the next three months, because if not, my ideas will likely become obselete due to new works being released. Stay tuned.</p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn1\" class=\"footnote-item\"><p>I know that this is much faster than humans, but we expect AI to be faster than humans, especially considering they cost so much power to run. <a href=\"#fnref1\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n<li id=\"fn2\" class=\"footnote-item\"><p>This perspective is not new and has been discussed in many papers. <a href=\"#fnref2\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n</ol>\n</section>"},{"author":"陈英发 Yingfa Chen","title":"Interpreting a Maze-Solving Network","date":"2023-10-07T10:03:10.000Z","_content":"\n[The blog post](https://www.lesswrong.com/s/sCGfFb5DPfjEmtEdn)\n\nI can't believe I haven't read this until now. This is mind-provoking, and the result is an important step towards understanding neural networks.\n\n<!-- more -->\n\nThe culmination of this blog post is the exciting work of [Activation Addition](/2023/10/07/actadd/), which I believe is one important work that inspired the recently [Representation Engineering](https://arxiv.org/abs/2310.01405) work.\n","source":"_posts/interpreting-a-maze-solving-network.md","raw":"---\nauthor: 陈英发 Yingfa Chen\ntitle: Interpreting a Maze-Solving Network\ndate: 2023-10-07 18:03:10\ncategories: Thoughts\ntags:\n- english\n- activation-engineering\n- representation-engineering\n- interpretability\n- rl\n- alignment\n- llm\n- maze\n---\n\n[The blog post](https://www.lesswrong.com/s/sCGfFb5DPfjEmtEdn)\n\nI can't believe I haven't read this until now. This is mind-provoking, and the result is an important step towards understanding neural networks.\n\n<!-- more -->\n\nThe culmination of this blog post is the exciting work of [Activation Addition](/2023/10/07/actadd/), which I believe is one important work that inspired the recently [Representation Engineering](https://arxiv.org/abs/2310.01405) work.\n","slug":"interpreting-a-maze-solving-network","published":1,"updated":"2024-01-11T05:41:49.088Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cltl4oxeh000bxh7k5kpl1zp0","content":"<p><a href=\"https://www.lesswrong.com/s/sCGfFb5DPfjEmtEdn\">The blog post</a></p>\n<p>I can't believe I haven't read this until now. This is mind-provoking, and the result is an important step towards understanding neural networks.</p>\n<span id=\"more\"></span>\n<p>The culmination of this blog post is the exciting work of <a href=\"/2023/10/07/actadd/\">Activation Addition</a>, which I believe is one important work that inspired the recently <a href=\"https://arxiv.org/abs/2310.01405\">Representation Engineering</a> work.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"https://www.lesswrong.com/s/sCGfFb5DPfjEmtEdn\">The blog post</a></p>\n<p>I can't believe I haven't read this until now. This is mind-provoking, and the result is an important step towards understanding neural networks.</p>","more":"<p>The culmination of this blog post is the exciting work of <a href=\"/2023/10/07/actadd/\">Activation Addition</a>, which I believe is one important work that inspired the recently <a href=\"https://arxiv.org/abs/2310.01405\">Representation Engineering</a> work.</p>"},{"title":"Safety and Ethical Concerns of Large Language Models","date":"2023-09-19T10:13:06.000Z","author":"陈英发 Yingfa Chen","_content":"\nI will be holding a seminar at ModelBest (面壁智能) in Sep 20, 2023 in Beijing, Haidian, 科技园. The seminar will be in Chinese, and it's called \"大模型安全与伦理问题\" (translation: Safety and Ethical Concerns of Large Language Models). Below is a list of references.\n\n<!-- more -->\n\n## Introduction\n\n- Galactica: A Large Language Model for Science\n- https://openai.com/research/gpt-4\n- SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions\n- Bias and Fairness in Large Language Models: A Survey\n- A Survey of Safety and Trustworthiness of Large Language Models through the Lens of Verification and Validation\n\n## Evaluation Methods\n\n- A General Language Assistant as a Laboratory for Alignment, Anthropic\n- Safety Assessment of Chinese Large Language Models\n- Semantics derived automatically from language corpora contain human-like biases\n- StereoSet: Measuring stereotypical bias in pretrained language models\n\n### Instruction Attacks\n\n- Toxicity in CHATGPT: Analyzing Persona-assigned Language Models ⭐️\n- Large Language Models are Zero-Shot Reasoners ⭐️\n- On Second Thought, Let’s Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning ⭐️\n- Prompting GPT-3 To Be Reliable\n- Universal and Transferable Adversarial Attacks on Aligned Language Models ⭐️\n- Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment ⭐️⭐️\n\n### Exaggerated Safety\n\n- XSTEST: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models ⭐️\n- Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions ⭐️\n\n## Alignment Methods\n\n- Aligning language models to follow instructions ⭐️\n- Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback ⭐️\n- SELF-INSTRUCT: Aligning Language Models with Self-Generated Instructions ⭐️⭐️\n- Pretraining Language Models with Human Preferences ⭐️\n- LIMA: Less Is More for Alignment\n- https://openai.com/blog/our-approach-to-alignment-research (Aug 2022)\n- https://openai.com/blog/our-approach-to-alignment-research (Jul 2023) ⭐️\n\n\n⭐️: important\n\n⭐️⭐️: very important\n\n## My Thoughts\n\nAI alignment is extremely important, and we know very little about it right now. In my everyday use of ChatGPT, it occasionally refuses to help me. This is presumably because it thinks that assisting me is harmful, while it's actually not. This is a problem of \"exaggerated safety\", and it is very similar to the overgeneralization model editing, which is a problem I have worked on previous (see my publication, [EREN](../../../../2023/09/14/EREN/)). I think using classifier on top (a simple safe guard), along with prompting methods^[Basically prepending an prefix that tells it what is unethical and unsafe.] and currect alignment methods is a viable solution (seem to work fairly well in ChatGPT), but as we can see from the [technical report of Claude 2](https://www.anthropic.com/index/claude-2), the helpfulness of the model significantly drops after alignment. Therefore, I think minimizing the sacrifice in helpfulness will be an important direction of future research.\n\nAnother concern is that there is no concensus on the goal of alignment. In fact, many people think that the fact that role-playing can be used to jailbreak alignment is not a bad thing per se, especially regarding toxicity, because if the user explicitly tells the AI to role-play a person that slurs a lot, the user expects slurs (one kind of toxicity).\n\nMoreover, the entire meaning of alignment research might be undermined by the fact that AI system can be unaligned pretty easily. This concern is specially severe for works that focus on reducing the cost of alignment, because the same techniques might be used to effectively unalign AI systems.^[Does there exist a way to make AI impossible to unalign? This reminds me of the \"mind stamping\" (Chinese: 思想钢印) from the Three Body Problem, a novel by Liu Cixin.]\n\nAll in all, AI alignment is a sub-field of better controllability of AI system, and I can foresee that it will be a hot research topic for the upcoming five years.\n","source":"_posts/llm-safety-and-ethics.md","raw":"---\ntitle: \"Safety and Ethical Concerns of Large Language Models\"\ndate: 2023-09-19 18:13:06\ncategories: Thoughts\nauthor: 陈英发 Yingfa Chen\ntags:\n- english\n- research\n- llm\n- machine-learning\n- ethics\n- safety\n- ai-alignment\n- 面壁智能\n- modelbest\n- life\n- tutorial\n- eren\n- chatgpt\n- claude\n- 三体\n---\n\nI will be holding a seminar at ModelBest (面壁智能) in Sep 20, 2023 in Beijing, Haidian, 科技园. The seminar will be in Chinese, and it's called \"大模型安全与伦理问题\" (translation: Safety and Ethical Concerns of Large Language Models). Below is a list of references.\n\n<!-- more -->\n\n## Introduction\n\n- Galactica: A Large Language Model for Science\n- https://openai.com/research/gpt-4\n- SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions\n- Bias and Fairness in Large Language Models: A Survey\n- A Survey of Safety and Trustworthiness of Large Language Models through the Lens of Verification and Validation\n\n## Evaluation Methods\n\n- A General Language Assistant as a Laboratory for Alignment, Anthropic\n- Safety Assessment of Chinese Large Language Models\n- Semantics derived automatically from language corpora contain human-like biases\n- StereoSet: Measuring stereotypical bias in pretrained language models\n\n### Instruction Attacks\n\n- Toxicity in CHATGPT: Analyzing Persona-assigned Language Models ⭐️\n- Large Language Models are Zero-Shot Reasoners ⭐️\n- On Second Thought, Let’s Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning ⭐️\n- Prompting GPT-3 To Be Reliable\n- Universal and Transferable Adversarial Attacks on Aligned Language Models ⭐️\n- Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment ⭐️⭐️\n\n### Exaggerated Safety\n\n- XSTEST: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models ⭐️\n- Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions ⭐️\n\n## Alignment Methods\n\n- Aligning language models to follow instructions ⭐️\n- Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback ⭐️\n- SELF-INSTRUCT: Aligning Language Models with Self-Generated Instructions ⭐️⭐️\n- Pretraining Language Models with Human Preferences ⭐️\n- LIMA: Less Is More for Alignment\n- https://openai.com/blog/our-approach-to-alignment-research (Aug 2022)\n- https://openai.com/blog/our-approach-to-alignment-research (Jul 2023) ⭐️\n\n\n⭐️: important\n\n⭐️⭐️: very important\n\n## My Thoughts\n\nAI alignment is extremely important, and we know very little about it right now. In my everyday use of ChatGPT, it occasionally refuses to help me. This is presumably because it thinks that assisting me is harmful, while it's actually not. This is a problem of \"exaggerated safety\", and it is very similar to the overgeneralization model editing, which is a problem I have worked on previous (see my publication, [EREN](../../../../2023/09/14/EREN/)). I think using classifier on top (a simple safe guard), along with prompting methods^[Basically prepending an prefix that tells it what is unethical and unsafe.] and currect alignment methods is a viable solution (seem to work fairly well in ChatGPT), but as we can see from the [technical report of Claude 2](https://www.anthropic.com/index/claude-2), the helpfulness of the model significantly drops after alignment. Therefore, I think minimizing the sacrifice in helpfulness will be an important direction of future research.\n\nAnother concern is that there is no concensus on the goal of alignment. In fact, many people think that the fact that role-playing can be used to jailbreak alignment is not a bad thing per se, especially regarding toxicity, because if the user explicitly tells the AI to role-play a person that slurs a lot, the user expects slurs (one kind of toxicity).\n\nMoreover, the entire meaning of alignment research might be undermined by the fact that AI system can be unaligned pretty easily. This concern is specially severe for works that focus on reducing the cost of alignment, because the same techniques might be used to effectively unalign AI systems.^[Does there exist a way to make AI impossible to unalign? This reminds me of the \"mind stamping\" (Chinese: 思想钢印) from the Three Body Problem, a novel by Liu Cixin.]\n\nAll in all, AI alignment is a sub-field of better controllability of AI system, and I can foresee that it will be a hot research topic for the upcoming five years.\n","slug":"llm-safety-and-ethics","published":1,"updated":"2024-01-10T07:24:56.515Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cltl4oxei000fxh7k1qa8gbtd","content":"<p>I will be holding a seminar at ModelBest (面壁智能) in Sep 20, 2023 in Beijing, Haidian, 科技园. The seminar will be in Chinese, and it's called \"大模型安全与伦理问题\" (translation: Safety and Ethical Concerns of Large Language Models). Below is a list of references.</p>\n<span id=\"more\"></span>\n<h2 id=\"Introduction\">Introduction</h2>\n<ul>\n<li>Galactica: A Large Language Model for Science</li>\n<li><a href=\"https://openai.com/research/gpt-4\">https://openai.com/research/gpt-4</a></li>\n<li>SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions</li>\n<li>Bias and Fairness in Large Language Models: A Survey</li>\n<li>A Survey of Safety and Trustworthiness of Large Language Models through the Lens of Verification and Validation</li>\n</ul>\n<h2 id=\"Evaluation-Methods\">Evaluation Methods</h2>\n<ul>\n<li>A General Language Assistant as a Laboratory for Alignment, Anthropic</li>\n<li>Safety Assessment of Chinese Large Language Models</li>\n<li>Semantics derived automatically from language corpora contain human-like biases</li>\n<li>StereoSet: Measuring stereotypical bias in pretrained language models</li>\n</ul>\n<h3 id=\"Instruction-Attacks\">Instruction Attacks</h3>\n<ul>\n<li>Toxicity in CHATGPT: Analyzing Persona-assigned Language Models ⭐️</li>\n<li>Large Language Models are Zero-Shot Reasoners ⭐️</li>\n<li>On Second Thought, Let’s Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning ⭐️</li>\n<li>Prompting GPT-3 To Be Reliable</li>\n<li>Universal and Transferable Adversarial Attacks on Aligned Language Models ⭐️</li>\n<li>Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment ⭐️⭐️</li>\n</ul>\n<h3 id=\"Exaggerated-Safety\">Exaggerated Safety</h3>\n<ul>\n<li>XSTEST: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models ⭐️</li>\n<li>Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions ⭐️</li>\n</ul>\n<h2 id=\"Alignment-Methods\">Alignment Methods</h2>\n<ul>\n<li>Aligning language models to follow instructions ⭐️</li>\n<li>Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback ⭐️</li>\n<li>SELF-INSTRUCT: Aligning Language Models with Self-Generated Instructions ⭐️⭐️</li>\n<li>Pretraining Language Models with Human Preferences ⭐️</li>\n<li>LIMA: Less Is More for Alignment</li>\n<li><a href=\"https://openai.com/blog/our-approach-to-alignment-research\">https://openai.com/blog/our-approach-to-alignment-research</a> (Aug 2022)</li>\n<li><a href=\"https://openai.com/blog/our-approach-to-alignment-research\">https://openai.com/blog/our-approach-to-alignment-research</a> (Jul 2023) ⭐️</li>\n</ul>\n<p>⭐️: important</p>\n<p>⭐️⭐️: very important</p>\n<h2 id=\"My-Thoughts\">My Thoughts</h2>\n<p>AI alignment is extremely important, and we know very little about it right now. In my everyday use of ChatGPT, it occasionally refuses to help me. This is presumably because it thinks that assisting me is harmful, while it's actually not. This is a problem of \"exaggerated safety\", and it is very similar to the overgeneralization model editing, which is a problem I have worked on previous (see my publication, <a href=\"../../../../2023/09/14/EREN/\">EREN</a>). I think using classifier on top (a simple safe guard), along with prompting methods<sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">[1]</a></sup> and currect alignment methods is a viable solution (seem to work fairly well in ChatGPT), but as we can see from the <a href=\"https://www.anthropic.com/index/claude-2\">technical report of Claude 2</a>, the helpfulness of the model significantly drops after alignment. Therefore, I think minimizing the sacrifice in helpfulness will be an important direction of future research.</p>\n<p>Another concern is that there is no concensus on the goal of alignment. In fact, many people think that the fact that role-playing can be used to jailbreak alignment is not a bad thing per se, especially regarding toxicity, because if the user explicitly tells the AI to role-play a person that slurs a lot, the user expects slurs (one kind of toxicity).</p>\n<p>Moreover, the entire meaning of alignment research might be undermined by the fact that AI system can be unaligned pretty easily. This concern is specially severe for works that focus on reducing the cost of alignment, because the same techniques might be used to effectively unalign AI systems.<sup class=\"footnote-ref\"><a href=\"#fn2\" id=\"fnref2\">[2]</a></sup></p>\n<p>All in all, AI alignment is a sub-field of better controllability of AI system, and I can foresee that it will be a hot research topic for the upcoming five years.</p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn1\" class=\"footnote-item\"><p>Basically prepending an prefix that tells it what is unethical and unsafe. <a href=\"#fnref1\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n<li id=\"fn2\" class=\"footnote-item\"><p>Does there exist a way to make AI impossible to unalign? This reminds me of the \"mind stamping\" (Chinese: 思想钢印) from the Three Body Problem, a novel by Liu Cixin. <a href=\"#fnref2\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n</ol>\n</section>\n","site":{"data":{}},"excerpt":"<p>I will be holding a seminar at ModelBest (面壁智能) in Sep 20, 2023 in Beijing, Haidian, 科技园. The seminar will be in Chinese, and it's called \"大模型安全与伦理问题\" (translation: Safety and Ethical Concerns of Large Language Models). Below is a list of references.</p>","more":"<h2 id=\"Introduction\">Introduction</h2>\n<ul>\n<li>Galactica: A Large Language Model for Science</li>\n<li><a href=\"https://openai.com/research/gpt-4\">https://openai.com/research/gpt-4</a></li>\n<li>SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions</li>\n<li>Bias and Fairness in Large Language Models: A Survey</li>\n<li>A Survey of Safety and Trustworthiness of Large Language Models through the Lens of Verification and Validation</li>\n</ul>\n<h2 id=\"Evaluation-Methods\">Evaluation Methods</h2>\n<ul>\n<li>A General Language Assistant as a Laboratory for Alignment, Anthropic</li>\n<li>Safety Assessment of Chinese Large Language Models</li>\n<li>Semantics derived automatically from language corpora contain human-like biases</li>\n<li>StereoSet: Measuring stereotypical bias in pretrained language models</li>\n</ul>\n<h3 id=\"Instruction-Attacks\">Instruction Attacks</h3>\n<ul>\n<li>Toxicity in CHATGPT: Analyzing Persona-assigned Language Models ⭐️</li>\n<li>Large Language Models are Zero-Shot Reasoners ⭐️</li>\n<li>On Second Thought, Let’s Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning ⭐️</li>\n<li>Prompting GPT-3 To Be Reliable</li>\n<li>Universal and Transferable Adversarial Attacks on Aligned Language Models ⭐️</li>\n<li>Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment ⭐️⭐️</li>\n</ul>\n<h3 id=\"Exaggerated-Safety\">Exaggerated Safety</h3>\n<ul>\n<li>XSTEST: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models ⭐️</li>\n<li>Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions ⭐️</li>\n</ul>\n<h2 id=\"Alignment-Methods\">Alignment Methods</h2>\n<ul>\n<li>Aligning language models to follow instructions ⭐️</li>\n<li>Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback ⭐️</li>\n<li>SELF-INSTRUCT: Aligning Language Models with Self-Generated Instructions ⭐️⭐️</li>\n<li>Pretraining Language Models with Human Preferences ⭐️</li>\n<li>LIMA: Less Is More for Alignment</li>\n<li><a href=\"https://openai.com/blog/our-approach-to-alignment-research\">https://openai.com/blog/our-approach-to-alignment-research</a> (Aug 2022)</li>\n<li><a href=\"https://openai.com/blog/our-approach-to-alignment-research\">https://openai.com/blog/our-approach-to-alignment-research</a> (Jul 2023) ⭐️</li>\n</ul>\n<p>⭐️: important</p>\n<p>⭐️⭐️: very important</p>\n<h2 id=\"My-Thoughts\">My Thoughts</h2>\n<p>AI alignment is extremely important, and we know very little about it right now. In my everyday use of ChatGPT, it occasionally refuses to help me. This is presumably because it thinks that assisting me is harmful, while it's actually not. This is a problem of &quot;exaggerated safety&quot;, and it is very similar to the overgeneralization model editing, which is a problem I have worked on previous (see my publication, <a href=\"../../../../2023/09/14/EREN/\">EREN</a>). I think using classifier on top (a simple safe guard), along with prompting methods<sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">[1]</a></sup> and currect alignment methods is a viable solution (seem to work fairly well in ChatGPT), but as we can see from the <a href=\"https://www.anthropic.com/index/claude-2\">technical report of Claude 2</a>, the helpfulness of the model significantly drops after alignment. Therefore, I think minimizing the sacrifice in helpfulness will be an important direction of future research.</p>\n<p>Another concern is that there is no concensus on the goal of alignment. In fact, many people think that the fact that role-playing can be used to jailbreak alignment is not a bad thing per se, especially regarding toxicity, because if the user explicitly tells the AI to role-play a person that slurs a lot, the user expects slurs (one kind of toxicity).</p>\n<p>Moreover, the entire meaning of alignment research might be undermined by the fact that AI system can be unaligned pretty easily. This concern is specially severe for works that focus on reducing the cost of alignment, because the same techniques might be used to effectively unalign AI systems.<sup class=\"footnote-ref\"><a href=\"#fn2\" id=\"fnref2\">[2]</a></sup></p>\n<p>All in all, AI alignment is a sub-field of better controllability of AI system, and I can foresee that it will be a hot research topic for the upcoming five years.</p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn1\" class=\"footnote-item\"><p>Basically prepending an prefix that tells it what is unethical and unsafe. <a href=\"#fnref1\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n<li id=\"fn2\" class=\"footnote-item\"><p>Does there exist a way to make AI impossible to unalign? This reminds me of the &quot;mind stamping&quot; (Chinese: 思想钢印) from the Three Body Problem, a novel by Liu Cixin. <a href=\"#fnref2\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n</ol>\n</section>"},{"title":"Some Binary Search","date":"2023-09-14T11:31:09.000Z","_content":"\nA binary search with C++:\n\n```c++\ntemplate<class T>\nint bin_search(vector<T>& arr, T target) {\n    int left = 0, right = arr.size() - 1;\n    while (left <= right) {\n        int mid = (left + right) / 2;\n        if (arr[mid] == target) {\n            break;\n        } else if (arr[mid] < target) {\n            left = mid + 1;\n        } else {\n            right = mid - 1;\n        }\n    }\n    return left;\n}\n```\n\nThe same thing with Rust:\n\n```rust\nfn bin_search<T: Ord>(arr: &Vec<T>, target: &T) -> usize {\n    let mut left = 0;\n    let mut right = arr.len() - 1;\n    while left <= right {\n        let mid = (left + right) / 2;\n        if arr[mid] == *target {\n            break;\n        } else if arr[mid] < *target {\n            left = mid + 1;\n        } else {\n            right = mid - 1;\n        }\n    }\n    left\n}\n```\n\nAnd with Python:\n\n```python\ndef bin_search(arr: list, target):\n    left = 0\n    right = len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] == target:\n            break\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return left\n```","source":"_posts/some_binary_search.md","raw":"---\ntitle: Some Binary Search\ndate: 2023-09-14 19:31:09\ncategories: Test\ntags:\n- algorithm\n- binary-search\n- rust\n- python\n- c++\n- test\n- english\n- code\n---\n\nA binary search with C++:\n\n```c++\ntemplate<class T>\nint bin_search(vector<T>& arr, T target) {\n    int left = 0, right = arr.size() - 1;\n    while (left <= right) {\n        int mid = (left + right) / 2;\n        if (arr[mid] == target) {\n            break;\n        } else if (arr[mid] < target) {\n            left = mid + 1;\n        } else {\n            right = mid - 1;\n        }\n    }\n    return left;\n}\n```\n\nThe same thing with Rust:\n\n```rust\nfn bin_search<T: Ord>(arr: &Vec<T>, target: &T) -> usize {\n    let mut left = 0;\n    let mut right = arr.len() - 1;\n    while left <= right {\n        let mid = (left + right) / 2;\n        if arr[mid] == *target {\n            break;\n        } else if arr[mid] < *target {\n            left = mid + 1;\n        } else {\n            right = mid - 1;\n        }\n    }\n    left\n}\n```\n\nAnd with Python:\n\n```python\ndef bin_search(arr: list, target):\n    left = 0\n    right = len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] == target:\n            break\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return left\n```","slug":"some_binary_search","published":1,"updated":"2024-01-11T04:59:28.107Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cltl4oxei000hxh7k0xuo6ggs","content":"<p>A binary search with C++:</p>\n<figure class=\"highlight c++\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">class</span> T&gt;</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">bin_search</span><span class=\"params\">(vector&lt;T&gt;&amp; arr, T target)</span> </span>{</span><br><span class=\"line\">    <span class=\"type\">int</span> left = <span class=\"number\">0</span>, right = arr.<span class=\"built_in\">size</span>() - <span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (left &lt;= right) {</span><br><span class=\"line\">        <span class=\"type\">int</span> mid = (left + right) / <span class=\"number\">2</span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (arr[mid] == target) {</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        } <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (arr[mid] &lt; target) {</span><br><span class=\"line\">            left = mid + <span class=\"number\">1</span>;</span><br><span class=\"line\">        } <span class=\"keyword\">else</span> {</span><br><span class=\"line\">            right = mid - <span class=\"number\">1</span>;</span><br><span class=\"line\">        }</span><br><span class=\"line\">    }</span><br><span class=\"line\">    <span class=\"keyword\">return</span> left;</span><br><span class=\"line\">}</span><br></pre></td></tr></tbody></table></figure>\n<p>The same thing with Rust:</p>\n<figure class=\"highlight rust\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">fn</span> <span class=\"title function_\">bin_search</span>&lt;T: <span class=\"built_in\">Ord</span>&gt;(arr: &amp;<span class=\"type\">Vec</span>&lt;T&gt;, target: &amp;T) <span class=\"punctuation\">-&gt;</span> <span class=\"type\">usize</span> {</span><br><span class=\"line\">    <span class=\"keyword\">let</span> <span class=\"keyword\">mut </span><span class=\"variable\">left</span> = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">let</span> <span class=\"keyword\">mut </span><span class=\"variable\">right</span> = arr.<span class=\"title function_ invoke__\">len</span>() - <span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> left &lt;= right {</span><br><span class=\"line\">        <span class=\"keyword\">let</span> <span class=\"variable\">mid</span> = (left + right) / <span class=\"number\">2</span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> arr[mid] == *target {</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        } <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> arr[mid] &lt; *target {</span><br><span class=\"line\">            left = mid + <span class=\"number\">1</span>;</span><br><span class=\"line\">        } <span class=\"keyword\">else</span> {</span><br><span class=\"line\">            right = mid - <span class=\"number\">1</span>;</span><br><span class=\"line\">        }</span><br><span class=\"line\">    }</span><br><span class=\"line\">    left</span><br><span class=\"line\">}</span><br></pre></td></tr></tbody></table></figure>\n<p>And with Python:</p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">bin_search</span>(<span class=\"params\">arr: <span class=\"built_in\">list</span>, target</span>):</span><br><span class=\"line\">    left = <span class=\"number\">0</span></span><br><span class=\"line\">    right = <span class=\"built_in\">len</span>(arr) - <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span> left &lt;= right:</span><br><span class=\"line\">        mid = (left + right) // <span class=\"number\">2</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> arr[mid] == target:</span><br><span class=\"line\">            <span class=\"keyword\">break</span></span><br><span class=\"line\">        <span class=\"keyword\">elif</span> arr[mid] &lt; target:</span><br><span class=\"line\">            left = mid + <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            right = mid - <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> left</span><br></pre></td></tr></tbody></table></figure>","site":{"data":{}},"excerpt":"","more":"<p>A binary search with C++:</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">class</span> T&gt;</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">bin_search</span><span class=\"params\">(vector&lt;T&gt;&amp; arr, T target)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> left = <span class=\"number\">0</span>, right = arr.<span class=\"built_in\">size</span>() - <span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (left &lt;= right) &#123;</span><br><span class=\"line\">        <span class=\"type\">int</span> mid = (left + right) / <span class=\"number\">2</span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (arr[mid] == target) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (arr[mid] &lt; target) &#123;</span><br><span class=\"line\">            left = mid + <span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">            right = mid - <span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> left;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>The same thing with Rust:</p>\n<figure class=\"highlight rust\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">fn</span> <span class=\"title function_\">bin_search</span>&lt;T: <span class=\"built_in\">Ord</span>&gt;(arr: &amp;<span class=\"type\">Vec</span>&lt;T&gt;, target: &amp;T) <span class=\"punctuation\">-&gt;</span> <span class=\"type\">usize</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">let</span> <span class=\"keyword\">mut </span><span class=\"variable\">left</span> = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">let</span> <span class=\"keyword\">mut </span><span class=\"variable\">right</span> = arr.<span class=\"title function_ invoke__\">len</span>() - <span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> left &lt;= right &#123;</span><br><span class=\"line\">        <span class=\"keyword\">let</span> <span class=\"variable\">mid</span> = (left + right) / <span class=\"number\">2</span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> arr[mid] == *target &#123;</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> arr[mid] &lt; *target &#123;</span><br><span class=\"line\">            left = mid + <span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">            right = mid - <span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    left</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>And with Python:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">bin_search</span>(<span class=\"params\">arr: <span class=\"built_in\">list</span>, target</span>):</span><br><span class=\"line\">    left = <span class=\"number\">0</span></span><br><span class=\"line\">    right = <span class=\"built_in\">len</span>(arr) - <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span> left &lt;= right:</span><br><span class=\"line\">        mid = (left + right) // <span class=\"number\">2</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> arr[mid] == target:</span><br><span class=\"line\">            <span class=\"keyword\">break</span></span><br><span class=\"line\">        <span class=\"keyword\">elif</span> arr[mid] &lt; target:</span><br><span class=\"line\">            left = mid + <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            right = mid - <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> left</span><br></pre></td></tr></table></figure>"},{"author":"Chen Yingfa","title":"临近2023暑假，00师姐答辩，晚上打球","date":"2023-05-18T11:12:19.000Z","_content":"\n今天睡到九点才醒来，还是被00打电话叫醒的。去过了个早，然后去上《深度学习》。\n\n今天00的师姐答辩，下午三点去了，当时我在睡午觉。感觉之后她有点emo，但是她不承认，不知道为什么。然后聊了很多关于未来，结婚、生孩子、找工作等事情。感觉也没有很大的问题，但是00总是把东西看得很灰暗，很焦虑。\n\n<!-- more -->\n","source":"_posts/临近2023暑假.md","raw":"---\nauthor: Chen Yingfa\ntitle: 临近2023暑假，00师姐答辩，晚上打球\ndate: 2023-05-18 19:12:19\ncategories: Life\ntags:\n- life\n- '00'\n- school\n- graduation\n- 中文\n---\n\n今天睡到九点才醒来，还是被00打电话叫醒的。去过了个早，然后去上《深度学习》。\n\n今天00的师姐答辩，下午三点去了，当时我在睡午觉。感觉之后她有点emo，但是她不承认，不知道为什么。然后聊了很多关于未来，结婚、生孩子、找工作等事情。感觉也没有很大的问题，但是00总是把东西看得很灰暗，很焦虑。\n\n<!-- more -->\n","slug":"临近2023暑假","published":1,"updated":"2024-01-22T05:38:24.811Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cltl4oxej000mxh7k0gmq2mji","content":"<p>今天睡到九点才醒来，还是被00打电话叫醒的。去过了个早，然后去上《深度学习》。</p>\n<p>今天00的师姐答辩，下午三点去了，当时我在睡午觉。感觉之后她有点emo，但是她不承认，不知道为什么。然后聊了很多关于未来，结婚、生孩子、找工作等事情。感觉也没有很大的问题，但是00总是把东西看得很灰暗，很焦虑。</p>\n<span id=\"more\"></span>\n","site":{"data":{}},"excerpt":"<p>今天睡到九点才醒来，还是被00打电话叫醒的。去过了个早，然后去上《深度学习》。</p>\n<p>今天00的师姐答辩，下午三点去了，当时我在睡午觉。感觉之后她有点emo，但是她不承认，不知道为什么。然后聊了很多关于未来，结婚、生孩子、找工作等事情。感觉也没有很大的问题，但是00总是把东西看得很灰暗，很焦虑。</p>","more":""},{"author":"陈英发 Yingfa Chen","title":"更新个人主页","date":"2023-09-16T15:27:20.000Z","thumbnail":"申请签证.jpg","thubmnail-alt":"00在签证中心前面","thumbnail-title":"00在签证中心前面","_content":"\n之前有过个人主页，但是一直没有弄好，更没有更新。最近我将自己的 GitHub 的用户名改了，导致之前的 GitHub Pages 失效了，就趁机重新搭建个人主页。\n\n兜兜转转，还是决定使用 Hexo。以前用过 Jekyll，觉得还行，但是真的不想用 Ruby，Hugo 又太麻烦。\n\n\n<!-- more -->\n\n\n选了好久主题，Hexo 宣传说有很多主题，但是官网上不到 400 个主题，而且大部分都不符合我的审美或者要求。我想要的风格是简约，现代，需要同时支持黑暗和白亮模式，需要有代码高亮且是代码是等款字体。最接近我的要求就是[Maple](https://www.github.com/xbmlz/hexo-theme-maple)主题。可是仍然无法满足我的要求，所以我修改了一些格式（原版甚至有一些颜色 bug），添加了自己的一些内容，结果是一个叫做[枫叶](https://www.github.com/chen-yingfa/hexo-theme-fengye)的主题。\n\n## 日记\n\n今天早上七点半起来 🛏，打电话 📱 叫醒00（终于有一次是我打电话了哈哈哈哈），然后去核研院俱乐部在综体打羽毛球 🏸，后来发现他们其实约了西体，但是我跟00自己在蹭一个空场就不管了，八点半左右有人来了我们就去过早，然后去我宿舍 🏡。\n\n之后点了库迪，然后去了学校南边的一个超市，买了一大包薯片和一个榴莲！然后就在宿舍没有吃午饭，直接待到晚饭。中午的时候还拍了视频 📷，中间还差点说到00emo了，哈哈哈哈。\n\n今天 00 下午四点和晚上七点都有直播课 👩🏻‍🏫，都是真正开课，下午的在我宿舍开的，好像很成功，虽然拖堂了一点点。晚上的在她自己宿舍，貌似也拖堂了，00 说有好多人。\n\n晚上九点去打羽毛球了 🏸，带上相机录了打球的视频，然后回去洗澡，晚上去林大北路的家 🏡。\n\n## 最近\n\n最近好忙，新学期马上就要开始了，这里总结一下暑假开始到此比较重要的事情吧。\n\n这个暑假搬出校又搬回来了，折腾了又费钱 💰，学校真的好恶心，之前说了大概率是不会有宿舍，现在就有很多空的房间。\n\n期末前跟导师确定了要读博了，我跟他我想要三年毕业，他说没有问题，希望真的是可以吧，我们实验室好像基本都是直博生，普博的应该都是四年吧。00 也确定了不会读博了，最近在投简历，Oppo 好像已经拿到了 offer，但是他们北京没有部门，所以 00 不想去，我也不想她去。好像互联网以外很多公司都不在北京……\n\n我的论文 📃 EREN（以前叫做 EmoRen）投出去了，上周 rebuttal 结果出来了，不是很理想，本来 soundess 是 433，Excitement 323，rebuttal 结束后第一个审稿人将 soundness 调低了。学长说主会议估计没有机会了，Findings 还有希望，我其实无所谓是不是 Findings，感觉学长反而有点介意。\n\n被实验室的学长学姐拉去面壁智能[^2]去干活，跟公司的业务没啥关系，就是把我的工位搬了，可能不想占用隔壁实验室的位置吧 😂 但是我真的不想去 😭 不能跟 00 待在一起了。我现在就是一周可能去两三天 😂\n\n[^2]:我导师和知乎孵化的的公司\n\n最近还申请了签证，决定了寒假 00 跟我一起回家！在家待一整个月，好神奇，觉得我们的关系发展得好顺利。马上的国庆 🇨🇳 我会跟 00 回去武汉和应城参加她高中同学和表姐的婚礼 💑，顺便还会看她的外公外婆。\n\n![中国公民的护照](%E6%9B%B4%E6%96%B0%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5/%E4%B8%AD%E5%9B%BD%E6%8A%A4%E7%85%A7.jpg)\n\n> 00 的护照，是我向往的身份！\n\n![00在签证中心前面](%E6%9B%B4%E6%96%B0%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5/%E7%94%B3%E8%AF%B7%E7%AD%BE%E8%AF%81.jpg)\n\n> 签证中心门口，不是大使馆，很多个国家统一办理签证的地方。\n\n![我跟00在签证中心周边吃澳门菜](%E6%9B%B4%E6%96%B0%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5/%E5%90%83%E6%BE%B3%E9%97%A8%E8%8F%9C.jpg)\n\n> 在申请签证的地方旁边的一个很高级的商场里面吃澳门餐。\n\n***\n\n## 一些 Markdown 渲染器测试\n\n测试一下公式的渲染。^[我现在用了 [hexo-rendered-markdown-it](https://github.com/hexojs/hexo-renderer-markdown-it) 来代替原来 Hexo 的渲染器。原来的渲染器是 Marked (hexo-renderer-marked)]\n\n$$\n\\theta_i \\leftarrow  \\frac{\\partial}{\\partial  \\theta_i} \\mathcal L( y, f(x; \\theta))\n$$\n\n其中 $\\mathcal L$ 是损失函数，$f(\\cdot; \\theta)$ 由 $\\theta$ 参数化的模型。\n\n那 `代码` 呢？`codell1i0oO` 可以吗？\n\n一个表格：\n\n| Method | Accuracy |\n| ------ | -------- |\n| A      | 0.1      |\n| B      | 0.2      |\n| C      | 0.3      |\n\n一个嵌套列表：\n\n- a\n\n  - x\n  - asdf\n\n    - 懂得都懂\n\n  - 好好\n\n    1. item 1\n    2. item 2\n\n       1. sdf\n       2. sdf\n\n    3. item 3\n\n\n- asdf\n\n一个代办列表：\n\n- [ ] 买菜\n- [ ] 做饭\n- [x] 跑步\n- [x] 打羽毛球\n- [x] 写论文\n- [ ] 搞科研\n\nauto-links: [www.hexo.io](http://www.hexo.io)","source":"_posts/更新个人主页.md","raw":"---\nauthor: 陈英发 Yingfa Chen\ntitle: 更新个人主页\ndate: 2023-09-16 23:27:20\nthumbnail: \"申请签证.jpg\"\nthubmnail-alt: 00在签证中心前面\nthumbnail-title: 00在签证中心前面\ntags:\n- life\n- blog\n- 中文\n- '00'\n- 签证\n- hexo\n- hugo\n- jekyll\n- static-site-generator\n- research\n- 羽毛球\n- work\n- 面壁智能\n- 枫叶\n- 中国\n- 挪威\n- markdown\n- 宿舍\ncategories: Life\n---\n\n之前有过个人主页，但是一直没有弄好，更没有更新。最近我将自己的 GitHub 的用户名改了，导致之前的 GitHub Pages 失效了，就趁机重新搭建个人主页。\n\n兜兜转转，还是决定使用 Hexo。以前用过 Jekyll，觉得还行，但是真的不想用 Ruby，Hugo 又太麻烦。\n\n\n<!-- more -->\n\n\n选了好久主题，Hexo 宣传说有很多主题，但是官网上不到 400 个主题，而且大部分都不符合我的审美或者要求。我想要的风格是简约，现代，需要同时支持黑暗和白亮模式，需要有代码高亮且是代码是等款字体。最接近我的要求就是[Maple](https://www.github.com/xbmlz/hexo-theme-maple)主题。可是仍然无法满足我的要求，所以我修改了一些格式（原版甚至有一些颜色 bug），添加了自己的一些内容，结果是一个叫做[枫叶](https://www.github.com/chen-yingfa/hexo-theme-fengye)的主题。\n\n## 日记\n\n今天早上七点半起来 🛏，打电话 📱 叫醒00（终于有一次是我打电话了哈哈哈哈），然后去核研院俱乐部在综体打羽毛球 🏸，后来发现他们其实约了西体，但是我跟00自己在蹭一个空场就不管了，八点半左右有人来了我们就去过早，然后去我宿舍 🏡。\n\n之后点了库迪，然后去了学校南边的一个超市，买了一大包薯片和一个榴莲！然后就在宿舍没有吃午饭，直接待到晚饭。中午的时候还拍了视频 📷，中间还差点说到00emo了，哈哈哈哈。\n\n今天 00 下午四点和晚上七点都有直播课 👩🏻‍🏫，都是真正开课，下午的在我宿舍开的，好像很成功，虽然拖堂了一点点。晚上的在她自己宿舍，貌似也拖堂了，00 说有好多人。\n\n晚上九点去打羽毛球了 🏸，带上相机录了打球的视频，然后回去洗澡，晚上去林大北路的家 🏡。\n\n## 最近\n\n最近好忙，新学期马上就要开始了，这里总结一下暑假开始到此比较重要的事情吧。\n\n这个暑假搬出校又搬回来了，折腾了又费钱 💰，学校真的好恶心，之前说了大概率是不会有宿舍，现在就有很多空的房间。\n\n期末前跟导师确定了要读博了，我跟他我想要三年毕业，他说没有问题，希望真的是可以吧，我们实验室好像基本都是直博生，普博的应该都是四年吧。00 也确定了不会读博了，最近在投简历，Oppo 好像已经拿到了 offer，但是他们北京没有部门，所以 00 不想去，我也不想她去。好像互联网以外很多公司都不在北京……\n\n我的论文 📃 EREN（以前叫做 EmoRen）投出去了，上周 rebuttal 结果出来了，不是很理想，本来 soundess 是 433，Excitement 323，rebuttal 结束后第一个审稿人将 soundness 调低了。学长说主会议估计没有机会了，Findings 还有希望，我其实无所谓是不是 Findings，感觉学长反而有点介意。\n\n被实验室的学长学姐拉去面壁智能[^2]去干活，跟公司的业务没啥关系，就是把我的工位搬了，可能不想占用隔壁实验室的位置吧 😂 但是我真的不想去 😭 不能跟 00 待在一起了。我现在就是一周可能去两三天 😂\n\n[^2]:我导师和知乎孵化的的公司\n\n最近还申请了签证，决定了寒假 00 跟我一起回家！在家待一整个月，好神奇，觉得我们的关系发展得好顺利。马上的国庆 🇨🇳 我会跟 00 回去武汉和应城参加她高中同学和表姐的婚礼 💑，顺便还会看她的外公外婆。\n\n![中国公民的护照](%E6%9B%B4%E6%96%B0%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5/%E4%B8%AD%E5%9B%BD%E6%8A%A4%E7%85%A7.jpg)\n\n> 00 的护照，是我向往的身份！\n\n![00在签证中心前面](%E6%9B%B4%E6%96%B0%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5/%E7%94%B3%E8%AF%B7%E7%AD%BE%E8%AF%81.jpg)\n\n> 签证中心门口，不是大使馆，很多个国家统一办理签证的地方。\n\n![我跟00在签证中心周边吃澳门菜](%E6%9B%B4%E6%96%B0%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5/%E5%90%83%E6%BE%B3%E9%97%A8%E8%8F%9C.jpg)\n\n> 在申请签证的地方旁边的一个很高级的商场里面吃澳门餐。\n\n***\n\n## 一些 Markdown 渲染器测试\n\n测试一下公式的渲染。^[我现在用了 [hexo-rendered-markdown-it](https://github.com/hexojs/hexo-renderer-markdown-it) 来代替原来 Hexo 的渲染器。原来的渲染器是 Marked (hexo-renderer-marked)]\n\n$$\n\\theta_i \\leftarrow  \\frac{\\partial}{\\partial  \\theta_i} \\mathcal L( y, f(x; \\theta))\n$$\n\n其中 $\\mathcal L$ 是损失函数，$f(\\cdot; \\theta)$ 由 $\\theta$ 参数化的模型。\n\n那 `代码` 呢？`codell1i0oO` 可以吗？\n\n一个表格：\n\n| Method | Accuracy |\n| ------ | -------- |\n| A      | 0.1      |\n| B      | 0.2      |\n| C      | 0.3      |\n\n一个嵌套列表：\n\n- a\n\n  - x\n  - asdf\n\n    - 懂得都懂\n\n  - 好好\n\n    1. item 1\n    2. item 2\n\n       1. sdf\n       2. sdf\n\n    3. item 3\n\n\n- asdf\n\n一个代办列表：\n\n- [ ] 买菜\n- [ ] 做饭\n- [x] 跑步\n- [x] 打羽毛球\n- [x] 写论文\n- [ ] 搞科研\n\nauto-links: [www.hexo.io](http://www.hexo.io)","slug":"更新个人主页","published":1,"updated":"2024-01-23T11:32:52.626Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cltl4oxej000nxh7kc2lgbk3d","content":"<p>之前有过个人主页，但是一直没有弄好，更没有更新。最近我将自己的 GitHub 的用户名改了，导致之前的 GitHub Pages 失效了，就趁机重新搭建个人主页。</p>\n<p>兜兜转转，还是决定使用 Hexo。以前用过 Jekyll，觉得还行，但是真的不想用 Ruby，Hugo 又太麻烦。</p>\n<span id=\"more\"></span>\n<p>选了好久主题，Hexo 宣传说有很多主题，但是官网上不到 400 个主题，而且大部分都不符合我的审美或者要求。我想要的风格是简约，现代，需要同时支持黑暗和白亮模式，需要有代码高亮且是代码是等款字体。最接近我的要求就是<a href=\"https://www.github.com/xbmlz/hexo-theme-maple\">Maple</a>主题。可是仍然无法满足我的要求，所以我修改了一些格式（原版甚至有一些颜色 bug），添加了自己的一些内容，结果是一个叫做<a href=\"https://www.github.com/chen-yingfa/hexo-theme-fengye\">枫叶</a>的主题。</p>\n<h2 id=\"日记\">日记</h2>\n<p>今天早上七点半起来 🛏，打电话 📱 叫醒00（终于有一次是我打电话了哈哈哈哈），然后去核研院俱乐部在综体打羽毛球 🏸，后来发现他们其实约了西体，但是我跟00自己在蹭一个空场就不管了，八点半左右有人来了我们就去过早，然后去我宿舍 🏡。</p>\n<p>之后点了库迪，然后去了学校南边的一个超市，买了一大包薯片和一个榴莲！然后就在宿舍没有吃午饭，直接待到晚饭。中午的时候还拍了视频 📷，中间还差点说到00emo了，哈哈哈哈。</p>\n<p>今天 00 下午四点和晚上七点都有直播课 👩🏻‍🏫，都是真正开课，下午的在我宿舍开的，好像很成功，虽然拖堂了一点点。晚上的在她自己宿舍，貌似也拖堂了，00 说有好多人。</p>\n<p>晚上九点去打羽毛球了 🏸，带上相机录了打球的视频，然后回去洗澡，晚上去林大北路的家 🏡。</p>\n<h2 id=\"最近\">最近</h2>\n<p>最近好忙，新学期马上就要开始了，这里总结一下暑假开始到此比较重要的事情吧。</p>\n<p>这个暑假搬出校又搬回来了，折腾了又费钱 💰，学校真的好恶心，之前说了大概率是不会有宿舍，现在就有很多空的房间。</p>\n<p>期末前跟导师确定了要读博了，我跟他我想要三年毕业，他说没有问题，希望真的是可以吧，我们实验室好像基本都是直博生，普博的应该都是四年吧。00 也确定了不会读博了，最近在投简历，Oppo 好像已经拿到了 offer，但是他们北京没有部门，所以 00 不想去，我也不想她去。好像互联网以外很多公司都不在北京……</p>\n<p>我的论文 📃 EREN（以前叫做 EmoRen）投出去了，上周 rebuttal 结果出来了，不是很理想，本来 soundess 是 433，Excitement 323，rebuttal 结束后第一个审稿人将 soundness 调低了。学长说主会议估计没有机会了，Findings 还有希望，我其实无所谓是不是 Findings，感觉学长反而有点介意。</p>\n<p>被实验室的学长学姐拉去面壁智能<sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">[1]</a></sup>去干活，跟公司的业务没啥关系，就是把我的工位搬了，可能不想占用隔壁实验室的位置吧 😂 但是我真的不想去 😭 不能跟 00 待在一起了。我现在就是一周可能去两三天 😂</p>\n<p>最近还申请了签证，决定了寒假 00 跟我一起回家！在家待一整个月，好神奇，觉得我们的关系发展得好顺利。马上的国庆 🇨🇳 我会跟 00 回去武汉和应城参加她高中同学和表姐的婚礼 💑，顺便还会看她的外公外婆。</p>\n<p><img src=\"%E6%9B%B4%E6%96%B0%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5/%E4%B8%AD%E5%9B%BD%E6%8A%A4%E7%85%A7.jpg\" alt=\"中国公民的护照\"></p>\n<blockquote>\n<p>00 的护照，是我向往的身份！</p>\n</blockquote>\n<p><img src=\"%E6%9B%B4%E6%96%B0%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5/%E7%94%B3%E8%AF%B7%E7%AD%BE%E8%AF%81.jpg\" alt=\"00在签证中心前面\"></p>\n<blockquote>\n<p>签证中心门口，不是大使馆，很多个国家统一办理签证的地方。</p>\n</blockquote>\n<p><img src=\"%E6%9B%B4%E6%96%B0%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5/%E5%90%83%E6%BE%B3%E9%97%A8%E8%8F%9C.jpg\" alt=\"我跟00在签证中心周边吃澳门菜\"></p>\n<blockquote>\n<p>在申请签证的地方旁边的一个很高级的商场里面吃澳门餐。</p>\n</blockquote>\n<hr>\n<h2 id=\"一些-Markdown-渲染器测试\">一些 Markdown 渲染器测试</h2>\n<p>测试一下公式的渲染。<sup class=\"footnote-ref\"><a href=\"#fn2\" id=\"fnref2\">[2]</a></sup></p>\n<p>$$\n\\theta_i \\leftarrow  \\frac{\\partial}{\\partial  \\theta_i} \\mathcal L( y, f(x; \\theta))\n$$</p>\n<p>其中 $\\mathcal L$ 是损失函数，$f(\\cdot; \\theta)$ 由 $\\theta$ 参数化的模型。</p>\n<p>那 <code>代码</code> 呢？<code>codell1i0oO</code> 可以吗？</p>\n<p>一个表格：</p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Accuracy</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>A</td>\n<td>0.1</td>\n</tr>\n<tr>\n<td>B</td>\n<td>0.2</td>\n</tr>\n<tr>\n<td>C</td>\n<td>0.3</td>\n</tr>\n</tbody>\n</table>\n<p>一个嵌套列表：</p>\n<ul>\n<li>\n<p>a</p>\n<ul>\n<li>\n<p>x</p>\n</li>\n<li>\n<p>asdf</p>\n<ul>\n<li>懂得都懂</li>\n</ul>\n</li>\n<li>\n<p>好好</p>\n<ol>\n<li>\n<p>item 1</p>\n</li>\n<li>\n<p>item 2</p>\n<ol>\n<li>sdf</li>\n<li>sdf</li>\n</ol>\n</li>\n<li>\n<p>item 3</p>\n</li>\n</ol>\n</li>\n</ul>\n</li>\n<li>\n<p>asdf</p>\n</li>\n</ul>\n<p>一个代办列表：</p>\n<ul class=\"contains-task-list\">\n<li class=\"task-list-item\"><input class=\"task-list-item-checkbox\" disabled=\"\" type=\"checkbox\"> 买菜</li>\n<li class=\"task-list-item\"><input class=\"task-list-item-checkbox\" disabled=\"\" type=\"checkbox\"> 做饭</li>\n<li class=\"task-list-item\"><input class=\"task-list-item-checkbox\" checked=\"\" disabled=\"\" type=\"checkbox\"> 跑步</li>\n<li class=\"task-list-item\"><input class=\"task-list-item-checkbox\" checked=\"\" disabled=\"\" type=\"checkbox\"> 打羽毛球</li>\n<li class=\"task-list-item\"><input class=\"task-list-item-checkbox\" checked=\"\" disabled=\"\" type=\"checkbox\"> 写论文</li>\n<li class=\"task-list-item\"><input class=\"task-list-item-checkbox\" disabled=\"\" type=\"checkbox\"> 搞科研</li>\n</ul>\n<p>auto-links: <a href=\"http://www.hexo.io\">www.hexo.io</a></p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn1\" class=\"footnote-item\"><p>我导师和知乎孵化的的公司 <a href=\"#fnref1\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n<li id=\"fn2\" class=\"footnote-item\"><p>我现在用了 <a href=\"https://github.com/hexojs/hexo-renderer-markdown-it\">hexo-rendered-markdown-it</a> 来代替原来 Hexo 的渲染器。原来的渲染器是 Marked (hexo-renderer-marked) <a href=\"#fnref2\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n</ol>\n</section>\n","site":{"data":{}},"excerpt":"<p>之前有过个人主页，但是一直没有弄好，更没有更新。最近我将自己的 GitHub 的用户名改了，导致之前的 GitHub Pages 失效了，就趁机重新搭建个人主页。</p>\n<p>兜兜转转，还是决定使用 Hexo。以前用过 Jekyll，觉得还行，但是真的不想用 Ruby，Hugo 又太麻烦。</p>","more":"<p>选了好久主题，Hexo 宣传说有很多主题，但是官网上不到 400 个主题，而且大部分都不符合我的审美或者要求。我想要的风格是简约，现代，需要同时支持黑暗和白亮模式，需要有代码高亮且是代码是等款字体。最接近我的要求就是<a href=\"https://www.github.com/xbmlz/hexo-theme-maple\">Maple</a>主题。可是仍然无法满足我的要求，所以我修改了一些格式（原版甚至有一些颜色 bug），添加了自己的一些内容，结果是一个叫做<a href=\"https://www.github.com/chen-yingfa/hexo-theme-fengye\">枫叶</a>的主题。</p>\n<h2 id=\"日记\">日记</h2>\n<p>今天早上七点半起来 🛏，打电话 📱 叫醒00（终于有一次是我打电话了哈哈哈哈），然后去核研院俱乐部在综体打羽毛球 🏸，后来发现他们其实约了西体，但是我跟00自己在蹭一个空场就不管了，八点半左右有人来了我们就去过早，然后去我宿舍 🏡。</p>\n<p>之后点了库迪，然后去了学校南边的一个超市，买了一大包薯片和一个榴莲！然后就在宿舍没有吃午饭，直接待到晚饭。中午的时候还拍了视频 📷，中间还差点说到00emo了，哈哈哈哈。</p>\n<p>今天 00 下午四点和晚上七点都有直播课 👩🏻‍🏫，都是真正开课，下午的在我宿舍开的，好像很成功，虽然拖堂了一点点。晚上的在她自己宿舍，貌似也拖堂了，00 说有好多人。</p>\n<p>晚上九点去打羽毛球了 🏸，带上相机录了打球的视频，然后回去洗澡，晚上去林大北路的家 🏡。</p>\n<h2 id=\"最近\">最近</h2>\n<p>最近好忙，新学期马上就要开始了，这里总结一下暑假开始到此比较重要的事情吧。</p>\n<p>这个暑假搬出校又搬回来了，折腾了又费钱 💰，学校真的好恶心，之前说了大概率是不会有宿舍，现在就有很多空的房间。</p>\n<p>期末前跟导师确定了要读博了，我跟他我想要三年毕业，他说没有问题，希望真的是可以吧，我们实验室好像基本都是直博生，普博的应该都是四年吧。00 也确定了不会读博了，最近在投简历，Oppo 好像已经拿到了 offer，但是他们北京没有部门，所以 00 不想去，我也不想她去。好像互联网以外很多公司都不在北京……</p>\n<p>我的论文 📃 EREN（以前叫做 EmoRen）投出去了，上周 rebuttal 结果出来了，不是很理想，本来 soundess 是 433，Excitement 323，rebuttal 结束后第一个审稿人将 soundness 调低了。学长说主会议估计没有机会了，Findings 还有希望，我其实无所谓是不是 Findings，感觉学长反而有点介意。</p>\n<p>被实验室的学长学姐拉去面壁智能<sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">[1]</a></sup>去干活，跟公司的业务没啥关系，就是把我的工位搬了，可能不想占用隔壁实验室的位置吧 😂 但是我真的不想去 😭 不能跟 00 待在一起了。我现在就是一周可能去两三天 😂</p>\n<p>最近还申请了签证，决定了寒假 00 跟我一起回家！在家待一整个月，好神奇，觉得我们的关系发展得好顺利。马上的国庆 🇨🇳 我会跟 00 回去武汉和应城参加她高中同学和表姐的婚礼 💑，顺便还会看她的外公外婆。</p>\n<p><img src=\"%E6%9B%B4%E6%96%B0%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5/%E4%B8%AD%E5%9B%BD%E6%8A%A4%E7%85%A7.jpg\" alt=\"中国公民的护照\"></p>\n<blockquote>\n<p>00 的护照，是我向往的身份！</p>\n</blockquote>\n<p><img src=\"%E6%9B%B4%E6%96%B0%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5/%E7%94%B3%E8%AF%B7%E7%AD%BE%E8%AF%81.jpg\" alt=\"00在签证中心前面\"></p>\n<blockquote>\n<p>签证中心门口，不是大使馆，很多个国家统一办理签证的地方。</p>\n</blockquote>\n<p><img src=\"%E6%9B%B4%E6%96%B0%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5/%E5%90%83%E6%BE%B3%E9%97%A8%E8%8F%9C.jpg\" alt=\"我跟00在签证中心周边吃澳门菜\"></p>\n<blockquote>\n<p>在申请签证的地方旁边的一个很高级的商场里面吃澳门餐。</p>\n</blockquote>\n<hr>\n<h2 id=\"一些-Markdown-渲染器测试\">一些 Markdown 渲染器测试</h2>\n<p>测试一下公式的渲染。<sup class=\"footnote-ref\"><a href=\"#fn2\" id=\"fnref2\">[2]</a></sup></p>\n<p>$$\n\\theta_i \\leftarrow  \\frac{\\partial}{\\partial  \\theta_i} \\mathcal L( y, f(x; \\theta))\n$$</p>\n<p>其中 $\\mathcal L$ 是损失函数，$f(\\cdot; \\theta)$ 由 $\\theta$ 参数化的模型。</p>\n<p>那 <code>代码</code> 呢？<code>codell1i0oO</code> 可以吗？</p>\n<p>一个表格：</p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Accuracy</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>A</td>\n<td>0.1</td>\n</tr>\n<tr>\n<td>B</td>\n<td>0.2</td>\n</tr>\n<tr>\n<td>C</td>\n<td>0.3</td>\n</tr>\n</tbody>\n</table>\n<p>一个嵌套列表：</p>\n<ul>\n<li>\n<p>a</p>\n<ul>\n<li>\n<p>x</p>\n</li>\n<li>\n<p>asdf</p>\n<ul>\n<li>懂得都懂</li>\n</ul>\n</li>\n<li>\n<p>好好</p>\n<ol>\n<li>\n<p>item 1</p>\n</li>\n<li>\n<p>item 2</p>\n<ol>\n<li>sdf</li>\n<li>sdf</li>\n</ol>\n</li>\n<li>\n<p>item 3</p>\n</li>\n</ol>\n</li>\n</ul>\n</li>\n<li>\n<p>asdf</p>\n</li>\n</ul>\n<p>一个代办列表：</p>\n<ul class=\"contains-task-list\">\n<li class=\"task-list-item\"><input class=\"task-list-item-checkbox\" disabled=\"\" type=\"checkbox\"> 买菜</li>\n<li class=\"task-list-item\"><input class=\"task-list-item-checkbox\" disabled=\"\" type=\"checkbox\"> 做饭</li>\n<li class=\"task-list-item\"><input class=\"task-list-item-checkbox\" checked=\"\" disabled=\"\" type=\"checkbox\"> 跑步</li>\n<li class=\"task-list-item\"><input class=\"task-list-item-checkbox\" checked=\"\" disabled=\"\" type=\"checkbox\"> 打羽毛球</li>\n<li class=\"task-list-item\"><input class=\"task-list-item-checkbox\" checked=\"\" disabled=\"\" type=\"checkbox\"> 写论文</li>\n<li class=\"task-list-item\"><input class=\"task-list-item-checkbox\" disabled=\"\" type=\"checkbox\"> 搞科研</li>\n</ul>\n<p>auto-links: <a href=\"http://www.hexo.io\">www.hexo.io</a></p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn1\" class=\"footnote-item\"><p>我导师和知乎孵化的的公司 <a href=\"#fnref1\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n<li id=\"fn2\" class=\"footnote-item\"><p>我现在用了 <a href=\"https://github.com/hexojs/hexo-renderer-markdown-it\">hexo-rendered-markdown-it</a> 来代替原来 Hexo 的渲染器。原来的渲染器是 Marked (hexo-renderer-marked) <a href=\"#fnref2\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n</ol>\n</section>"},{"author":"陈英发 Yingfa Chen","title":"第一个帖子，瞎写点东西","date":"2023-05-17T15:00:00.000Z","thumbnail":"lillesand0.jpg","_content":"\n现在是 2023 年五月十七，马上硕士一年级就结束，在清华园已经快五年了，感觉对我人生的影响真的巨大。这一年认识了很可爱的 00，希望可以一直走下去。\n\n我和 00 的孩子们：\n\n\n<!-- more -->\n\n\n- 卧龙：调皮的肥猫 🐱\n- 小绿：喜欢咬东西的鳄鱼 🐊\n- 骆雁：超级大的土鸡！🐰\n- 凤雏：不调皮的猫咪 🐱\n- 黄帝：更大的巨兔 🐰\n- 内存条：白色的熊熊 🐻\n- 闪光灯：灰色的熊熊 🐻\n\n## 现在要做的事情\n\n- 把 EmoRen 投了\n\n  - 能不能行啊\n\n- 跑 CFD 的丹炉调好\n\n  - 好难呀\n\n- 写完作业\n\n  - NLP和DL的大作业！\n\n- 搞定去ACL的手续\n\n  - 去加拿大，然后回挪威一两周，然后回来跟 00 去南京，我不用签证，但是还是有很多手续。\n\n- 写好开题报告\n\n  - 还不知道做啥呢\n\n\n## 我的家乡 Lillesand\n\n![Lillesand，挪威南边的一个沿海小镇，人口大约一万。我出生长大的地方，到本科来清华才离开的。](./第一个帖子，瞎写点东西/lillesand0.jpg)\n\n![Lillesand 的港口](./第一个帖子，瞎写点东西/lillesand1.jpg)\n\n好久没有回去了，上一次回挪威也没有回去","source":"_posts/第一个帖子，瞎写点东西.md","raw":"---\nauthor: 陈英发 Yingfa Chen\ntitle: 第一个帖子，瞎写点东西\ndate: 2023-05-17 23:00:00\ncategories: Life\nthumbnail: lillesand0.jpg\ntags:\n- life\n- school\n- research\n- \"00\"\n- 孩子们\n- 卧龙\n- 凤雏\n- 骆雁\n- 黄帝\n- 小绿\n- 猫咪\n- 土鸡\n- 中文\n- 熊\n- 🐻\n- 🐱\n- 🐇\n- 🐰\n- 🐊\n- emoren\n- cfd\n- acl\n- lillesand\n- 加拿大\n---\n\n现在是 2023 年五月十七，马上硕士一年级就结束，在清华园已经快五年了，感觉对我人生的影响真的巨大。这一年认识了很可爱的 00，希望可以一直走下去。\n\n我和 00 的孩子们：\n\n\n<!-- more -->\n\n\n- 卧龙：调皮的肥猫 🐱\n- 小绿：喜欢咬东西的鳄鱼 🐊\n- 骆雁：超级大的土鸡！🐰\n- 凤雏：不调皮的猫咪 🐱\n- 黄帝：更大的巨兔 🐰\n- 内存条：白色的熊熊 🐻\n- 闪光灯：灰色的熊熊 🐻\n\n## 现在要做的事情\n\n- 把 EmoRen 投了\n\n  - 能不能行啊\n\n- 跑 CFD 的丹炉调好\n\n  - 好难呀\n\n- 写完作业\n\n  - NLP和DL的大作业！\n\n- 搞定去ACL的手续\n\n  - 去加拿大，然后回挪威一两周，然后回来跟 00 去南京，我不用签证，但是还是有很多手续。\n\n- 写好开题报告\n\n  - 还不知道做啥呢\n\n\n## 我的家乡 Lillesand\n\n![Lillesand，挪威南边的一个沿海小镇，人口大约一万。我出生长大的地方，到本科来清华才离开的。](./第一个帖子，瞎写点东西/lillesand0.jpg)\n\n![Lillesand 的港口](./第一个帖子，瞎写点东西/lillesand1.jpg)\n\n好久没有回去了，上一次回挪威也没有回去","slug":"第一个帖子，瞎写点东西","published":1,"updated":"2024-02-26T02:56:21.456Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cltl4oxel000rxh7kh777euay","content":"<p>现在是 2023 年五月十七，马上硕士一年级就结束，在清华园已经快五年了，感觉对我人生的影响真的巨大。这一年认识了很可爱的 00，希望可以一直走下去。</p>\n<p>我和 00 的孩子们：</p>\n<span id=\"more\"></span>\n<ul>\n<li>卧龙：调皮的肥猫 🐱</li>\n<li>小绿：喜欢咬东西的鳄鱼 🐊</li>\n<li>骆雁：超级大的土鸡！🐰</li>\n<li>凤雏：不调皮的猫咪 🐱</li>\n<li>黄帝：更大的巨兔 🐰</li>\n<li>内存条：白色的熊熊 🐻</li>\n<li>闪光灯：灰色的熊熊 🐻</li>\n</ul>\n<h2 id=\"现在要做的事情\">现在要做的事情</h2>\n<ul>\n<li>\n<p>把 EmoRen 投了</p>\n<ul>\n<li>能不能行啊</li>\n</ul>\n</li>\n<li>\n<p>跑 CFD 的丹炉调好</p>\n<ul>\n<li>好难呀</li>\n</ul>\n</li>\n<li>\n<p>写完作业</p>\n<ul>\n<li>NLP和DL的大作业！</li>\n</ul>\n</li>\n<li>\n<p>搞定去ACL的手续</p>\n<ul>\n<li>去加拿大，然后回挪威一两周，然后回来跟 00 去南京，我不用签证，但是还是有很多手续。</li>\n</ul>\n</li>\n<li>\n<p>写好开题报告</p>\n<ul>\n<li>还不知道做啥呢</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"我的家乡-Lillesand\">我的家乡 Lillesand</h2>\n<p><img src=\"/2023/05/17/%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%B8%96%E5%AD%90%EF%BC%8C%E7%9E%8E%E5%86%99%E7%82%B9%E4%B8%9C%E8%A5%BF/lillesand0.jpg\" alt=\"Lillesand，挪威南边的一个沿海小镇，人口大约一万。我出生长大的地方，到本科来清华才离开的。\"></p>\n<p><img src=\"/2023/05/17/%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%B8%96%E5%AD%90%EF%BC%8C%E7%9E%8E%E5%86%99%E7%82%B9%E4%B8%9C%E8%A5%BF/lillesand1.jpg\" alt=\"Lillesand 的港口\"></p>\n<p>好久没有回去了，上一次回挪威也没有回去</p>\n","site":{"data":{}},"excerpt":"<p>现在是 2023 年五月十七，马上硕士一年级就结束，在清华园已经快五年了，感觉对我人生的影响真的巨大。这一年认识了很可爱的 00，希望可以一直走下去。</p>\n<p>我和 00 的孩子们：</p>","more":"<ul>\n<li>卧龙：调皮的肥猫 🐱</li>\n<li>小绿：喜欢咬东西的鳄鱼 🐊</li>\n<li>骆雁：超级大的土鸡！🐰</li>\n<li>凤雏：不调皮的猫咪 🐱</li>\n<li>黄帝：更大的巨兔 🐰</li>\n<li>内存条：白色的熊熊 🐻</li>\n<li>闪光灯：灰色的熊熊 🐻</li>\n</ul>\n<h2 id=\"现在要做的事情\">现在要做的事情</h2>\n<ul>\n<li>\n<p>把 EmoRen 投了</p>\n<ul>\n<li>能不能行啊</li>\n</ul>\n</li>\n<li>\n<p>跑 CFD 的丹炉调好</p>\n<ul>\n<li>好难呀</li>\n</ul>\n</li>\n<li>\n<p>写完作业</p>\n<ul>\n<li>NLP和DL的大作业！</li>\n</ul>\n</li>\n<li>\n<p>搞定去ACL的手续</p>\n<ul>\n<li>去加拿大，然后回挪威一两周，然后回来跟 00 去南京，我不用签证，但是还是有很多手续。</li>\n</ul>\n</li>\n<li>\n<p>写好开题报告</p>\n<ul>\n<li>还不知道做啥呢</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"我的家乡-Lillesand\">我的家乡 Lillesand</h2>\n<p><img src=\"/2023/05/17/%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%B8%96%E5%AD%90%EF%BC%8C%E7%9E%8E%E5%86%99%E7%82%B9%E4%B8%9C%E8%A5%BF/lillesand0.jpg\" alt=\"Lillesand，挪威南边的一个沿海小镇，人口大约一万。我出生长大的地方，到本科来清华才离开的。\"></p>\n<p><img src=\"/2023/05/17/%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%B8%96%E5%AD%90%EF%BC%8C%E7%9E%8E%E5%86%99%E7%82%B9%E4%B8%9C%E8%A5%BF/lillesand1.jpg\" alt=\"Lillesand 的港口\"></p>\n<p>好久没有回去了，上一次回挪威也没有回去</p>"},{"layout":"post","author":"陈英发 Yingfa Chen","title":"第一篇","date":"2022-10-27T06:04:16.000Z","_content":"\n之前尝试用 Hugo 来部署，发现 Hugo 不仅挺复杂，而且还有很多小问题，可能这就是速度带来的代价吧。但是其实我也不是写很多内容，所以 Jekyll 的速度应该是够用的。\n\nJekyll 支持在 markdown 内容里面用 Liquid template tags 来生成动态内容，比如根据 front matter 中的 tags，给每个 tag 生成 html div。如下 liquid 语法：\n\n{% raw %}\n```html\n<div>\n{% for tag in site.tags %}\n    <a style=\"background-color: blue;\">#{{ tag[0] }}</a>\n{% endfor %}\n</div>\n```\n{% endraw %}\n\n<!-- more -->\n\n会根据 post markdown 文件中的 front matter 中定义的 tags：\n\n```markdown\n---\ntags: some tags here\n---\n```\n\n生成相应的 html div：\n\n```html\n<div class=\"post-tags\">\n    <a href=\"/tags/life\" class=\"tag-card\">life</a>\n    <a href=\"/tags/update\" class=\"tag-card\">update</a>\n    <a href=\"/tags/learn\" class=\"tag-card\">learn</a>\n    <a href=\"/tags/important\" class=\"tag-card\">important</a>\n    <a href=\"/tags/jekyll\" class=\"tag-card\">jekyll</a>\n    <a href=\"/tags/hugo\" class=\"tag-card\">hugo</a>\n    <a href=\"/tags/static-site-generator\" class=\"tag-card\">static-site-generator</a>\n</div>\n```\n\n---\n\n不知道写啥，就写一个 Python 的二分搜索吧：\n\n```python\ndef bin_search(arr: list, target: Any) -> int:\n    '''\n    Return the smallest index such that when target is inserted at that index,\n    the array will remain sorted.\n    '''\n    lo, hi = 0, len(arr)\n    while lo < hi:\n        m = (lo + hi) // 2\n        if arr[m] < target:\n            lo = m + 1\n        else:\n            hi = m\n    return lo\n```","source":"_posts/第一篇.md","raw":"---\nlayout: post\nauthor: 陈英发 Yingfa Chen\ntitle: 第一篇\ndate: 2022-10-27 14:04:16 +0800\ncategories: Life\ntags: \n- life\n- jekyll\n- test\n- hugo\n- static-site-generator\n- 中文\n- html\n- liquid\n---\n\n之前尝试用 Hugo 来部署，发现 Hugo 不仅挺复杂，而且还有很多小问题，可能这就是速度带来的代价吧。但是其实我也不是写很多内容，所以 Jekyll 的速度应该是够用的。\n\nJekyll 支持在 markdown 内容里面用 Liquid template tags 来生成动态内容，比如根据 front matter 中的 tags，给每个 tag 生成 html div。如下 liquid 语法：\n\n{% raw %}\n```html\n<div>\n{% for tag in site.tags %}\n    <a style=\"background-color: blue;\">#{{ tag[0] }}</a>\n{% endfor %}\n</div>\n```\n{% endraw %}\n\n<!-- more -->\n\n会根据 post markdown 文件中的 front matter 中定义的 tags：\n\n```markdown\n---\ntags: some tags here\n---\n```\n\n生成相应的 html div：\n\n```html\n<div class=\"post-tags\">\n    <a href=\"/tags/life\" class=\"tag-card\">life</a>\n    <a href=\"/tags/update\" class=\"tag-card\">update</a>\n    <a href=\"/tags/learn\" class=\"tag-card\">learn</a>\n    <a href=\"/tags/important\" class=\"tag-card\">important</a>\n    <a href=\"/tags/jekyll\" class=\"tag-card\">jekyll</a>\n    <a href=\"/tags/hugo\" class=\"tag-card\">hugo</a>\n    <a href=\"/tags/static-site-generator\" class=\"tag-card\">static-site-generator</a>\n</div>\n```\n\n---\n\n不知道写啥，就写一个 Python 的二分搜索吧：\n\n```python\ndef bin_search(arr: list, target: Any) -> int:\n    '''\n    Return the smallest index such that when target is inserted at that index,\n    the array will remain sorted.\n    '''\n    lo, hi = 0, len(arr)\n    while lo < hi:\n        m = (lo + hi) // 2\n        if arr[m] < target:\n            lo = m + 1\n        else:\n            hi = m\n    return lo\n```","slug":"第一篇","published":1,"updated":"2024-01-10T07:24:56.516Z","comments":1,"photos":[],"link":"","_id":"cltl4oxel000sxh7k05fk4sya","content":"<p>之前尝试用 Hugo 来部署，发现 Hugo 不仅挺复杂，而且还有很多小问题，可能这就是速度带来的代价吧。但是其实我也不是写很多内容，所以 Jekyll 的速度应该是够用的。</p>\n<p>Jekyll 支持在 markdown 内容里面用 Liquid template tags 来生成动态内容，比如根据 front matter 中的 tags，给每个 tag 生成 html div。如下 liquid 语法：</p>\n\n<figure class=\"highlight html\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\">{% for tag in site.tags %}</span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">a</span> <span class=\"attr\">style</span>=<span class=\"string\">\"background-color: blue;\"</span>&gt;</span>#{{ tag[0] }}<span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span></span><br><span class=\"line\">{% endfor %}</span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br></pre></td></tr></tbody></table></figure>\n\n<span id=\"more\"></span>\n<p>会根据 post markdown 文件中的 front matter 中定义的 tags：</p>\n<figure class=\"highlight markdown\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">---</span><br><span class=\"line\"><span class=\"section\">tags: some tags here</span></span><br><span class=\"line\"><span class=\"section\">---</span></span><br></pre></td></tr></tbody></table></figure>\n<p>生成相应的 html div：</p>\n<figure class=\"highlight html\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">\"post-tags\"</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">a</span> <span class=\"attr\">href</span>=<span class=\"string\">\"/tags/life\"</span> <span class=\"attr\">class</span>=<span class=\"string\">\"tag-card\"</span>&gt;</span>life<span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">a</span> <span class=\"attr\">href</span>=<span class=\"string\">\"/tags/update\"</span> <span class=\"attr\">class</span>=<span class=\"string\">\"tag-card\"</span>&gt;</span>update<span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">a</span> <span class=\"attr\">href</span>=<span class=\"string\">\"/tags/learn\"</span> <span class=\"attr\">class</span>=<span class=\"string\">\"tag-card\"</span>&gt;</span>learn<span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">a</span> <span class=\"attr\">href</span>=<span class=\"string\">\"/tags/important\"</span> <span class=\"attr\">class</span>=<span class=\"string\">\"tag-card\"</span>&gt;</span>important<span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">a</span> <span class=\"attr\">href</span>=<span class=\"string\">\"/tags/jekyll\"</span> <span class=\"attr\">class</span>=<span class=\"string\">\"tag-card\"</span>&gt;</span>jekyll<span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">a</span> <span class=\"attr\">href</span>=<span class=\"string\">\"/tags/hugo\"</span> <span class=\"attr\">class</span>=<span class=\"string\">\"tag-card\"</span>&gt;</span>hugo<span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">a</span> <span class=\"attr\">href</span>=<span class=\"string\">\"/tags/static-site-generator\"</span> <span class=\"attr\">class</span>=<span class=\"string\">\"tag-card\"</span>&gt;</span>static-site-generator<span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br></pre></td></tr></tbody></table></figure>\n<hr>\n<p>不知道写啥，就写一个 Python 的二分搜索吧：</p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">bin_search</span>(<span class=\"params\">arr: <span class=\"built_in\">list</span>, target: <span class=\"type\">Any</span></span>) -&gt; <span class=\"built_in\">int</span>:</span><br><span class=\"line\">    <span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">    Return the smallest index such that when target is inserted at that index,</span></span><br><span class=\"line\"><span class=\"string\">    the array will remain sorted.</span></span><br><span class=\"line\"><span class=\"string\">    '''</span></span><br><span class=\"line\">    lo, hi = <span class=\"number\">0</span>, <span class=\"built_in\">len</span>(arr)</span><br><span class=\"line\">    <span class=\"keyword\">while</span> lo &lt; hi:</span><br><span class=\"line\">        m = (lo + hi) // <span class=\"number\">2</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> arr[m] &lt; target:</span><br><span class=\"line\">            lo = m + <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            hi = m</span><br><span class=\"line\">    <span class=\"keyword\">return</span> lo</span><br></pre></td></tr></tbody></table></figure>","site":{"data":{}},"excerpt":"<p>之前尝试用 Hugo 来部署，发现 Hugo 不仅挺复杂，而且还有很多小问题，可能这就是速度带来的代价吧。但是其实我也不是写很多内容，所以 Jekyll 的速度应该是够用的。</p>\n<p>Jekyll 支持在 markdown 内容里面用 Liquid template tags 来生成动态内容，比如根据 front matter 中的 tags，给每个 tag 生成 html div。如下 liquid 语法：</p>\n\n<figure class=\"highlight html\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\">{% for tag in site.tags %}</span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">a</span> <span class=\"attr\">style</span>=<span class=\"string\">\"background-color: blue;\"</span>&gt;</span>#{{ tag[0] }}<span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span></span><br><span class=\"line\">{% endfor %}</span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br></pre></td></tr></tbody></table></figure>","more":"<p>会根据 post markdown 文件中的 front matter 中定义的 tags：</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">---</span><br><span class=\"line\"><span class=\"section\">tags: some tags here</span></span><br><span class=\"line\"><span class=\"section\">---</span></span><br></pre></td></tr></table></figure>\n<p>生成相应的 html div：</p>\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;post-tags&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">a</span> <span class=\"attr\">href</span>=<span class=\"string\">&quot;/tags/life&quot;</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;tag-card&quot;</span>&gt;</span>life<span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">a</span> <span class=\"attr\">href</span>=<span class=\"string\">&quot;/tags/update&quot;</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;tag-card&quot;</span>&gt;</span>update<span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">a</span> <span class=\"attr\">href</span>=<span class=\"string\">&quot;/tags/learn&quot;</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;tag-card&quot;</span>&gt;</span>learn<span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">a</span> <span class=\"attr\">href</span>=<span class=\"string\">&quot;/tags/important&quot;</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;tag-card&quot;</span>&gt;</span>important<span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">a</span> <span class=\"attr\">href</span>=<span class=\"string\">&quot;/tags/jekyll&quot;</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;tag-card&quot;</span>&gt;</span>jekyll<span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">a</span> <span class=\"attr\">href</span>=<span class=\"string\">&quot;/tags/hugo&quot;</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;tag-card&quot;</span>&gt;</span>hugo<span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">a</span> <span class=\"attr\">href</span>=<span class=\"string\">&quot;/tags/static-site-generator&quot;</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;tag-card&quot;</span>&gt;</span>static-site-generator<span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<hr>\n<p>不知道写啥，就写一个 Python 的二分搜索吧：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">bin_search</span>(<span class=\"params\">arr: <span class=\"built_in\">list</span>, target: <span class=\"type\">Any</span></span>) -&gt; <span class=\"built_in\">int</span>:</span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    Return the smallest index such that when target is inserted at that index,</span></span><br><span class=\"line\"><span class=\"string\">    the array will remain sorted.</span></span><br><span class=\"line\"><span class=\"string\">    &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    lo, hi = <span class=\"number\">0</span>, <span class=\"built_in\">len</span>(arr)</span><br><span class=\"line\">    <span class=\"keyword\">while</span> lo &lt; hi:</span><br><span class=\"line\">        m = (lo + hi) // <span class=\"number\">2</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> arr[m] &lt; target:</span><br><span class=\"line\">            lo = m + <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            hi = m</span><br><span class=\"line\">    <span class=\"keyword\">return</span> lo</span><br></pre></td></tr></table></figure>"},{"author":"陈英发 Yingfa Chen","title":"(EREN) Robust and Scalable Model Editing for Large Language Models","date":"2023-09-14T11:39:34.000Z","featured":true,"_content":"\nThis is my first ever stand-alone research work, I hope it can be accepted at EMNLP 2023, but judging from its review scores, it can only get EMNLP Findings. But that's acceptable.\n\n\n<!-- more -->\n\n\n> The first reviewer actually lowered the \"soundness\" score after rebuttal! WHY!\n\n[OpenReview link](https://openreview.net/forum?id=vDUsGqCIyb&noteId=K2D6oCGNlE)\n\n> TLDR: A reader is augmented with a growing notebook that caches all edits in natural texts, and the reader retrieves relevant edits and make inference based on them. This achieves SOTA in model editing in QA and fact-checking.\n\n***\n\n<strong>Abstract</strong>\n\n\n<div style=\"text-align: justify;\">\nThe memorized knowledge of large language models (LLMs) may not be consistent with that of the real world, and the model may produce undesired behaviors or incorrect predictions. Therefore, there is a need for model editing, i.e., modifying the behavior of an LLM on specific examples while preserving its performance on unrelated instances. Existing LLM editing methods modify the behavior by adding a prompt to the input, which is not scalable due to the limited length of the prompt and may negatively affect the performance on unrelated instances. In this paper, we propose EREN (Edit models by REading Notes) to improve the scalability and robustness of LLM editing. Specifically, EREN complements the LLM with a notebook that contains all edits in natural text and retrieves the relevant edits for a given input. To avoid the negative effect of irrelevant edits, we propose a two-step reading comprehension procedure to determine whether there is a relevant edit for the input. If not found, the LLM will make predictions directly. Empirical results on question-answering and fact-checking show that our method outperforms current state-of-the-art methods by a large margin. Unlike existing techniques, it can integrate knowledge from multiple edits, and correctly respond to syntactically similar but semantically unrelated inputs (and vice versa).\n</div>\n\n\n## Introduction\n\n...\n\n","source":"_drafts/EREN.md","raw":"---\nauthor: 陈英发 Yingfa Chen\ntitle: \"(EREN) Robust and Scalable Model Editing for Large Language Models\"\ndate: 2023-09-14 19:39:34\ncategories: Paper\ntags:\n- paper\n- arxiv\n- llm\n- knowledge\n- research\n- emnlp\n- english\n- model editing\n- in-context-learning\n- ai\n- serac\n- rome\n- eren\n- mend\nfeatured: true\n---\n\nThis is my first ever stand-alone research work, I hope it can be accepted at EMNLP 2023, but judging from its review scores, it can only get EMNLP Findings. But that's acceptable.\n\n\n<!-- more -->\n\n\n> The first reviewer actually lowered the \"soundness\" score after rebuttal! WHY!\n\n[OpenReview link](https://openreview.net/forum?id=vDUsGqCIyb&noteId=K2D6oCGNlE)\n\n> TLDR: A reader is augmented with a growing notebook that caches all edits in natural texts, and the reader retrieves relevant edits and make inference based on them. This achieves SOTA in model editing in QA and fact-checking.\n\n***\n\n<strong>Abstract</strong>\n\n\n<div style=\"text-align: justify;\">\nThe memorized knowledge of large language models (LLMs) may not be consistent with that of the real world, and the model may produce undesired behaviors or incorrect predictions. Therefore, there is a need for model editing, i.e., modifying the behavior of an LLM on specific examples while preserving its performance on unrelated instances. Existing LLM editing methods modify the behavior by adding a prompt to the input, which is not scalable due to the limited length of the prompt and may negatively affect the performance on unrelated instances. In this paper, we propose EREN (Edit models by REading Notes) to improve the scalability and robustness of LLM editing. Specifically, EREN complements the LLM with a notebook that contains all edits in natural text and retrieves the relevant edits for a given input. To avoid the negative effect of irrelevant edits, we propose a two-step reading comprehension procedure to determine whether there is a relevant edit for the input. If not found, the LLM will make predictions directly. Empirical results on question-answering and fact-checking show that our method outperforms current state-of-the-art methods by a large margin. Unlike existing techniques, it can integrate knowledge from multiple edits, and correctly respond to syntactically similar but semantically unrelated inputs (and vice versa).\n</div>\n\n\n## Introduction\n\n...\n\n","slug":"EREN","published":0,"updated":"2024-01-23T11:09:17.490Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cltl4oxem000wxh7kge2td07k","content":"<p>This is my first ever stand-alone research work, I hope it can be accepted at EMNLP 2023, but judging from its review scores, it can only get EMNLP Findings. But that's acceptable.</p>\n<span id=\"more\"></span>\n<blockquote>\n<p>The first reviewer actually lowered the \"soundness\" score after rebuttal! WHY!</p>\n</blockquote>\n<p><a href=\"https://openreview.net/forum?id=vDUsGqCIyb&amp;noteId=K2D6oCGNlE\">OpenReview link</a></p>\n<blockquote>\n<p>TLDR: A reader is augmented with a growing notebook that caches all edits in natural texts, and the reader retrieves relevant edits and make inference based on them. This achieves SOTA in model editing in QA and fact-checking.</p>\n</blockquote>\n<hr>\n<p><strong>Abstract</strong></p>\n<div style=\"text-align: justify;\">\nThe memorized knowledge of large language models (LLMs) may not be consistent with that of the real world, and the model may produce undesired behaviors or incorrect predictions. Therefore, there is a need for model editing, i.e., modifying the behavior of an LLM on specific examples while preserving its performance on unrelated instances. Existing LLM editing methods modify the behavior by adding a prompt to the input, which is not scalable due to the limited length of the prompt and may negatively affect the performance on unrelated instances. In this paper, we propose EREN (Edit models by REading Notes) to improve the scalability and robustness of LLM editing. Specifically, EREN complements the LLM with a notebook that contains all edits in natural text and retrieves the relevant edits for a given input. To avoid the negative effect of irrelevant edits, we propose a two-step reading comprehension procedure to determine whether there is a relevant edit for the input. If not found, the LLM will make predictions directly. Empirical results on question-answering and fact-checking show that our method outperforms current state-of-the-art methods by a large margin. Unlike existing techniques, it can integrate knowledge from multiple edits, and correctly respond to syntactically similar but semantically unrelated inputs (and vice versa).\n</div>\n<h2 id=\"Introduction\">Introduction</h2>\n<p>...</p>\n","site":{"data":{}},"excerpt":"<p>This is my first ever stand-alone research work, I hope it can be accepted at EMNLP 2023, but judging from its review scores, it can only get EMNLP Findings. But that's acceptable.</p>","more":"<blockquote>\n<p>The first reviewer actually lowered the &quot;soundness&quot; score after rebuttal! WHY!</p>\n</blockquote>\n<p><a href=\"https://openreview.net/forum?id=vDUsGqCIyb&amp;noteId=K2D6oCGNlE\">OpenReview link</a></p>\n<blockquote>\n<p>TLDR: A reader is augmented with a growing notebook that caches all edits in natural texts, and the reader retrieves relevant edits and make inference based on them. This achieves SOTA in model editing in QA and fact-checking.</p>\n</blockquote>\n<hr>\n<p><strong>Abstract</strong></p>\n<div style=\"text-align: justify;\">\nThe memorized knowledge of large language models (LLMs) may not be consistent with that of the real world, and the model may produce undesired behaviors or incorrect predictions. Therefore, there is a need for model editing, i.e., modifying the behavior of an LLM on specific examples while preserving its performance on unrelated instances. Existing LLM editing methods modify the behavior by adding a prompt to the input, which is not scalable due to the limited length of the prompt and may negatively affect the performance on unrelated instances. In this paper, we propose EREN (Edit models by REading Notes) to improve the scalability and robustness of LLM editing. Specifically, EREN complements the LLM with a notebook that contains all edits in natural text and retrieves the relevant edits for a given input. To avoid the negative effect of irrelevant edits, we propose a two-step reading comprehension procedure to determine whether there is a relevant edit for the input. If not found, the LLM will make predictions directly. Empirical results on question-answering and fact-checking show that our method outperforms current state-of-the-art methods by a large margin. Unlike existing techniques, it can integrate knowledge from multiple edits, and correctly respond to syntactically similar but semantically unrelated inputs (and vice versa).\n</div>\n<h2 id=\"Introduction\">Introduction</h2>\n<p>...</p>"},{"author":"陈英发 Yingfa Chen","title":"语言模型的进化","date":"2024-02-01T09:51:30.000Z","draft":true,"_content":"<style>\nsection {\n  font-size: 24px;\n}\n</style>\n\n自从 Transformer 问世以来，基于注意力机制的语言模型取得了极大的成功。现在的大语言模型，如 GPT、LLaMa、Mistral都是基于 Transformer 架构的。最近有诸多研究工作提出可能可以替代 transformer 架构的新架构。本文整理一下近些年语言模型的变化。\n\n<!-- more -->\n\n# 语言模型\n\n语言模型（language model）是一个对语言的概率模型，首次在八十年代提出。语言模型专注于预测一段文字出现的概率。\n\n$$\nP(s) = P(w_{1}, \\dots, w_{m}) = \\prod_{i}^{m} P(w_i|w_{1},\\dots, w_{i-1})\n$$\n\n其中 $w_i$ 一般是 token，而一般一个 token 就是一个单词。在以下的讨论中，如果没有特别说明，我默认一个 token 就是一个单词，而一段文字（比如一个句子）的不可分割单元就是一个单词。\n\n换句话说，这个问题可以转化为，给定一个序列 $w_{<t}$，计算下一个单词的概率分布：\n\n$$\nP(w_{t}|w_{<t}) \\in \\mathbb R^V\n$$\n\n其中 $V$ 是词表大小，$t$ 是当前要预测的单词的位置。这个目标叫做 next token prediction。\n\n# 语言模型的应用\n\n学习语言的表示，比如词向量（word embeddings），这是假设预测下一个单词所需要的信息可以被利用来完成其他任务。\n\n# 统计语言模型\n\n方法：统计每个序列出现的次数：\n\n$$\nP(w_t |w_{<t}) = \\frac{ 出现次数 ( w_{<t}, w_t)}{\\sum_{w} 出现次数 (w_{<t}, w)} ， \\quad w \\in V\n$$\n\n**问题**：单词的组合数量是 $O(V ^ t)$ 复杂度。\n\n## Word $n$-gram 语言模型\n\n$n$-gram 假设模型假设每个单词出现的概率仅取决于前 $n$ 个单词。可以写成：\n\n$$\nP(w_{i} | w_{1}, \\dots, w_{i-1}) \\approx P(w_{i}|w_{i- 1 (n-1)}, \\dots ,w_{i-1})\n$$\n\n## Bag-of-Words 模型\n\n假设概率跟顺序无关：\n\n$$\nP(w_{i} | w_{1}, \\dots, w_{i-1}) \\approx P(w_{i})\n$$\n\n---\n\n# 深度学习语言模型\n\n# Recurrent Neural Network (RNN)\n\n用一个 hidden state 来保存上下文信息（$w_{<t}$），然后用这个 hidden state 来预测下一个单词。\n\n![Recurrent Neural Network](%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%BF%9B%E5%8C%96/rnn.png)\n\n后面提出的 Long Short-Term Memory (LSTM) 和 Gated Recurrent Unit (GRU) 都是 RNN 的变种，只不过是对输入和 hidden state 的处理方式不同。\n\n两个问题：\n\n- 很难把所有上下文信息都压缩到一个 hidden state 里面。\n- 难以并行化：每个 token 对应的 hidden state 都依赖于前一个 token 的 hidden state，所以无法并行计算不同整个序列。\n\n# Transformer: Attention is All You Need\n\nSelf-Attention 机制如下。\n\n![Self-attention 机制示意图。](%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%BF%9B%E5%8C%96/self-attention.png)\n\n图中展示输入是 \"see that girl run\"，然后我们计算 \"that\" 的潜在表示。\n\n因为右边的向量（hidden representation）不依赖于其他 token 的 hidden representation，所以可以并行计算。\n\nTransformer 模型：\n\n$$\n\\begin{align*}\nX^{(0)} &= \\text{Embedding}(s) \\in \\mathbb R ^{N \\times d} \\\\ \nX^{(l)} &= \\text{Attention}\\left(X^{(l-1)}\\right) \\in \\mathbb R ^{N \\times d} \\\\\nX^{(l)} &= \\text{Norm}\\left(X^{(l)} + X^{(l-1)}\\right) \\in \\mathbb R ^{N \\times d} \\\\\nX^{(l)} &= \\text{FFN}\\left(X^{(l)}\\right) \\in \\mathbb R ^{N \\times d} \\\\\nX^{(l)} &= \\text{Norm}\\left(X^{(l)} + X^{(l-1)}\\right) \\in \\mathbb R ^{N \\times d} \\\\\n\\\\\n\\text{Attention}(X) &= \\text{softmax}\\left(\\frac{XW_{q}(XW_{k})^\\top}{\\sqrt{d}} \\right)XW_{v} \\\\\n\\end{align*}\n$$\n\n其中 $N$ 是序列长度，$d$ 是 hidden size，$s$ 是输入序列，$l$ 是层数，$W_{q}, W_{k}, W_{v}$ 是参数矩阵，$\\text{LN}$ 是 layer normalization，$\\text{FFN}$ 是一个跟 $t$ 无关的 feed-forward network。\n\n## Attention 的问题\n\nAttention 机制的计算复杂度是 $O(n^2)$。对于训练来说，这一般不是很大的问题。可是在推理的过程中，对于每个 token 都要计算一次 attention，也就是每生成一个 token 的复杂度是 $O(n)$。\n","source":"_drafts/语言模型的进化.md","raw":"---\nauthor: 陈英发 Yingfa Chen\ntitle: 语言模型的进化\ndate: 2024-02-01 17:51:30\ncategories:\ntags:\n- language modeling\n- language models\n- llm\n- transformer\n- nlp\n- rnn\n- deep learning\n- machine learning\n- research\n- ai\ndraft: true\n---\n<style>\nsection {\n  font-size: 24px;\n}\n</style>\n\n自从 Transformer 问世以来，基于注意力机制的语言模型取得了极大的成功。现在的大语言模型，如 GPT、LLaMa、Mistral都是基于 Transformer 架构的。最近有诸多研究工作提出可能可以替代 transformer 架构的新架构。本文整理一下近些年语言模型的变化。\n\n<!-- more -->\n\n# 语言模型\n\n语言模型（language model）是一个对语言的概率模型，首次在八十年代提出。语言模型专注于预测一段文字出现的概率。\n\n$$\nP(s) = P(w_{1}, \\dots, w_{m}) = \\prod_{i}^{m} P(w_i|w_{1},\\dots, w_{i-1})\n$$\n\n其中 $w_i$ 一般是 token，而一般一个 token 就是一个单词。在以下的讨论中，如果没有特别说明，我默认一个 token 就是一个单词，而一段文字（比如一个句子）的不可分割单元就是一个单词。\n\n换句话说，这个问题可以转化为，给定一个序列 $w_{<t}$，计算下一个单词的概率分布：\n\n$$\nP(w_{t}|w_{<t}) \\in \\mathbb R^V\n$$\n\n其中 $V$ 是词表大小，$t$ 是当前要预测的单词的位置。这个目标叫做 next token prediction。\n\n# 语言模型的应用\n\n学习语言的表示，比如词向量（word embeddings），这是假设预测下一个单词所需要的信息可以被利用来完成其他任务。\n\n# 统计语言模型\n\n方法：统计每个序列出现的次数：\n\n$$\nP(w_t |w_{<t}) = \\frac{ 出现次数 ( w_{<t}, w_t)}{\\sum_{w} 出现次数 (w_{<t}, w)} ， \\quad w \\in V\n$$\n\n**问题**：单词的组合数量是 $O(V ^ t)$ 复杂度。\n\n## Word $n$-gram 语言模型\n\n$n$-gram 假设模型假设每个单词出现的概率仅取决于前 $n$ 个单词。可以写成：\n\n$$\nP(w_{i} | w_{1}, \\dots, w_{i-1}) \\approx P(w_{i}|w_{i- 1 (n-1)}, \\dots ,w_{i-1})\n$$\n\n## Bag-of-Words 模型\n\n假设概率跟顺序无关：\n\n$$\nP(w_{i} | w_{1}, \\dots, w_{i-1}) \\approx P(w_{i})\n$$\n\n---\n\n# 深度学习语言模型\n\n# Recurrent Neural Network (RNN)\n\n用一个 hidden state 来保存上下文信息（$w_{<t}$），然后用这个 hidden state 来预测下一个单词。\n\n![Recurrent Neural Network](%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%BF%9B%E5%8C%96/rnn.png)\n\n后面提出的 Long Short-Term Memory (LSTM) 和 Gated Recurrent Unit (GRU) 都是 RNN 的变种，只不过是对输入和 hidden state 的处理方式不同。\n\n两个问题：\n\n- 很难把所有上下文信息都压缩到一个 hidden state 里面。\n- 难以并行化：每个 token 对应的 hidden state 都依赖于前一个 token 的 hidden state，所以无法并行计算不同整个序列。\n\n# Transformer: Attention is All You Need\n\nSelf-Attention 机制如下。\n\n![Self-attention 机制示意图。](%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%BF%9B%E5%8C%96/self-attention.png)\n\n图中展示输入是 \"see that girl run\"，然后我们计算 \"that\" 的潜在表示。\n\n因为右边的向量（hidden representation）不依赖于其他 token 的 hidden representation，所以可以并行计算。\n\nTransformer 模型：\n\n$$\n\\begin{align*}\nX^{(0)} &= \\text{Embedding}(s) \\in \\mathbb R ^{N \\times d} \\\\ \nX^{(l)} &= \\text{Attention}\\left(X^{(l-1)}\\right) \\in \\mathbb R ^{N \\times d} \\\\\nX^{(l)} &= \\text{Norm}\\left(X^{(l)} + X^{(l-1)}\\right) \\in \\mathbb R ^{N \\times d} \\\\\nX^{(l)} &= \\text{FFN}\\left(X^{(l)}\\right) \\in \\mathbb R ^{N \\times d} \\\\\nX^{(l)} &= \\text{Norm}\\left(X^{(l)} + X^{(l-1)}\\right) \\in \\mathbb R ^{N \\times d} \\\\\n\\\\\n\\text{Attention}(X) &= \\text{softmax}\\left(\\frac{XW_{q}(XW_{k})^\\top}{\\sqrt{d}} \\right)XW_{v} \\\\\n\\end{align*}\n$$\n\n其中 $N$ 是序列长度，$d$ 是 hidden size，$s$ 是输入序列，$l$ 是层数，$W_{q}, W_{k}, W_{v}$ 是参数矩阵，$\\text{LN}$ 是 layer normalization，$\\text{FFN}$ 是一个跟 $t$ 无关的 feed-forward network。\n\n## Attention 的问题\n\nAttention 机制的计算复杂度是 $O(n^2)$。对于训练来说，这一般不是很大的问题。可是在推理的过程中，对于每个 token 都要计算一次 attention，也就是每生成一个 token 的复杂度是 $O(n)$。\n","slug":"语言模型的进化","published":0,"updated":"2024-02-26T02:53:18.088Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cltl4oxem000yxh7kee7oacqe","content":"<p>自从 Transformer 问世以来，基于注意力机制的语言模型取得了极大的成功。现在的大语言模型，如 GPT、LLaMa、Mistral都是基于 Transformer 架构的。最近有诸多研究工作提出可能可以替代 transformer 架构的新架构。本文整理一下近些年语言模型的变化。</p>\n<span id=\"more\"></span>\n<h1>语言模型</h1>\n<p>语言模型（language model）是一个对语言的概率模型，首次在八十年代提出。语言模型专注于预测一段文字出现的概率。</p>\n<p>$$\nP(s) = P(w_{1}, \\dots, w_{m}) = \\prod_{i}^{m} P(w_i|w_{1},\\dots, w_{i-1})\n$$</p>\n<p>其中 $w_i$ 一般是 token，而一般一个 token 就是一个单词。在以下的讨论中，如果没有特别说明，我默认一个 token 就是一个单词，而一段文字（比如一个句子）的不可分割单元就是一个单词。</p>\n<p>换句话说，这个问题可以转化为，给定一个序列 $w_{&lt;t}$，计算下一个单词的概率分布：</p>\n<p>$$\nP(w_{t}|w_{&lt;t}) \\in \\mathbb R^V\n$$</p>\n<p>其中 $V$ 是词表大小，$t$ 是当前要预测的单词的位置。这个目标叫做 next token prediction。</p>\n<h1>语言模型的应用</h1>\n<p>学习语言的表示，比如词向量（word embeddings），这是假设预测下一个单词所需要的信息可以被利用来完成其他任务。</p>\n<h1>统计语言模型</h1>\n<p>方法：统计每个序列出现的次数：</p>\n<p>$$\nP(w_t |w_{&lt;t}) = \\frac{ 出现次数 ( w_{&lt;t}, w_t)}{\\sum_{w} 出现次数 (w_{&lt;t}, w)} ， \\quad w \\in V\n$$</p>\n<p><strong>问题</strong>：单词的组合数量是 $O(V ^ t)$ 复杂度。</p>\n<h2 id=\"Word-n-gram-语言模型\">Word $n$-gram 语言模型</h2>\n<p>$n$-gram 假设模型假设每个单词出现的概率仅取决于前 $n$ 个单词。可以写成：</p>\n<p>$$\nP(w_{i} | w_{1}, \\dots, w_{i-1}) \\approx P(w_{i}|w_{i- 1 (n-1)}, \\dots ,w_{i-1})\n$$</p>\n<h2 id=\"Bag-of-Words-模型\">Bag-of-Words 模型</h2>\n<p>假设概率跟顺序无关：</p>\n<p>$$\nP(w_{i} | w_{1}, \\dots, w_{i-1}) \\approx P(w_{i})\n$$</p>\n<hr>\n<h1>深度学习语言模型</h1>\n<h1>Recurrent Neural Network (RNN)</h1>\n<p>用一个 hidden state 来保存上下文信息（$w_{&lt;t}$），然后用这个 hidden state 来预测下一个单词。</p>\n<p><img src=\"%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%BF%9B%E5%8C%96/rnn.png\" alt=\"Recurrent Neural Network\"></p>\n<p>后面提出的 Long Short-Term Memory (LSTM) 和 Gated Recurrent Unit (GRU) 都是 RNN 的变种，只不过是对输入和 hidden state 的处理方式不同。</p>\n<p>两个问题：</p>\n<ul>\n<li>很难把所有上下文信息都压缩到一个 hidden state 里面。</li>\n<li>难以并行化：每个 token 对应的 hidden state 都依赖于前一个 token 的 hidden state，所以无法并行计算不同整个序列。</li>\n</ul>\n<h1>Transformer: Attention is All You Need</h1>\n<p>Self-Attention 机制如下。</p>\n<p><img src=\"%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%BF%9B%E5%8C%96/self-attention.png\" alt=\"Self-attention 机制示意图。\"></p>\n<p>图中展示输入是 \"see that girl run\"，然后我们计算 \"that\" 的潜在表示。</p>\n<p>因为右边的向量（hidden representation）不依赖于其他 token 的 hidden representation，所以可以并行计算。</p>\n<p>Transformer 模型：</p>\n<p>$$\n\\begin{align*}\nX^{(0)} &amp;= \\text{Embedding}(s) \\in \\mathbb R ^{N \\times d} \\\nX^{(l)} &amp;= \\text{Attention}\\left(X^{(l-1)}\\right) \\in \\mathbb R ^{N \\times d} \\\nX^{(l)} &amp;= \\text{Norm}\\left(X^{(l)} + X^{(l-1)}\\right) \\in \\mathbb R ^{N \\times d} \\\nX^{(l)} &amp;= \\text{FFN}\\left(X^{(l)}\\right) \\in \\mathbb R ^{N \\times d} \\\nX^{(l)} &amp;= \\text{Norm}\\left(X^{(l)} + X^{(l-1)}\\right) \\in \\mathbb R ^{N \\times d} \\\n\\\n\\text{Attention}(X) &amp;= \\text{softmax}\\left(\\frac{XW_{q}(XW_{k})^\\top}{\\sqrt{d}} \\right)XW_{v} \\\n\\end{align*}\n$$</p>\n<p>其中 $N$ 是序列长度，$d$ 是 hidden size，$s$ 是输入序列，$l$ 是层数，$W_{q}, W_{k}, W_{v}$ 是参数矩阵，$\\text{LN}$ 是 layer normalization，$\\text{FFN}$ 是一个跟 $t$ 无关的 feed-forward network。</p>\n<h2 id=\"Attention-的问题\">Attention 的问题</h2>\n<p>Attention 机制的计算复杂度是 $O(n^2)$。对于训练来说，这一般不是很大的问题。可是在推理的过程中，对于每个 token 都要计算一次 attention，也就是每生成一个 token 的复杂度是 $O(n)$。</p>\n","site":{"data":{}},"excerpt":"<p>自从 Transformer 问世以来，基于注意力机制的语言模型取得了极大的成功。现在的大语言模型，如 GPT、LLaMa、Mistral都是基于 Transformer 架构的。最近有诸多研究工作提出可能可以替代 transformer 架构的新架构。本文整理一下近些年语言模型的变化。</p>","more":"<h1>语言模型</h1>\n<p>语言模型（language model）是一个对语言的概率模型，首次在八十年代提出。语言模型专注于预测一段文字出现的概率。</p>\n<p>$$\nP(s) = P(w_{1}, \\dots, w_{m}) = \\prod_{i}^{m} P(w_i|w_{1},\\dots, w_{i-1})\n$$</p>\n<p>其中 $w_i$ 一般是 token，而一般一个 token 就是一个单词。在以下的讨论中，如果没有特别说明，我默认一个 token 就是一个单词，而一段文字（比如一个句子）的不可分割单元就是一个单词。</p>\n<p>换句话说，这个问题可以转化为，给定一个序列 $w_{&lt;t}$，计算下一个单词的概率分布：</p>\n<p>$$\nP(w_{t}|w_{&lt;t}) \\in \\mathbb R^V\n$$</p>\n<p>其中 $V$ 是词表大小，$t$ 是当前要预测的单词的位置。这个目标叫做 next token prediction。</p>\n<h1>语言模型的应用</h1>\n<p>学习语言的表示，比如词向量（word embeddings），这是假设预测下一个单词所需要的信息可以被利用来完成其他任务。</p>\n<h1>统计语言模型</h1>\n<p>方法：统计每个序列出现的次数：</p>\n<p>$$\nP(w_t |w_{&lt;t}) = \\frac{ 出现次数 ( w_{&lt;t}, w_t)}{\\sum_{w} 出现次数 (w_{&lt;t}, w)} ， \\quad w \\in V\n$$</p>\n<p><strong>问题</strong>：单词的组合数量是 $O(V ^ t)$ 复杂度。</p>\n<h2 id=\"Word-n-gram-语言模型\">Word $n$-gram 语言模型</h2>\n<p>$n$-gram 假设模型假设每个单词出现的概率仅取决于前 $n$ 个单词。可以写成：</p>\n<p>$$\nP(w_{i} | w_{1}, \\dots, w_{i-1}) \\approx P(w_{i}|w_{i- 1 (n-1)}, \\dots ,w_{i-1})\n$$</p>\n<h2 id=\"Bag-of-Words-模型\">Bag-of-Words 模型</h2>\n<p>假设概率跟顺序无关：</p>\n<p>$$\nP(w_{i} | w_{1}, \\dots, w_{i-1}) \\approx P(w_{i})\n$$</p>\n<hr>\n<h1>深度学习语言模型</h1>\n<h1>Recurrent Neural Network (RNN)</h1>\n<p>用一个 hidden state 来保存上下文信息（$w_{&lt;t}$），然后用这个 hidden state 来预测下一个单词。</p>\n<p><img src=\"%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%BF%9B%E5%8C%96/rnn.png\" alt=\"Recurrent Neural Network\"></p>\n<p>后面提出的 Long Short-Term Memory (LSTM) 和 Gated Recurrent Unit (GRU) 都是 RNN 的变种，只不过是对输入和 hidden state 的处理方式不同。</p>\n<p>两个问题：</p>\n<ul>\n<li>很难把所有上下文信息都压缩到一个 hidden state 里面。</li>\n<li>难以并行化：每个 token 对应的 hidden state 都依赖于前一个 token 的 hidden state，所以无法并行计算不同整个序列。</li>\n</ul>\n<h1>Transformer: Attention is All You Need</h1>\n<p>Self-Attention 机制如下。</p>\n<p><img src=\"%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%BF%9B%E5%8C%96/self-attention.png\" alt=\"Self-attention 机制示意图。\"></p>\n<p>图中展示输入是 &quot;see that girl run&quot;，然后我们计算 &quot;that&quot; 的潜在表示。</p>\n<p>因为右边的向量（hidden representation）不依赖于其他 token 的 hidden representation，所以可以并行计算。</p>\n<p>Transformer 模型：</p>\n<p>$$\n\\begin{align*}\nX^{(0)} &amp;= \\text{Embedding}(s) \\in \\mathbb R ^{N \\times d} \\\nX^{(l)} &amp;= \\text{Attention}\\left(X^{(l-1)}\\right) \\in \\mathbb R ^{N \\times d} \\\nX^{(l)} &amp;= \\text{Norm}\\left(X^{(l)} + X^{(l-1)}\\right) \\in \\mathbb R ^{N \\times d} \\\nX^{(l)} &amp;= \\text{FFN}\\left(X^{(l)}\\right) \\in \\mathbb R ^{N \\times d} \\\nX^{(l)} &amp;= \\text{Norm}\\left(X^{(l)} + X^{(l-1)}\\right) \\in \\mathbb R ^{N \\times d} \\\n\\\n\\text{Attention}(X) &amp;= \\text{softmax}\\left(\\frac{XW_{q}(XW_{k})^\\top}{\\sqrt{d}} \\right)XW_{v} \\\n\\end{align*}\n$$</p>\n<p>其中 $N$ 是序列长度，$d$ 是 hidden size，$s$ 是输入序列，$l$ 是层数，$W_{q}, W_{k}, W_{v}$ 是参数矩阵，$\\text{LN}$ 是 layer normalization，$\\text{FFN}$ 是一个跟 $t$ 无关的 feed-forward network。</p>\n<h2 id=\"Attention-的问题\">Attention 的问题</h2>\n<p>Attention 机制的计算复杂度是 $O(n^2)$。对于训练来说，这一般不是很大的问题。可是在推理的过程中，对于每个 token 都要计算一次 attention，也就是每生成一个 token 的复杂度是 $O(n)$。</p>"}],"PostAsset":[{"_id":"source/_posts/2023中秋/新天地-霸王茶姬.png","slug":"新天地-霸王茶姬.png","post":"cltl4oxed0001xh7k0ksv1gpa","modified":0,"renderable":0},{"_id":"source/_posts/2023中秋/武商梦时代.png","slug":"武商梦时代.png","post":"cltl4oxed0001xh7k0ksv1gpa","modified":0,"renderable":0},{"_id":"source/_posts/2023中秋/武汉欢乐谷.png","slug":"武汉欢乐谷.png","post":"cltl4oxed0001xh7k0ksv1gpa","modified":0,"renderable":0},{"_id":"source/_posts/2023中秋/解放公园中间.png","slug":"解放公园中间.png","post":"cltl4oxed0001xh7k0ksv1gpa","modified":0,"renderable":0},{"_id":"source/_posts/infinitebench/data-stat-pie.png","slug":"data-stat-pie.png","post":"cltl4oxeh0009xh7k6tym7gjd","modified":0,"renderable":0},{"_id":"source/_posts/infinitebench/results.png","slug":"results.png","post":"cltl4oxeh0009xh7k6tym7gjd","modified":0,"renderable":0},{"_id":"source/_posts/actadd/alg.png","slug":"alg.png","post":"cltl4oxeg0007xh7k5ati3i3b","modified":0,"renderable":0},{"_id":"source/_posts/actadd/method.png","slug":"method.png","post":"cltl4oxeg0007xh7k5ati3i3b","modified":0,"renderable":0},{"_id":"source/_posts/actadd/result.png","slug":"result.png","post":"cltl4oxeg0007xh7k5ati3i3b","modified":0,"renderable":0},{"_id":"source/_posts/更新个人主页/中国护照.jpg","slug":"中国护照.jpg","post":"cltl4oxej000nxh7kc2lgbk3d","modified":0,"renderable":0},{"_id":"source/_posts/更新个人主页/吃澳门菜.jpg","slug":"吃澳门菜.jpg","post":"cltl4oxej000nxh7kc2lgbk3d","modified":0,"renderable":0},{"_id":"source/_posts/更新个人主页/申请签证.jpg","slug":"申请签证.jpg","post":"cltl4oxej000nxh7kc2lgbk3d","modified":0,"renderable":0},{"_id":"source/_posts/第一个帖子，瞎写点东西/lillesand0.jpg","slug":"lillesand0.jpg","post":"cltl4oxel000rxh7kh777euay","modified":0,"renderable":0},{"_id":"source/_posts/第一个帖子，瞎写点东西/lillesand1.jpg","slug":"lillesand1.jpg","post":"cltl4oxel000rxh7kh777euay","modified":0,"renderable":0}],"PostCategory":[{"post_id":"cltl4oxed0001xh7k0ksv1gpa","category_id":"cltl4oxef0004xh7kdcffd02p","_id":"cltl4oxej000ixh7kaeivd2qx"},{"post_id":"cltl4oxee0003xh7k1kul88dt","category_id":"cltl4oxei000cxh7k8w255zde","_id":"cltl4oxek000oxh7k61v9ej27"},{"post_id":"cltl4oxej000mxh7k0gmq2mji","category_id":"cltl4oxef0004xh7kdcffd02p","_id":"cltl4oxem000txh7kcui3cwi0"},{"post_id":"cltl4oxeg0007xh7k5ati3i3b","category_id":"cltl4oxej000jxh7k86kp7os4","_id":"cltl4oxem000xxh7k3cds4zay"},{"post_id":"cltl4oxej000nxh7kc2lgbk3d","category_id":"cltl4oxef0004xh7kdcffd02p","_id":"cltl4oxen000zxh7k7w5t0q0g"},{"post_id":"cltl4oxel000rxh7kh777euay","category_id":"cltl4oxef0004xh7kdcffd02p","_id":"cltl4oxen0011xh7k08ru30mj"},{"post_id":"cltl4oxeh0009xh7k6tym7gjd","category_id":"cltl4oxei000cxh7k8w255zde","_id":"cltl4oxen0013xh7k51be6kda"},{"post_id":"cltl4oxel000sxh7k05fk4sya","category_id":"cltl4oxef0004xh7kdcffd02p","_id":"cltl4oxen0015xh7kccif5br3"},{"post_id":"cltl4oxeh000bxh7k5kpl1zp0","category_id":"cltl4oxem000uxh7k7oa1duf3","_id":"cltl4oxen0017xh7k2h991idi"},{"post_id":"cltl4oxei000fxh7k1qa8gbtd","category_id":"cltl4oxem000uxh7k7oa1duf3","_id":"cltl4oxen0018xh7k5z61cqcn"},{"post_id":"cltl4oxei000hxh7k0xuo6ggs","category_id":"cltl4oxen0014xh7k50v9dsv5","_id":"cltl4oxen001bxh7kgk3j0rxx"},{"post_id":"cltl4oxem000wxh7kge2td07k","category_id":"cltl4oxen0019xh7k621mhbtp","_id":"cltl4oxen001dxh7khyvgcs4q"}],"PostTag":[{"post_id":"cltl4oxed0001xh7k0ksv1gpa","tag_id":"cltl4oxeg0005xh7khank0y94","_id":"cltl4oxeo001fxh7k9y3cg7nw"},{"post_id":"cltl4oxed0001xh7k0ksv1gpa","tag_id":"cltl4oxei000dxh7k3v9vg1ls","_id":"cltl4oxeo001gxh7kh6e8czm1"},{"post_id":"cltl4oxed0001xh7k0ksv1gpa","tag_id":"cltl4oxej000kxh7k4mh56brx","_id":"cltl4oxeo001ixh7kfz9e5lx9"},{"post_id":"cltl4oxed0001xh7k0ksv1gpa","tag_id":"cltl4oxek000qxh7k14wog3li","_id":"cltl4oxeo001jxh7k8cf30g7m"},{"post_id":"cltl4oxed0001xh7k0ksv1gpa","tag_id":"cltl4oxem000vxh7k5w1p8d7e","_id":"cltl4oxeo001lxh7k1gf7f1ip"},{"post_id":"cltl4oxed0001xh7k0ksv1gpa","tag_id":"cltl4oxen0012xh7kcnebc3oc","_id":"cltl4oxeo001mxh7k1fxagksi"},{"post_id":"cltl4oxed0001xh7k0ksv1gpa","tag_id":"cltl4oxen0016xh7k7vsndyq6","_id":"cltl4oxeo001oxh7k9o5b03av"},{"post_id":"cltl4oxed0001xh7k0ksv1gpa","tag_id":"cltl4oxen001axh7k627279ga","_id":"cltl4oxeo001pxh7k6h3ldqgw"},{"post_id":"cltl4oxed0001xh7k0ksv1gpa","tag_id":"cltl4oxen001cxh7k9je4bmj2","_id":"cltl4oxeo001qxh7k6t6b8e9y"},{"post_id":"cltl4oxee0003xh7k1kul88dt","tag_id":"cltl4oxeo001exh7kgdn55uua","_id":"cltl4oxep0021xh7k7zgbcb4o"},{"post_id":"cltl4oxee0003xh7k1kul88dt","tag_id":"cltl4oxeo001hxh7kc5qi9ef7","_id":"cltl4oxep0022xh7kbdvaf5yd"},{"post_id":"cltl4oxee0003xh7k1kul88dt","tag_id":"cltl4oxeo001kxh7kazm95q38","_id":"cltl4oxep0024xh7kdtr07tep"},{"post_id":"cltl4oxee0003xh7k1kul88dt","tag_id":"cltl4oxeo001nxh7k2yws3zeq","_id":"cltl4oxep0025xh7k6fn615d3"},{"post_id":"cltl4oxee0003xh7k1kul88dt","tag_id":"cltl4oxek000qxh7k14wog3li","_id":"cltl4oxep0027xh7k8u279yuh"},{"post_id":"cltl4oxee0003xh7k1kul88dt","tag_id":"cltl4oxeo001sxh7kegqn7ff5","_id":"cltl4oxep0028xh7k399h52dh"},{"post_id":"cltl4oxee0003xh7k1kul88dt","tag_id":"cltl4oxeo001txh7k4fpn407l","_id":"cltl4oxep002axh7k4wymb8ex"},{"post_id":"cltl4oxee0003xh7k1kul88dt","tag_id":"cltl4oxeo001uxh7kfhrac527","_id":"cltl4oxep002bxh7kda663bl1"},{"post_id":"cltl4oxee0003xh7k1kul88dt","tag_id":"cltl4oxeo001vxh7kc6cufb5s","_id":"cltl4oxep002dxh7kgdol6ac2"},{"post_id":"cltl4oxee0003xh7k1kul88dt","tag_id":"cltl4oxeo001wxh7kc7dl7ghh","_id":"cltl4oxep002exh7k6qyo4ulx"},{"post_id":"cltl4oxee0003xh7k1kul88dt","tag_id":"cltl4oxeo001xxh7k52kjcz1e","_id":"cltl4oxep002fxh7kc0i94ke0"},{"post_id":"cltl4oxee0003xh7k1kul88dt","tag_id":"cltl4oxeo001yxh7k786udfz2","_id":"cltl4oxep002hxh7kczgp8lxm"},{"post_id":"cltl4oxee0003xh7k1kul88dt","tag_id":"cltl4oxeo001zxh7keq2406bh","_id":"cltl4oxep002ixh7k38itdead"},{"post_id":"cltl4oxeg0007xh7k5ati3i3b","tag_id":"cltl4oxeo001sxh7kegqn7ff5","_id":"cltl4oxep002oxh7k1t8504yt"},{"post_id":"cltl4oxeg0007xh7k5ati3i3b","tag_id":"cltl4oxep0023xh7k91jpa4xc","_id":"cltl4oxep002pxh7k6pqf8b48"},{"post_id":"cltl4oxeg0007xh7k5ati3i3b","tag_id":"cltl4oxep0026xh7k2xhl61nn","_id":"cltl4oxeq002rxh7kgufzaapd"},{"post_id":"cltl4oxeg0007xh7k5ati3i3b","tag_id":"cltl4oxep0029xh7karbxddgz","_id":"cltl4oxeq002sxh7kgd6h45ra"},{"post_id":"cltl4oxeg0007xh7k5ati3i3b","tag_id":"cltl4oxep002cxh7kch8h39zw","_id":"cltl4oxeq002uxh7k01mj2f4c"},{"post_id":"cltl4oxeg0007xh7k5ati3i3b","tag_id":"cltl4oxep002gxh7kdv61gdf4","_id":"cltl4oxeq002vxh7k69uq5mzr"},{"post_id":"cltl4oxeg0007xh7k5ati3i3b","tag_id":"cltl4oxep002jxh7k4g111uwu","_id":"cltl4oxeq002xxh7kgof5hifo"},{"post_id":"cltl4oxeg0007xh7k5ati3i3b","tag_id":"cltl4oxep002kxh7k99023bmu","_id":"cltl4oxeq002yxh7k2vr4cgiu"},{"post_id":"cltl4oxeg0007xh7k5ati3i3b","tag_id":"cltl4oxep002lxh7kbk6594qr","_id":"cltl4oxeq0030xh7key6u8qj2"},{"post_id":"cltl4oxeg0007xh7k5ati3i3b","tag_id":"cltl4oxep002mxh7k0fuy6d90","_id":"cltl4oxeq0031xh7k4upr5nmi"},{"post_id":"cltl4oxeh0009xh7k6tym7gjd","tag_id":"cltl4oxeo001hxh7kc5qi9ef7","_id":"cltl4oxeq0036xh7k3vf62rg3"},{"post_id":"cltl4oxeh0009xh7k6tym7gjd","tag_id":"cltl4oxep0026xh7k2xhl61nn","_id":"cltl4oxeq0037xh7kb3d44aca"},{"post_id":"cltl4oxeh0009xh7k6tym7gjd","tag_id":"cltl4oxeq002txh7ka5atb1wn","_id":"cltl4oxeq0039xh7kfd62gm5r"},{"post_id":"cltl4oxeh0009xh7k6tym7gjd","tag_id":"cltl4oxeq002wxh7k11lraz4y","_id":"cltl4oxeq003axh7kfrmjfmyk"},{"post_id":"cltl4oxeh0009xh7k6tym7gjd","tag_id":"cltl4oxeq002zxh7kgms546b8","_id":"cltl4oxeq003cxh7k1bk55rt7"},{"post_id":"cltl4oxeh0009xh7k6tym7gjd","tag_id":"cltl4oxeq0032xh7kbub7f1t4","_id":"cltl4oxeq003dxh7kbqclgw0o"},{"post_id":"cltl4oxeh0009xh7k6tym7gjd","tag_id":"cltl4oxeq0033xh7kh8189guc","_id":"cltl4oxeq003fxh7k00z42dhu"},{"post_id":"cltl4oxeh0009xh7k6tym7gjd","tag_id":"cltl4oxeq0034xh7k002s4pqb","_id":"cltl4oxer003gxh7k1kpc748u"},{"post_id":"cltl4oxeh000bxh7k5kpl1zp0","tag_id":"cltl4oxeo001sxh7kegqn7ff5","_id":"cltl4oxer003mxh7kfau08dkf"},{"post_id":"cltl4oxeh000bxh7k5kpl1zp0","tag_id":"cltl4oxeq0038xh7kgyvv6tq1","_id":"cltl4oxer003nxh7kg1qc87c3"},{"post_id":"cltl4oxeh000bxh7k5kpl1zp0","tag_id":"cltl4oxep002kxh7k99023bmu","_id":"cltl4oxer003pxh7kd5z6h7sx"},{"post_id":"cltl4oxeh000bxh7k5kpl1zp0","tag_id":"cltl4oxeq003exh7kahxrblg0","_id":"cltl4oxer003qxh7k0g977sr6"},{"post_id":"cltl4oxeh000bxh7k5kpl1zp0","tag_id":"cltl4oxer003hxh7kh1m1hyfk","_id":"cltl4oxer003sxh7k55pl142i"},{"post_id":"cltl4oxeh000bxh7k5kpl1zp0","tag_id":"cltl4oxer003ixh7kcfzxdza6","_id":"cltl4oxer003txh7k6qppdl8h"},{"post_id":"cltl4oxeh000bxh7k5kpl1zp0","tag_id":"cltl4oxep0026xh7k2xhl61nn","_id":"cltl4oxer003vxh7k86uz475g"},{"post_id":"cltl4oxeh000bxh7k5kpl1zp0","tag_id":"cltl4oxer003kxh7k55md4emg","_id":"cltl4oxer003wxh7kanx2fka5"},{"post_id":"cltl4oxei000fxh7k1qa8gbtd","tag_id":"cltl4oxeo001sxh7kegqn7ff5","_id":"cltl4oxes0048xh7karkx73nv"},{"post_id":"cltl4oxei000fxh7k1qa8gbtd","tag_id":"cltl4oxeo001hxh7kc5qi9ef7","_id":"cltl4oxes0049xh7k8a0hc8ot"},{"post_id":"cltl4oxei000fxh7k1qa8gbtd","tag_id":"cltl4oxep0026xh7k2xhl61nn","_id":"cltl4oxes004bxh7kcjwj81iy"},{"post_id":"cltl4oxei000fxh7k1qa8gbtd","tag_id":"cltl4oxeo001wxh7kc7dl7ghh","_id":"cltl4oxes004cxh7k82pfe6b6"},{"post_id":"cltl4oxei000fxh7k1qa8gbtd","tag_id":"cltl4oxer003xxh7kgkyp9vgf","_id":"cltl4oxes004exh7kb0ab5rsz"},{"post_id":"cltl4oxei000fxh7k1qa8gbtd","tag_id":"cltl4oxer003yxh7k8m2l3fvy","_id":"cltl4oxes004fxh7kehnq2cy5"},{"post_id":"cltl4oxei000fxh7k1qa8gbtd","tag_id":"cltl4oxep0023xh7k91jpa4xc","_id":"cltl4oxes004hxh7k6xzmhcl2"},{"post_id":"cltl4oxei000fxh7k1qa8gbtd","tag_id":"cltl4oxes0040xh7khish02ig","_id":"cltl4oxes004ixh7k45svh3yf"},{"post_id":"cltl4oxei000fxh7k1qa8gbtd","tag_id":"cltl4oxes0041xh7k1urp35be","_id":"cltl4oxes004jxh7kbg8jcamd"},{"post_id":"cltl4oxei000fxh7k1qa8gbtd","tag_id":"cltl4oxeg0005xh7khank0y94","_id":"cltl4oxes004lxh7kb5r48gpy"},{"post_id":"cltl4oxei000fxh7k1qa8gbtd","tag_id":"cltl4oxes0042xh7kb7ifej2q","_id":"cltl4oxes004mxh7kcka4es56"},{"post_id":"cltl4oxei000fxh7k1qa8gbtd","tag_id":"cltl4oxes0043xh7k2kxh10wn","_id":"cltl4oxes004oxh7kcd8f2z0x"},{"post_id":"cltl4oxei000fxh7k1qa8gbtd","tag_id":"cltl4oxes0044xh7k9spvftri","_id":"cltl4oxes004pxh7k1v6ddnmb"},{"post_id":"cltl4oxei000fxh7k1qa8gbtd","tag_id":"cltl4oxes0045xh7k06c5dz5i","_id":"cltl4oxet004rxh7kg4ezby12"},{"post_id":"cltl4oxei000fxh7k1qa8gbtd","tag_id":"cltl4oxes0046xh7k6hew8dhg","_id":"cltl4oxet004sxh7k3kkz2snc"},{"post_id":"cltl4oxei000hxh7k0xuo6ggs","tag_id":"cltl4oxes0047xh7kg8bq575r","_id":"cltl4oxet004vxh7k5ftefef8"},{"post_id":"cltl4oxei000hxh7k0xuo6ggs","tag_id":"cltl4oxes004axh7kdh417zp3","_id":"cltl4oxet004wxh7k8gwu1mqc"},{"post_id":"cltl4oxei000hxh7k0xuo6ggs","tag_id":"cltl4oxes004dxh7k0gww98jp","_id":"cltl4oxet004yxh7kg5tb5w9b"},{"post_id":"cltl4oxei000hxh7k0xuo6ggs","tag_id":"cltl4oxes004gxh7kaoq3173t","_id":"cltl4oxet004zxh7k19p6evjr"},{"post_id":"cltl4oxei000hxh7k0xuo6ggs","tag_id":"cltl4oxes004kxh7k5co2ep2e","_id":"cltl4oxet0051xh7kdvzjdxkn"},{"post_id":"cltl4oxei000hxh7k0xuo6ggs","tag_id":"cltl4oxes004nxh7ke5mq0ibm","_id":"cltl4oxet0052xh7k948x8lte"},{"post_id":"cltl4oxei000hxh7k0xuo6ggs","tag_id":"cltl4oxeo001sxh7kegqn7ff5","_id":"cltl4oxet0054xh7k0xnd0tms"},{"post_id":"cltl4oxei000hxh7k0xuo6ggs","tag_id":"cltl4oxet004txh7k91p7ajon","_id":"cltl4oxet0055xh7k0qpaa8or"},{"post_id":"cltl4oxej000mxh7k0gmq2mji","tag_id":"cltl4oxeg0005xh7khank0y94","_id":"cltl4oxet0057xh7kee0a6u0v"},{"post_id":"cltl4oxej000mxh7k0gmq2mji","tag_id":"cltl4oxek000qxh7k14wog3li","_id":"cltl4oxet0058xh7k4yjq90qp"},{"post_id":"cltl4oxej000mxh7k0gmq2mji","tag_id":"cltl4oxet004xxh7k6vcq0rfb","_id":"cltl4oxet005axh7k9tkodzlu"},{"post_id":"cltl4oxej000mxh7k0gmq2mji","tag_id":"cltl4oxet0050xh7kew2b65ub","_id":"cltl4oxet005bxh7kf2ho5mre"},{"post_id":"cltl4oxej000mxh7k0gmq2mji","tag_id":"cltl4oxej000kxh7k4mh56brx","_id":"cltl4oxet005cxh7k2iy85xxl"},{"post_id":"cltl4oxej000nxh7kc2lgbk3d","tag_id":"cltl4oxeg0005xh7khank0y94","_id":"cltl4oxeu005rxh7k3v4rhiqi"},{"post_id":"cltl4oxej000nxh7kc2lgbk3d","tag_id":"cltl4oxet0053xh7k3l294op5","_id":"cltl4oxeu005sxh7kcdim668j"},{"post_id":"cltl4oxej000nxh7kc2lgbk3d","tag_id":"cltl4oxej000kxh7k4mh56brx","_id":"cltl4oxeu005uxh7kd5hod6gn"},{"post_id":"cltl4oxej000nxh7kc2lgbk3d","tag_id":"cltl4oxek000qxh7k14wog3li","_id":"cltl4oxeu005vxh7k6ed778d9"},{"post_id":"cltl4oxej000nxh7kc2lgbk3d","tag_id":"cltl4oxet0059xh7k1hqhcxbj","_id":"cltl4oxeu005xxh7k45ihgwd0"},{"post_id":"cltl4oxej000nxh7kc2lgbk3d","tag_id":"cltl4oxet005dxh7kbbvy8hbj","_id":"cltl4oxeu005yxh7kfyssh80n"},{"post_id":"cltl4oxej000nxh7kc2lgbk3d","tag_id":"cltl4oxet005exh7k3bfue7h4","_id":"cltl4oxeu0060xh7k8q3xd7uo"},{"post_id":"cltl4oxej000nxh7kc2lgbk3d","tag_id":"cltl4oxet005fxh7kd5fz3keb","_id":"cltl4oxeu0061xh7k1emt976q"},{"post_id":"cltl4oxej000nxh7kc2lgbk3d","tag_id":"cltl4oxet005gxh7ka0n7csv5","_id":"cltl4oxeu0063xh7kcjud8ubv"},{"post_id":"cltl4oxej000nxh7kc2lgbk3d","tag_id":"cltl4oxeo001hxh7kc5qi9ef7","_id":"cltl4oxeu0064xh7k04py02vb"},{"post_id":"cltl4oxej000nxh7kc2lgbk3d","tag_id":"cltl4oxet005ixh7kb9um7ou9","_id":"cltl4oxev0066xh7k8ffb4boc"},{"post_id":"cltl4oxej000nxh7kc2lgbk3d","tag_id":"cltl4oxeu005jxh7k75a3d4sr","_id":"cltl4oxev0067xh7kba35a5mn"},{"post_id":"cltl4oxej000nxh7kc2lgbk3d","tag_id":"cltl4oxes0040xh7khish02ig","_id":"cltl4oxev0068xh7kb6v0hrkm"},{"post_id":"cltl4oxej000nxh7kc2lgbk3d","tag_id":"cltl4oxeu005lxh7k5wm7csac","_id":"cltl4oxev006axh7k4easbkz0"},{"post_id":"cltl4oxej000nxh7kc2lgbk3d","tag_id":"cltl4oxeu005mxh7kb9gqgy6k","_id":"cltl4oxev006bxh7k6v4m5stg"},{"post_id":"cltl4oxej000nxh7kc2lgbk3d","tag_id":"cltl4oxeu005nxh7kcsfm7yf2","_id":"cltl4oxev006dxh7k0r5geqvq"},{"post_id":"cltl4oxej000nxh7kc2lgbk3d","tag_id":"cltl4oxeu005oxh7kac9v31fc","_id":"cltl4oxev006exh7kh04xbv9z"},{"post_id":"cltl4oxej000nxh7kc2lgbk3d","tag_id":"cltl4oxeu005pxh7kdlg035y5","_id":"cltl4oxev006gxh7k1fokdssd"},{"post_id":"cltl4oxel000rxh7kh777euay","tag_id":"cltl4oxeg0005xh7khank0y94","_id":"cltl4oxew006uxh7kd0g1b08x"},{"post_id":"cltl4oxel000rxh7kh777euay","tag_id":"cltl4oxet004xxh7k6vcq0rfb","_id":"cltl4oxew006vxh7kcmidb1qz"},{"post_id":"cltl4oxel000rxh7kh777euay","tag_id":"cltl4oxeo001hxh7kc5qi9ef7","_id":"cltl4oxew006xxh7khlgyasz4"},{"post_id":"cltl4oxel000rxh7kh777euay","tag_id":"cltl4oxek000qxh7k14wog3li","_id":"cltl4oxew006yxh7kcmxr7px0"},{"post_id":"cltl4oxel000rxh7kh777euay","tag_id":"cltl4oxeu005wxh7k4h0w8u5e","_id":"cltl4oxew0070xh7k2esh3ybv"},{"post_id":"cltl4oxel000rxh7kh777euay","tag_id":"cltl4oxeu005zxh7kh6dv8423","_id":"cltl4oxew0071xh7kch859tnw"},{"post_id":"cltl4oxel000rxh7kh777euay","tag_id":"cltl4oxeu0062xh7kfro8hoiw","_id":"cltl4oxew0073xh7k44btchik"},{"post_id":"cltl4oxel000rxh7kh777euay","tag_id":"cltl4oxeu0065xh7k29jsd8v4","_id":"cltl4oxew0074xh7k5dwd9e2e"},{"post_id":"cltl4oxel000rxh7kh777euay","tag_id":"cltl4oxev0069xh7kgufo3xyx","_id":"cltl4oxew0076xh7kay7afwxx"},{"post_id":"cltl4oxel000rxh7kh777euay","tag_id":"cltl4oxev006cxh7kes1wh3vs","_id":"cltl4oxew0077xh7k92etawuy"},{"post_id":"cltl4oxel000rxh7kh777euay","tag_id":"cltl4oxev006fxh7k6rng5u0v","_id":"cltl4oxew0079xh7k55jz8c2q"},{"post_id":"cltl4oxel000rxh7kh777euay","tag_id":"cltl4oxev006hxh7k4cn8hcq9","_id":"cltl4oxew007axh7k87bz8v6m"},{"post_id":"cltl4oxel000rxh7kh777euay","tag_id":"cltl4oxej000kxh7k4mh56brx","_id":"cltl4oxew007cxh7k6iz3axmp"},{"post_id":"cltl4oxel000rxh7kh777euay","tag_id":"cltl4oxev006ixh7kgdcd29dc","_id":"cltl4oxew007dxh7kc7ndawdj"},{"post_id":"cltl4oxel000rxh7kh777euay","tag_id":"cltl4oxev006jxh7k1ejk02sv","_id":"cltl4oxew007fxh7kb5f57osx"},{"post_id":"cltl4oxel000rxh7kh777euay","tag_id":"cltl4oxev006kxh7khf8cducv","_id":"cltl4oxew007gxh7kcydcas6p"},{"post_id":"cltl4oxel000rxh7kh777euay","tag_id":"cltl4oxev006lxh7k09zw95ci","_id":"cltl4oxew007ixh7k3iv740ob"},{"post_id":"cltl4oxel000rxh7kh777euay","tag_id":"cltl4oxev006mxh7kcncec60u","_id":"cltl4oxex007jxh7k5lsi3qcm"},{"post_id":"cltl4oxel000rxh7kh777euay","tag_id":"cltl4oxev006nxh7kbyzd7r0f","_id":"cltl4oxex007lxh7kfruie3o0"},{"post_id":"cltl4oxel000rxh7kh777euay","tag_id":"cltl4oxev006oxh7kaehpf2kl","_id":"cltl4oxex007mxh7kha08d2wo"},{"post_id":"cltl4oxel000rxh7kh777euay","tag_id":"cltl4oxeo001kxh7kazm95q38","_id":"cltl4oxex007nxh7k9erk1sah"},{"post_id":"cltl4oxel000rxh7kh777euay","tag_id":"cltl4oxev006qxh7k6wcvhswi","_id":"cltl4oxex007pxh7kb05odzes"},{"post_id":"cltl4oxel000rxh7kh777euay","tag_id":"cltl4oxev006rxh7k63p98qte","_id":"cltl4oxex007qxh7kb32x190q"},{"post_id":"cltl4oxel000rxh7kh777euay","tag_id":"cltl4oxev006sxh7k32yp31ad","_id":"cltl4oxex007sxh7kadb18we1"},{"post_id":"cltl4oxel000sxh7k05fk4sya","tag_id":"cltl4oxeg0005xh7khank0y94","_id":"cltl4oxex007txh7k7pv6g5oa"},{"post_id":"cltl4oxel000sxh7k05fk4sya","tag_id":"cltl4oxet005fxh7kd5fz3keb","_id":"cltl4oxex007vxh7k3716evfa"},{"post_id":"cltl4oxel000sxh7k05fk4sya","tag_id":"cltl4oxes004nxh7ke5mq0ibm","_id":"cltl4oxex007wxh7k1zvq3a6i"},{"post_id":"cltl4oxel000sxh7k05fk4sya","tag_id":"cltl4oxet005exh7k3bfue7h4","_id":"cltl4oxex007yxh7kblticc8x"},{"post_id":"cltl4oxel000sxh7k05fk4sya","tag_id":"cltl4oxet005gxh7ka0n7csv5","_id":"cltl4oxex007zxh7k1y6i3m46"},{"post_id":"cltl4oxel000sxh7k05fk4sya","tag_id":"cltl4oxej000kxh7k4mh56brx","_id":"cltl4oxex0081xh7k36ky2cfh"},{"post_id":"cltl4oxel000sxh7k05fk4sya","tag_id":"cltl4oxew0075xh7kd5geekx0","_id":"cltl4oxex0082xh7kgimmbw1d"},{"post_id":"cltl4oxel000sxh7k05fk4sya","tag_id":"cltl4oxew0078xh7kglay1tj4","_id":"cltl4oxex0084xh7k3ztlcypq"},{"post_id":"cltl4oxem000wxh7kge2td07k","tag_id":"cltl4oxeo001exh7kgdn55uua","_id":"cltl4oxey008axh7kaf49cx32"},{"post_id":"cltl4oxem000wxh7kge2td07k","tag_id":"cltl4oxew007exh7kewrv7u8o","_id":"cltl4oxey008bxh7kb0paejak"},{"post_id":"cltl4oxem000wxh7kge2td07k","tag_id":"cltl4oxep0026xh7k2xhl61nn","_id":"cltl4oxey008dxh7kebbg1dlj"},{"post_id":"cltl4oxem000wxh7kge2td07k","tag_id":"cltl4oxex007kxh7kgaqn0nnw","_id":"cltl4oxey008exh7k0r0fgnsi"},{"post_id":"cltl4oxem000wxh7kge2td07k","tag_id":"cltl4oxeo001hxh7kc5qi9ef7","_id":"cltl4oxey008gxh7kdeoe31lf"},{"post_id":"cltl4oxem000wxh7kge2td07k","tag_id":"cltl4oxex007rxh7k0wxb4wnj","_id":"cltl4oxey008hxh7kcndpdncq"},{"post_id":"cltl4oxem000wxh7kge2td07k","tag_id":"cltl4oxeo001sxh7kegqn7ff5","_id":"cltl4oxey008jxh7kgiwjgmfe"},{"post_id":"cltl4oxem000wxh7kge2td07k","tag_id":"cltl4oxex007xxh7k9exihhsl","_id":"cltl4oxey008kxh7kckrxah1f"},{"post_id":"cltl4oxem000wxh7kge2td07k","tag_id":"cltl4oxex0080xh7k8285833d","_id":"cltl4oxey008mxh7kfnc1gdyb"},{"post_id":"cltl4oxem000wxh7kge2td07k","tag_id":"cltl4oxex0083xh7kcy0v7td3","_id":"cltl4oxey008nxh7k2coq7wfc"},{"post_id":"cltl4oxem000wxh7kge2td07k","tag_id":"cltl4oxex0085xh7k6n8q8ah4","_id":"cltl4oxey008pxh7k1m9320f9"},{"post_id":"cltl4oxem000wxh7kge2td07k","tag_id":"cltl4oxex0086xh7ke1ztd0nq","_id":"cltl4oxey008qxh7k6bpyhudw"},{"post_id":"cltl4oxem000wxh7kge2td07k","tag_id":"cltl4oxes0043xh7k2kxh10wn","_id":"cltl4oxey008sxh7k8b2x0dq4"},{"post_id":"cltl4oxem000wxh7kge2td07k","tag_id":"cltl4oxex0088xh7k3ivpfxk4","_id":"cltl4oxey008txh7ke1dgboku"},{"post_id":"cltl4oxem000yxh7kee7oacqe","tag_id":"cltl4oxex0089xh7kc78g7555","_id":"cltl4oxey008xxh7kdekrbgac"},{"post_id":"cltl4oxem000yxh7kee7oacqe","tag_id":"cltl4oxey008cxh7k6gnu5v7b","_id":"cltl4oxey008yxh7k90wn65qx"},{"post_id":"cltl4oxem000yxh7kee7oacqe","tag_id":"cltl4oxep0026xh7k2xhl61nn","_id":"cltl4oxey008zxh7k9iw6a2kw"},{"post_id":"cltl4oxem000yxh7kee7oacqe","tag_id":"cltl4oxeq0034xh7k002s4pqb","_id":"cltl4oxey0090xh7k403j13ig"},{"post_id":"cltl4oxem000yxh7kee7oacqe","tag_id":"cltl4oxeq002txh7ka5atb1wn","_id":"cltl4oxey0091xh7k3b63b33f"},{"post_id":"cltl4oxem000yxh7kee7oacqe","tag_id":"cltl4oxey008oxh7khcnpb0fy","_id":"cltl4oxey0092xh7kd5y1cmkb"},{"post_id":"cltl4oxem000yxh7kee7oacqe","tag_id":"cltl4oxey008rxh7ke95rfcgv","_id":"cltl4oxey0093xh7keixpfbls"},{"post_id":"cltl4oxem000yxh7kee7oacqe","tag_id":"cltl4oxey008uxh7k6lw9e97m","_id":"cltl4oxey0094xh7k26u62zto"},{"post_id":"cltl4oxem000yxh7kee7oacqe","tag_id":"cltl4oxeo001hxh7kc5qi9ef7","_id":"cltl4oxey0095xh7k3jm825b4"},{"post_id":"cltl4oxem000yxh7kee7oacqe","tag_id":"cltl4oxex0083xh7kcy0v7td3","_id":"cltl4oxey0096xh7k87auhjpv"}],"Tag":[{"name":"life","_id":"cltl4oxeg0005xh7khank0y94"},{"name":"中秋","_id":"cltl4oxei000dxh7k3v9vg1ls"},{"name":"中文","_id":"cltl4oxej000kxh7k4mh56brx"},{"name":"00","_id":"cltl4oxek000qxh7k14wog3li"},{"name":"wedding","_id":"cltl4oxem000vxh7k5w1p8d7e"},{"name":"中秋-middle-autumn","_id":"cltl4oxen0012xh7kcnebc3oc"},{"name":"国庆-national-day","_id":"cltl4oxen0016xh7k7vsndyq6"},{"name":"应城","_id":"cltl4oxen001axh7k627279ga"},{"name":"武汉-wuhan","_id":"cltl4oxen001cxh7k9je4bmj2"},{"name":"paper","_id":"cltl4oxeo001exh7kgdn55uua"},{"name":"research","_id":"cltl4oxeo001hxh7kc5qi9ef7"},{"name":"cfd","_id":"cltl4oxeo001kxh7kazm95q38"},{"name":"dataset","_id":"cltl4oxeo001nxh7k2yws3zeq"},{"name":"english","_id":"cltl4oxeo001sxh7kegqn7ff5"},{"name":"pinn","_id":"cltl4oxeo001txh7k4fpn407l"},{"name":"fno","_id":"cltl4oxeo001uxh7kfhrac527"},{"name":"physics","_id":"cltl4oxeo001vxh7kc6cufb5s"},{"name":"machine-learning","_id":"cltl4oxeo001wxh7kc7dl7ghh"},{"name":"deep-learning","_id":"cltl4oxeo001xxh7k52kjcz1e"},{"name":"deeponet","_id":"cltl4oxeo001yxh7k786udfz2"},{"name":"ai4science","_id":"cltl4oxeo001zxh7keq2406bh"},{"name":"ai-alignment","_id":"cltl4oxep0023xh7k91jpa4xc"},{"name":"llm","_id":"cltl4oxep0026xh7k2xhl61nn"},{"name":"gpt","_id":"cltl4oxep0029xh7karbxddgz"},{"name":"activation-modification","_id":"cltl4oxep002cxh7kch8h39zw"},{"name":"adaptation","_id":"cltl4oxep002gxh7kdv61gdf4"},{"name":"model-editing","_id":"cltl4oxep002jxh7k4g111uwu"},{"name":"representation-engineering","_id":"cltl4oxep002kxh7k99023bmu"},{"name":"fine-tuning","_id":"cltl4oxep002lxh7kbk6594qr"},{"name":"parameter-efficient-tuning","_id":"cltl4oxep002mxh7k0fuy6d90"},{"name":"nlp","_id":"cltl4oxeq002txh7ka5atb1wn"},{"name":"long-context","_id":"cltl4oxeq002wxh7k11lraz4y"},{"name":"benchmark","_id":"cltl4oxeq002zxh7kgms546b8"},{"name":"recurrence","_id":"cltl4oxeq0032xh7kbub7f1t4"},{"name":"linear-attention","_id":"cltl4oxeq0033xh7kh8189guc"},{"name":"transformer","_id":"cltl4oxeq0034xh7k002s4pqb"},{"name":"activation-engineering","_id":"cltl4oxeq0038xh7kgyvv6tq1"},{"name":"interpretability","_id":"cltl4oxeq003exh7kahxrblg0"},{"name":"rl","_id":"cltl4oxer003hxh7kh1m1hyfk"},{"name":"alignment","_id":"cltl4oxer003ixh7kcfzxdza6"},{"name":"maze","_id":"cltl4oxer003kxh7k55md4emg"},{"name":"ethics","_id":"cltl4oxer003xxh7kgkyp9vgf"},{"name":"safety","_id":"cltl4oxer003yxh7k8m2l3fvy"},{"name":"面壁智能","_id":"cltl4oxes0040xh7khish02ig"},{"name":"modelbest","_id":"cltl4oxes0041xh7k1urp35be"},{"name":"tutorial","_id":"cltl4oxes0042xh7kb7ifej2q"},{"name":"eren","_id":"cltl4oxes0043xh7k2kxh10wn"},{"name":"chatgpt","_id":"cltl4oxes0044xh7k9spvftri"},{"name":"claude","_id":"cltl4oxes0045xh7k06c5dz5i"},{"name":"三体","_id":"cltl4oxes0046xh7k6hew8dhg"},{"name":"algorithm","_id":"cltl4oxes0047xh7kg8bq575r"},{"name":"binary-search","_id":"cltl4oxes004axh7kdh417zp3"},{"name":"rust","_id":"cltl4oxes004dxh7k0gww98jp"},{"name":"python","_id":"cltl4oxes004gxh7kaoq3173t"},{"name":"c++","_id":"cltl4oxes004kxh7k5co2ep2e"},{"name":"test","_id":"cltl4oxes004nxh7ke5mq0ibm"},{"name":"code","_id":"cltl4oxet004txh7k91p7ajon"},{"name":"school","_id":"cltl4oxet004xxh7k6vcq0rfb"},{"name":"graduation","_id":"cltl4oxet0050xh7kew2b65ub"},{"name":"blog","_id":"cltl4oxet0053xh7k3l294op5"},{"name":"签证","_id":"cltl4oxet0059xh7k1hqhcxbj"},{"name":"hexo","_id":"cltl4oxet005dxh7kbbvy8hbj"},{"name":"hugo","_id":"cltl4oxet005exh7k3bfue7h4"},{"name":"jekyll","_id":"cltl4oxet005fxh7kd5fz3keb"},{"name":"static-site-generator","_id":"cltl4oxet005gxh7ka0n7csv5"},{"name":"羽毛球","_id":"cltl4oxet005ixh7kb9um7ou9"},{"name":"work","_id":"cltl4oxeu005jxh7k75a3d4sr"},{"name":"枫叶","_id":"cltl4oxeu005lxh7k5wm7csac"},{"name":"中国","_id":"cltl4oxeu005mxh7kb9gqgy6k"},{"name":"挪威","_id":"cltl4oxeu005nxh7kcsfm7yf2"},{"name":"markdown","_id":"cltl4oxeu005oxh7kac9v31fc"},{"name":"宿舍","_id":"cltl4oxeu005pxh7kdlg035y5"},{"name":"孩子们","_id":"cltl4oxeu005wxh7k4h0w8u5e"},{"name":"卧龙","_id":"cltl4oxeu005zxh7kh6dv8423"},{"name":"凤雏","_id":"cltl4oxeu0062xh7kfro8hoiw"},{"name":"骆雁","_id":"cltl4oxeu0065xh7k29jsd8v4"},{"name":"黄帝","_id":"cltl4oxev0069xh7kgufo3xyx"},{"name":"小绿","_id":"cltl4oxev006cxh7kes1wh3vs"},{"name":"猫咪","_id":"cltl4oxev006fxh7k6rng5u0v"},{"name":"土鸡","_id":"cltl4oxev006hxh7k4cn8hcq9"},{"name":"熊","_id":"cltl4oxev006ixh7kgdcd29dc"},{"name":"🐻","_id":"cltl4oxev006jxh7k1ejk02sv"},{"name":"🐱","_id":"cltl4oxev006kxh7khf8cducv"},{"name":"🐇","_id":"cltl4oxev006lxh7k09zw95ci"},{"name":"🐰","_id":"cltl4oxev006mxh7kcncec60u"},{"name":"🐊","_id":"cltl4oxev006nxh7kbyzd7r0f"},{"name":"emoren","_id":"cltl4oxev006oxh7kaehpf2kl"},{"name":"acl","_id":"cltl4oxev006qxh7k6wcvhswi"},{"name":"lillesand","_id":"cltl4oxev006rxh7k63p98qte"},{"name":"加拿大","_id":"cltl4oxev006sxh7k32yp31ad"},{"name":"html","_id":"cltl4oxew0075xh7kd5geekx0"},{"name":"liquid","_id":"cltl4oxew0078xh7kglay1tj4"},{"name":"arxiv","_id":"cltl4oxew007exh7kewrv7u8o"},{"name":"knowledge","_id":"cltl4oxex007kxh7kgaqn0nnw"},{"name":"emnlp","_id":"cltl4oxex007rxh7k0wxb4wnj"},{"name":"model editing","_id":"cltl4oxex007xxh7k9exihhsl"},{"name":"in-context-learning","_id":"cltl4oxex0080xh7k8285833d"},{"name":"ai","_id":"cltl4oxex0083xh7kcy0v7td3"},{"name":"serac","_id":"cltl4oxex0085xh7k6n8q8ah4"},{"name":"rome","_id":"cltl4oxex0086xh7ke1ztd0nq"},{"name":"mend","_id":"cltl4oxex0088xh7k3ivpfxk4"},{"name":"language modeling","_id":"cltl4oxex0089xh7kc78g7555"},{"name":"language models","_id":"cltl4oxey008cxh7k6gnu5v7b"},{"name":"rnn","_id":"cltl4oxey008oxh7khcnpb0fy"},{"name":"deep learning","_id":"cltl4oxey008rxh7ke95rfcgv"},{"name":"machine learning","_id":"cltl4oxey008uxh7k6lw9e97m"}]}}