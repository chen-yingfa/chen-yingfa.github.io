{"meta":{"version":1,"warehouse":"4.0.2"},"models":{"Asset":[{"_id":"themes/fengye/source/favicon-32x32.png","path":"favicon-32x32.png","modified":0,"renderable":1},{"_id":"themes/fengye/source/favicon.ico","path":"favicon.ico","modified":0,"renderable":1},{"_id":"themes/fengye/source/css/highlight.styl","path":"css/highlight.styl","modified":0,"renderable":1},{"_id":"themes/fengye/source/css/search.styl","path":"css/search.styl","modified":0,"renderable":1},{"_id":"themes/fengye/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/fengye/source/js/main.js","path":"js/main.js","modified":0,"renderable":1},{"_id":"themes/fengye/source/js/search.js","path":"js/search.js","modified":0,"renderable":1},{"_id":"themes/fengye/source/lib/clipboard.min.js","path":"lib/clipboard.min.js","modified":0,"renderable":1},{"_id":"themes/fengye/source/lib/iconify-icon.min.js","path":"lib/iconify-icon.min.js","modified":0,"renderable":1},{"_id":"themes/fengye/source/lib/jquery.min.js","path":"lib/jquery.min.js","modified":0,"renderable":1},{"_id":"themes/fengye/source/images/Fengye.png","path":"images/Fengye.png","modified":0,"renderable":1},{"_id":"themes/fengye/source/images/logo.svg","path":"images/logo.svg","modified":0,"renderable":1},{"_id":"themes/fengye/source/lib/fancybox/fancybox.min.css","path":"lib/fancybox/fancybox.min.css","modified":0,"renderable":1},{"_id":"themes/fengye/source/lib/fancybox/fancybox.umd.min.js","path":"lib/fancybox/fancybox.umd.min.js","modified":0,"renderable":1},{"_id":"themes/fengye/source/lib/tocbot/tocbot.min.css","path":"lib/tocbot/tocbot.min.css","modified":0,"renderable":1},{"_id":"themes/fengye/source/lib/tocbot/tocbot.min.js","path":"lib/tocbot/tocbot.min.js","modified":0,"renderable":1},{"_id":"source/images/favicon-32x32.png","path":"images/favicon-32x32.png","modified":0,"renderable":0},{"_id":"source/images/favicon.ico","path":"images/favicon.ico","modified":0,"renderable":0},{"_id":"source/images/portrait.png","path":"images/portrait.png","modified":0,"renderable":0},{"_id":"source/images/portrait.jpg","path":"images/portrait.jpg","modified":0,"renderable":0},{"_id":"source/pdf/cv.pdf","path":"pdf/cv.pdf","modified":0,"renderable":0}],"Cache":[{"_id":"source/.DS_Store","hash":"3f0c25e5b4975cb37e74b893f0af73abb89883db","modified":1706814084843},{"_id":"source/about.md","hash":"d26e3110cf867bfb287e64f88b3073513a8ee8f6","modified":1696064779412},{"_id":"source/publications.md","hash":"487fd6a28ef51b6e7797d508fbfeae6d915fdee6","modified":1704945062707},{"_id":"source/indexx.md","hash":"e7a89c4563ef74f1b1c9439db3e82c4abe6468ff","modified":1695454296827},{"_id":"source/projects.md","hash":"d529b2396a9daa08a77068d2bbb8b2edc9089196","modified":1695454114813},{"_id":"source/_posts/2023ä¸­ç§‹.md","hash":"76c05e3a7a67a3d2927d3a524366fc7104aebd66","modified":1704951873394},{"_id":"source/_posts/actadd.md","hash":"788d9eb175022a2d888a911e5afd36d44ecc31d1","modified":1704951722734},{"_id":"source/_posts/.DS_Store","hash":"1ff83699470dc1649bdaf0815bb4f4256acf5fdd","modified":1709464743338},{"_id":"source/_posts/infinitebench.md","hash":"2f887f8a28cd3dda186387f287278b71677358ba","modified":1708916824673},{"_id":"source/_posts/some_binary_search.md","hash":"bd37ac2fa1894f5e306b0b32f1319d01cc5efaad","modified":1704949168107},{"_id":"source/_posts/CFDBench.md","hash":"635cbc1a9ba5c73579d153764e1d6964d4cf6e1d","modified":1704944740727},{"_id":"source/_posts/ä¸´è¿‘2023æš‘å‡.md","hash":"7be9d2213d8cea4982502db7f8d883866019e675","modified":1705901904811},{"_id":"source/_posts/ç¬¬ä¸€ä¸ªå¸–å­ï¼Œçå†™ç‚¹ä¸œè¥¿.md","hash":"da89f515329cdebf2bbcf5af703130a595b547a7","modified":1708916181456},{"_id":"source/_posts/æ›´æ–°ä¸ªäººä¸»é¡µ.md","hash":"1324f3678164635831cc6b148ce9232c05d26ef4","modified":1706009572626},{"_id":"source/friends/index.md","hash":"640ab2bae07bedc4c163f679a746f7ab7fb5d1fa","modified":1694796577338},{"_id":"source/_posts/llm-safety-and-ethics.md","hash":"7b8071ecca76b5473a3144cba5a513dc584eae6e","modified":1704871496515},{"_id":"source/_drafts/EREN.md","hash":"a90bfd52bb6bc40244623575fd8de871162b217d","modified":1706008157490},{"_id":"source/_posts/ç¬¬ä¸€ç¯‡.md","hash":"87d255ec553fa1f4a38ec9f9e3174efaf93550f4","modified":1704871496516},{"_id":"source/_posts/interpreting-a-maze-solving-network.md","hash":"af6564de1ec2bf91229a46e9c1ccf833c0a87e28","modified":1704951709088},{"_id":"source/_drafts/è¯­è¨€æ¨¡å‹çš„è¿›åŒ–.md","hash":"074cc63ea66033f9d28fb9a509cb886165679b3c","modified":1708915998088},{"_id":"source/home/index.md","hash":"76e8103ebc508f55471a9676e8b22fb52f12d03c","modified":1694851730129},{"_id":"source/categories/index.md","hash":"4c4f89189728ad20934104d3d598fc3414bf10cc","modified":1710046158292},{"_id":"source/images/favicon.ico","hash":"8ede8a7e8edefbd78bb978d781a502f696c32be9","modified":1694861604941},{"_id":"source/images/favicon-32x32.png","hash":"ebaaa72b28fc97b0e4c419837ac1f2c54110d070","modified":1695511244000},{"_id":"source/index/index.md","hash":"263e77b32aef07196b099dccb348d39e217fe3ad","modified":1706008323884},{"_id":"source/tags/index.md","hash":"3ecc8ff83978866b006b59fb6b8d1977e45ae09e","modified":1710046123903},{"_id":"source/_drafts/è¯­è¨€æ¨¡å‹çš„è¿›åŒ–/rnn.png","hash":"aac5c34e5dd58a5aef3986971fbdb86995e55fc8","modified":1706807686007},{"_id":"source/_posts/actadd/alg.png","hash":"dc702db52c81311391f17e05666a5a5c9e082ed2","modified":1696759929479},{"_id":"source/pdf/cv.pdf","hash":"84479a0480b08fc28ed34e856c8a8dcad815dbc4","modified":1693635547998},{"_id":"source/_drafts/è¯­è¨€æ¨¡å‹çš„è¿›åŒ–/self-attention.png","hash":"077705a17f32431f3ebe5191671ca05b7bc656e3","modified":1706809422310},{"_id":"themes/fengye/package.json","hash":"2fd1608c7164b35dfbc325e9d3a1293f92ea672d","modified":1694946750845},{"_id":"themes/fengye/.gitignore","hash":"2a191a03434ba584ea5e70d36ab4eb2a1b6324da","modified":1694839598097},{"_id":"themes/fengye/.editorconfig","hash":"b16f01a7b04ad512e15ebb32c5786d432a536779","modified":1694798100057},{"_id":"themes/fengye/LICENSE","hash":"632b916dd7e4f5c11790ab808388cda6610210ed","modified":1694798100058},{"_id":"themes/fengye/languages/default.yml","hash":"da38f00bb45a318f118db0d74df24a137351777e","modified":1694798100058},{"_id":"themes/fengye/.DS_Store","hash":"ee7a7465a1b0a4a13207339a23f29012f8b7b5b4","modified":1705901982879},{"_id":"themes/fengye/_config.yml","hash":"fff2b123cd2d603014193592dcc1598e8f83c7bb","modified":1710051530939},{"_id":"themes/fengye/languages/en.yml","hash":"72066419d6682a017c97910921ade125f21b04cd","modified":1694798100058},{"_id":"themes/fengye/languages/ja.yml","hash":"3e2fedca096678c0c234ebffa4637828979296fa","modified":1694798100059},{"_id":"themes/fengye/scripts/echarts.js","hash":"2c5a1439a12b3c3ab3fd51b3572748a0ccd8667a","modified":1694798100064},{"_id":"themes/fengye/scripts/mermaid.js","hash":"778f3daf90edab9bd5c007cfb131e69e3e65709f","modified":1694798100064},{"_id":"themes/fengye/scripts/wordcount.js","hash":"d1c45c18bc63c144f02f3400c21b179534ae99ce","modified":1694798100064},{"_id":"themes/fengye/package-lock.json","hash":"f963312fc6344c02b731e735fd14818a813ce98b","modified":1694946891794},{"_id":"themes/fengye/languages/ko.yml","hash":"11330316e3c1262474a2b496e40dbc29f93fe01b","modified":1694798100059},{"_id":"themes/fengye/README.md","hash":"f2aec2b29e9844daa7d7f9c6a282ed605734e209","modified":1709517996130},{"_id":"themes/fengye/languages/no.yml","hash":"182bd9ea76313ec9dc769b5dd2845c0d1c56e3a0","modified":1694947626048},{"_id":"themes/fengye/source/favicon-32x32.png","hash":"ebaaa72b28fc97b0e4c419837ac1f2c54110d070","modified":1695511244000},{"_id":"themes/fengye/languages/de.yml","hash":"d45cea36c5c83d7d09afcd1c26fff4a4c513c25b","modified":1694798100058},{"_id":"themes/fengye/source/favicon.ico","hash":"8ede8a7e8edefbd78bb978d781a502f696c32be9","modified":1694798100065},{"_id":"themes/fengye/layout/archive.ejs","hash":"bed6fea1a69ef1eb13b8d4e6616adab7282a7ebf","modified":1704874448957},{"_id":"themes/fengye/source/.DS_Store","hash":"43be40210f45a8cca5810daf05ae8d117010cfc1","modified":1705901982878},{"_id":"themes/fengye/layout/categories.ejs","hash":"50c3fc1c0fe4a619f4504722a5ae693c482ded7d","modified":1695974049298},{"_id":"themes/fengye/layout/layout.ejs","hash":"6bcddd59ee27ad6b8f94d54a08e80903627c885e","modified":1696507318605},{"_id":"themes/fengye/languages/zh-CN.yml","hash":"e41d1e0e3a9e15c30b7142491bed39dc50371e96","modified":1694947135799},{"_id":"themes/fengye/layout/tags.ejs","hash":"f4cfea6489b5c25253cb80df3de8603591399f02","modified":1704951509685},{"_id":"themes/fengye/source/css/highlight.styl","hash":"92aa8c7a03febcebe91e58cabdda94086407a4a3","modified":1695633123647},{"_id":"themes/fengye/layout/index.ejs","hash":"0c3a330a3b3bb90f7f0ea161c78d097d5f199a69","modified":1709518106495},{"_id":"themes/fengye/layout/post.ejs","hash":"cde0234b96a98b0af0d067f518dbce18639ef84b","modified":1704950797619},{"_id":"themes/fengye/layout/.DS_Store","hash":"503f8df87987f2567c18b293ef8add5a56e23abb","modified":1705901977825},{"_id":"themes/fengye/source/js/main.js","hash":"954fb2295daaa1d918d4afe13163f4282567b624","modified":1695998347849},{"_id":"themes/fengye/source/css/search.styl","hash":"c2707e9c4ab6ea6a2f0c5445639ed6debabd4fe3","modified":1698947373651},{"_id":"themes/fengye/source/lib/clipboard.min.js","hash":"91d8fe48e42d7d985918d4f244f6e7e3cc5adc2d","modified":1694798100082},{"_id":"themes/fengye/source/lib/iconify-icon.min.js","hash":"014e6de8308a10051823d39ba49834ff18e4dba7","modified":1694798100083},{"_id":"themes/fengye/source/css/main.styl","hash":"a111232eb79526f8a3321a681920d596ae59b129","modified":1710046986661},{"_id":"themes/fengye/layout/_partial/after-footer.ejs","hash":"e0441196b4db18962ec563128de25fcf6b2547ac","modified":1694947748330},{"_id":"themes/fengye/layout/_partial/head.ejs","hash":"d798e3af9099197a2e8080034defa2e152a406cb","modified":1704860574098},{"_id":"themes/fengye/source/js/search.js","hash":"9b14f8e4a76d8da293d538d73d340dfd5b5e5402","modified":1710051538026},{"_id":"themes/fengye/source/images/Fengye.png","hash":"bd5ebaf9ffd792aa83701e7701ad5130e1b4df4c","modified":1709517996137},{"_id":"themes/fengye/layout/_partial/header.ejs","hash":"65fb66831696f323f7cf6a8c28078c6094c80f58","modified":1710051584458},{"_id":"themes/fengye/source/images/logo.svg","hash":"1b7a73d948e593dcec7549d63e5ac60ad6db6a8f","modified":1694798100073},{"_id":"themes/fengye/layout/_partial/post-list-item.ejs","hash":"d5bb4e6c0c736e09612c8b581320952dd780e8a5","modified":1709517996133},{"_id":"themes/fengye/layout/_partial/paginator.ejs","hash":"76f929c87797cb6b81c1003846001915c98ed336","modified":1709517996132},{"_id":"themes/fengye/layout/_partial/tag-list.ejs","hash":"3e5ad39f0866e7eab2768a93a9778ba418c9d741","modified":1709517996135},{"_id":"themes/fengye/layout/_partial/toc.ejs","hash":"b2d43f20570cac0c58e3b0578791ed79ab3321a1","modified":1704944938535},{"_id":"themes/fengye/layout/_plugins/baidu-analytics.ejs","hash":"5d651a50ab521334b3cc0bff34ce3de3e76ac750","modified":1694798100061},{"_id":"themes/fengye/layout/_partial/post-list.ejs","hash":"1303877ae7ad5ba759694385e079d427b3b3c594","modified":1709517996134},{"_id":"themes/fengye/layout/_partial/footer.ejs","hash":"f07cae693f3b78401168edd73db30e96ca9eb35a","modified":1704949293627},{"_id":"themes/fengye/layout/_partial/social-list.ejs","hash":"71bb8e8ad61c4ad836ab335aa24e67731a6b162e","modified":1696074432629},{"_id":"themes/fengye/layout/_plugins/disqusjs.ejs","hash":"5f9b22b65eb828e3af178379c3d5be56d93a7aaa","modified":1694798100061},{"_id":"themes/fengye/layout/_plugins/giscus.ejs","hash":"d6f97ceb3ccfcedd61ac7890c5ca46f252f73802","modified":1694798100061},{"_id":"themes/fengye/layout/_plugins/google-analytics.ejs","hash":"a8a6e445d6016bfd8ae657d33c213ec828612184","modified":1694798100061},{"_id":"themes/fengye/layout/_plugins/mathjax.ejs","hash":"c358356a7595b5e1fe1d65aee10616eec8eb00e7","modified":1694798100062},{"_id":"themes/fengye/layout/_plugins/mermaid.ejs","hash":"804c36256f9232a1fe6f308e06ae89642fe0eb1b","modified":1694798100062},{"_id":"themes/fengye/layout/_plugins/fancybox.ejs","hash":"d2d0858a34f550c80d00fb705f94e08b33150eab","modified":1704865863839},{"_id":"themes/fengye/layout/_plugins/busuanzi.ejs","hash":"7bd277fb195c6a499e36f4870a80fd7073c61052","modified":1695467772525},{"_id":"themes/fengye/source/lib/fancybox/fancybox.min.css","hash":"faa1beb3cde9b3abf714bf1b8410eb71199e798d","modified":1694798100082},{"_id":"themes/fengye/layout/_plugins/tocbot.ejs","hash":"e17e921d40c71405a37ee3c782b7584f8b339653","modified":1694973231694},{"_id":"themes/fengye/layout/_plugins/tailwindcss.ejs","hash":"3bc366434255a72d0447c88af03abc1c34c128a7","modified":1695461959702},{"_id":"themes/fengye/layout/_partial/search.ejs","hash":"cdc5676141e436a61c279213d412f1f37cb48e8a","modified":1710051320561},{"_id":"themes/fengye/layout/_plugins/theme.ejs","hash":"af6f16cf670f50731bc89a31f9f208a65776ccf4","modified":1695019010493},{"_id":"themes/fengye/source/lib/tocbot/tocbot.min.js","hash":"30be41afb1f21ae821a143d74beab47094bc87a3","modified":1694798100085},{"_id":"themes/fengye/source/lib/tocbot/tocbot.min.css","hash":"fb0f0f1d2ceff6621f4e20191a103330c03ea2f6","modified":1695547118134},{"_id":"source/_posts/ç¬¬ä¸€ä¸ªå¸–å­ï¼Œçå†™ç‚¹ä¸œè¥¿/lillesand0.jpg","hash":"ea04c507fe1c7859b5e2c13265649e7ee5e10ca9","modified":1704866075031},{"_id":"source/_posts/ç¬¬ä¸€ä¸ªå¸–å­ï¼Œçå†™ç‚¹ä¸œè¥¿/lillesand1.jpg","hash":"208e8d0ed9178a7935a930082b48e0a7371e5f62","modified":1704866075036},{"_id":"source/_posts/actadd/method.png","hash":"a8b682bfe5ab4dbe09b14b083055f6690dd032c1","modified":1696674147963},{"_id":"source/_posts/infinitebench/data-stat-pie.png","hash":"3384e825a982ec9038295ef68d0856b1b04a1d07","modified":1704855164330},{"_id":"source/_posts/actadd/result.png","hash":"9e94dccb92d8f3d25345b515322d23cf9ba24def","modified":1696814224211},{"_id":"themes/fengye/source/lib/fancybox/fancybox.umd.min.js","hash":"91f17b14286c64506ed5e214d7e31209d34940c0","modified":1694798100083},{"_id":"themes/fengye/source/lib/jquery.min.js","hash":"f694238d616f579a0690001f37984af430c19963","modified":1694798100084},{"_id":"source/_posts/infinitebench/results.png","hash":"5b36c793d61b0c070f426e02db6223526d51e102","modified":1704856068843},{"_id":"source/_posts/æ›´æ–°ä¸ªäººä¸»é¡µ/ä¸­å›½æŠ¤ç…§.jpg","hash":"ff10a197a19b140980e804180b0503216ef1d863","modified":1694882171768},{"_id":"source/images/portrait.jpg","hash":"c910db58974a63b1c7d8d256b99b0c6ee4901a50","modified":1695527695106},{"_id":"source/_posts/æ›´æ–°ä¸ªäººä¸»é¡µ/ç”³è¯·ç­¾è¯.jpg","hash":"7df3acbc98aa10dca03efdb285fb40f2d4525f52","modified":1694882064635},{"_id":"source/_posts/æ›´æ–°ä¸ªäººä¸»é¡µ/åƒæ¾³é—¨èœ.jpg","hash":"92855a153bb0c0644b327b55f433ebe661a78fa2","modified":1694882171854},{"_id":"source/images/portrait.png","hash":"049faf25dabe7878afe4181dc7355845504ce90c","modified":1695528095993},{"_id":"source/_posts/2023ä¸­ç§‹/æ­¦æ±‰æ¬¢ä¹è°·.png","hash":"977e23f8ad8150689b0b34f3df1a9bbed2909460","modified":1704865384673},{"_id":"source/_posts/2023ä¸­ç§‹/æ­¦å•†æ¢¦æ—¶ä»£.png","hash":"9dec428cc6c918f9eaf6d47dd1a36f797678f72b","modified":1704865384690},{"_id":"source/_posts/2023ä¸­ç§‹/æ–°å¤©åœ°-éœ¸ç‹èŒ¶å§¬.png","hash":"cd9609e61378cea89811ef3d1cf348ec59822036","modified":1704865384714},{"_id":"source/_posts/2023ä¸­ç§‹/è§£æ”¾å…¬å›­ä¸­é—´.png","hash":"359aaf1437e94f494e6dd419ff6b7001a6e0b362","modified":1704865384660},{"_id":"public/search.xml","hash":"e4fc0f4b011b1b7393704d9dc4b64aa84d178bc3","modified":1710051804435},{"_id":"public/about.html","hash":"4f7e7faf107fd8bbfeae0c71a0572ea164aaadf2","modified":1710051856894},{"_id":"public/friends/index.html","hash":"9d8d752e00a84fee6e0974eab9ce149e414a0d68","modified":1710051856894},{"_id":"public/publications.html","hash":"92f402b3724a13271603c7cc0b00cec70a6cb5c3","modified":1710051856894},{"_id":"public/categories/index.html","hash":"a4f720f1fd8b504911de5efc466d7246b8a20355","modified":1710051856894},{"_id":"public/home/index.html","hash":"c63a8f71c4dc64b399af7083675cdd306e4e7f16","modified":1710051856894},{"_id":"public/index/index.html","hash":"9ee9caf3e61998eda92f8fd8805806c750cae13c","modified":1710051856894},{"_id":"public/tags/index.html","hash":"d52260dd9b2d630e88977d8d9982b186074353f6","modified":1710051856894},{"_id":"public/projects.html","hash":"ac5ed097c420487468276d27c11625c9f8d6fc8a","modified":1710051856894},{"_id":"public/2024/01/10/infinitebench/index.html","hash":"f6660316052b5a6693e6211dae548f16eb2f48ab","modified":1710051856894},{"_id":"public/2023/10/07/interpreting-a-maze-solving-network/index.html","hash":"f1a724cb535e0987c5cd811a64c494458dc08f01","modified":1710051856894},{"_id":"public/indexx.html","hash":"2af3d7b2b051b65d09148cd7d930ae0a99c178e4","modified":1710051856894},{"_id":"public/2023/10/07/actadd/index.html","hash":"3318217ed50fefb837eb298ef382d26aafb889e8","modified":1710051856894},{"_id":"public/2023/10/05/2023ä¸­ç§‹/index.html","hash":"8d77714c554757185d7c6cc820c57aa02a6059ae","modified":1710051856894},{"_id":"public/2023/09/19/llm-safety-and-ethics/index.html","hash":"d9a5133875fb383a7d4ab3dc4b1a820f419deb7b","modified":1710051856894},{"_id":"public/2023/09/16/æ›´æ–°ä¸ªäººä¸»é¡µ/index.html","hash":"961fa5601a59e744f8ac4003b51a74f28c5614c2","modified":1710051856894},{"_id":"public/2023/09/16/CFDBench/index.html","hash":"c7a663f183b0abb1a2b6197c5437387ca051a810","modified":1710051856894},{"_id":"public/2023/09/14/some_binary_search/index.html","hash":"904509f9513a9ee8d2e18638a73355d53e0f8a21","modified":1710051856894},{"_id":"public/2023/05/18/ä¸´è¿‘2023æš‘å‡/index.html","hash":"81fc53bce31072fffd14c5b3375bd02c57462616","modified":1710051856894},{"_id":"public/2023/05/17/ç¬¬ä¸€ä¸ªå¸–å­ï¼Œçå†™ç‚¹ä¸œè¥¿/index.html","hash":"85c47bafdb0114f27a8c0b36cc1c5f402e5bf43b","modified":1710051856894},{"_id":"public/2022/10/27/ç¬¬ä¸€ç¯‡/index.html","hash":"16b2c1e0300f87b8c4366aa11ade3adf45e645aa","modified":1710051856894},{"_id":"public/page/2/index.html","hash":"15287dccb41674bdb19eab5561e37e0d77e0fe35","modified":1710051856894},{"_id":"public/index.html","hash":"15287dccb41674bdb19eab5561e37e0d77e0fe35","modified":1710051856894},{"_id":"public/archives/index.html","hash":"c2a012d2d68375cbe5318b78008ae62f374221f0","modified":1710051856894},{"_id":"public/archives/2022/index.html","hash":"596ff995968927346bada90207050a88f2b02e5f","modified":1710051856894},{"_id":"public/archives/page/2/index.html","hash":"ee8219d248de8d817366f742b4b6d481fda95811","modified":1710051856894},{"_id":"public/archives/2022/10/index.html","hash":"74550b9c1603dea4004fd5050ee4c6af63404451","modified":1710051856894},{"_id":"public/archives/2023/index.html","hash":"9ae1dacbcd27b0b95ba4629086d282da3065bdb7","modified":1710051856894},{"_id":"public/archives/2023/05/index.html","hash":"b0b490e1fefdca4cf8b02ce634068a00914899f3","modified":1710051856894},{"_id":"public/archives/2023/09/index.html","hash":"5eafc4f5665ebf80837777725d308b46de9bdd5d","modified":1710051856894},{"_id":"public/archives/2023/10/index.html","hash":"b38108df1d7a7335110a004eb970e13079a43b3a","modified":1710051856894},{"_id":"public/archives/2024/01/index.html","hash":"75a9991ae85dd7bf95296c4ff5ae53e971c10b9c","modified":1710051856894},{"_id":"public/archives/2024/index.html","hash":"3426471d17335599696175a7120eeaa67ce0fec6","modified":1710051856894},{"_id":"public/tags/life/index.html","hash":"be78e0cee91fdecea094e5bedd4581e830d68d8c","modified":1710051856894},{"_id":"public/tags/ä¸­ç§‹/index.html","hash":"fcabb1a9d8f03f13d6863b85681d1506623cfd3f","modified":1710051856894},{"_id":"public/tags/ä¸­æ–‡/index.html","hash":"934cb35fcd4e662d258f51b1ce7cf34b8e4fbdf7","modified":1710051856894},{"_id":"public/tags/00/index.html","hash":"18c45825a985dbae33baf4248f8092f05f98923a","modified":1710051856894},{"_id":"public/tags/wedding/index.html","hash":"0d9085fefece08857987477ef3f0480a41d59ee1","modified":1710051856894},{"_id":"public/tags/ä¸­ç§‹-middle-autumn/index.html","hash":"3ec8d00f2c80893b24ffe9977f8fffef55b05aab","modified":1710051856894},{"_id":"public/tags/å›½åº†-national-day/index.html","hash":"35d17e097cba6486970ed6f09709e2eb0427c656","modified":1710051856894},{"_id":"public/tags/åº”åŸ/index.html","hash":"528b6c51de3080d79db21cefb0a6249d99da21c2","modified":1710051856894},{"_id":"public/tags/æ­¦æ±‰-wuhan/index.html","hash":"da00365d197dffd34df549612ce9fd144d88cd73","modified":1710051856894},{"_id":"public/tags/paper/index.html","hash":"a5a7a39215e0ea466f330142a1b3bb54738bba57","modified":1710051856894},{"_id":"public/tags/research/index.html","hash":"2d0dcaff812d30c8f4413625f62c17a712a83c82","modified":1710051856894},{"_id":"public/tags/cfd/index.html","hash":"eaaafe8d30c0cd4ba8b25f64a8efdc3a145c9c6b","modified":1710051856894},{"_id":"public/tags/dataset/index.html","hash":"978f0fa353f6767a08c87ae252390222376c343f","modified":1710051856894},{"_id":"public/tags/english/index.html","hash":"74352b307f1f1f84a1ac03f0fce64b13af26fb6d","modified":1710051856894},{"_id":"public/tags/pinn/index.html","hash":"a19b9a7b950df43ff7f749002ab832faca1f48fa","modified":1710051856894},{"_id":"public/tags/fno/index.html","hash":"b9080fffb029e096a604c540a2ce1f419d16ebaa","modified":1710051856894},{"_id":"public/tags/physics/index.html","hash":"de37c0e7e6775a026691d30144f60683dd3deb4f","modified":1710051856894},{"_id":"public/tags/machine-learning/index.html","hash":"b17886600722ab132d69cb61439e91c2f3bd7aa4","modified":1710051856894},{"_id":"public/tags/deep-learning/index.html","hash":"0d920eb547256bcdc155d8a33ce27e79ead034a4","modified":1710051856894},{"_id":"public/tags/deeponet/index.html","hash":"cf21511ace0aa467bfb078d2b9c9ad64ede037c2","modified":1710051856894},{"_id":"public/tags/ai4science/index.html","hash":"7cdf6ca72ac7f98f97ebbd6029528076090624dd","modified":1710051856894},{"_id":"public/tags/ai-alignment/index.html","hash":"b6ab247709ae4e8f09d020cca74716e6ddbb88a4","modified":1710051856894},{"_id":"public/tags/llm/index.html","hash":"33a27c5c9d09c32131f499137d400d71a69cef24","modified":1710051856894},{"_id":"public/tags/gpt/index.html","hash":"d032f1a248d74e613273715e2ba250ca8aef925e","modified":1710051856894},{"_id":"public/tags/activation-modification/index.html","hash":"67eab9bdc2abd68ccf4de3c5451fca7e8639dd3e","modified":1710051856894},{"_id":"public/tags/adaptation/index.html","hash":"fa66f7a0bc5a5bc214c10a9d3324247227f153ea","modified":1710051856894},{"_id":"public/tags/model-editing/index.html","hash":"5bfd83430afda698e923b96b3ba6d4cd5fadabd2","modified":1710051856894},{"_id":"public/tags/representation-engineering/index.html","hash":"fd18b488151d157303319b50dd728d76cb778244","modified":1710051856894},{"_id":"public/tags/fine-tuning/index.html","hash":"07fcd0842f37c13ee7e5a21c3c54b5b0cc73be18","modified":1710051856894},{"_id":"public/tags/parameter-efficient-tuning/index.html","hash":"2c34a155cb40bae3b7221d72fb5db87d414fa5ed","modified":1710051856894},{"_id":"public/tags/nlp/index.html","hash":"4a6435478b8058bc64bb19ed46ee7141b0baeceb","modified":1710051856894},{"_id":"public/tags/long-context/index.html","hash":"aa369fba6788a6d1804c93455339555111e7d278","modified":1710051856894},{"_id":"public/tags/benchmark/index.html","hash":"107e732264576d6efbacad9ad4d5cfbf768a57fe","modified":1710051856894},{"_id":"public/tags/recurrence/index.html","hash":"247f2f7e40a42d0cb60c2d9645a79736b7c72d3c","modified":1710051856894},{"_id":"public/tags/linear-attention/index.html","hash":"e0f9dc60c00c21bd45abe6d6188bb059c9d90125","modified":1710051856894},{"_id":"public/tags/transformer/index.html","hash":"cfd0f3e78f0c7d8e84e195fcca9cdc9c9333cffe","modified":1710051856894},{"_id":"public/tags/activation-engineering/index.html","hash":"5d1cc0335b58e149de5fdb05b9ffd53a62c5bca8","modified":1710051856894},{"_id":"public/tags/interpretability/index.html","hash":"eab1689816f59ca28556f6369a423cb5d466fc49","modified":1710051856894},{"_id":"public/tags/rl/index.html","hash":"a56930cd3fd9577b5212a1a4f01388269234c92b","modified":1710051856894},{"_id":"public/tags/alignment/index.html","hash":"1849d54af19722f0d8f252c7b9490346ce4b0d13","modified":1710051856894},{"_id":"public/tags/maze/index.html","hash":"a80b70e5896e5b5ef970cc81871cb6189bb2669a","modified":1710051856894},{"_id":"public/tags/ethics/index.html","hash":"c6595fafdb7a7a5f097e69f46eb999e52c92ac4f","modified":1710051856894},{"_id":"public/tags/safety/index.html","hash":"18b1069943b3ec7e53a7aa71d8380cabb60b935b","modified":1710051856894},{"_id":"public/tags/é¢å£æ™ºèƒ½/index.html","hash":"8b4b55ca95193637bdd2b822d8f4083b4c120e42","modified":1710051856894},{"_id":"public/tags/tutorial/index.html","hash":"ee8f58254709b38a99c528d72f13ccf293aae29c","modified":1710051856894},{"_id":"public/tags/eren/index.html","hash":"f569ee0e2dcc9c7d6e9199467c7449dc4fae6c3e","modified":1710051856894},{"_id":"public/tags/chatgpt/index.html","hash":"571673ec151dab7dfa7776854cab70ef89ad221a","modified":1710051856894},{"_id":"public/tags/claude/index.html","hash":"45a61d6096014666a2971db3e8d9f5efd6063b2a","modified":1710051856894},{"_id":"public/tags/ä¸‰ä½“/index.html","hash":"6aa3c63f76117b9a065d3daa5c043479a05dc856","modified":1710051856894},{"_id":"public/tags/modelbest/index.html","hash":"a0bd1ba12b85fd867bc5ca3b146a78c24fd02b5b","modified":1710051856894},{"_id":"public/tags/binary-search/index.html","hash":"31a713640ff4400624118736979be5b43eba190c","modified":1710051856894},{"_id":"public/tags/rust/index.html","hash":"072e60ed71e94b1c040b96d54a5b3ed01c0971a7","modified":1710051856894},{"_id":"public/tags/algorithm/index.html","hash":"daae322134de513af94b4a706ef75460673ca53b","modified":1710051856894},{"_id":"public/tags/python/index.html","hash":"41409248ae5fe7f99c7328d4f95553d469e64e4c","modified":1710051856894},{"_id":"public/tags/c/index.html","hash":"12f259cdae59a3d1f8255b89f69453ba468bd3b2","modified":1710051856894},{"_id":"public/tags/test/index.html","hash":"4698113d962f4feee6d2a34bf30cec5c4987dffe","modified":1710051856894},{"_id":"public/tags/code/index.html","hash":"30581f6861d6f08a92a6bdd66956873c0fe1d4ca","modified":1710051856894},{"_id":"public/tags/school/index.html","hash":"8c3467255dc8e9c4a8de7d22d5ebc9e20ff16271","modified":1710051856894},{"_id":"public/tags/graduation/index.html","hash":"61c5a262ff1b7fc53f300fc00d4fd644484fd23e","modified":1710051856894},{"_id":"public/tags/blog/index.html","hash":"4816aee3f6b42144b6438f8986d344b9b27619d8","modified":1710051856894},{"_id":"public/tags/ç­¾è¯/index.html","hash":"6e10a7bc2a26227b2ea91d7bd2bc57c9b5ca04da","modified":1710051856894},{"_id":"public/tags/hugo/index.html","hash":"bb23c038867493a0ae751b7a6d96772dfcb72e37","modified":1710051856894},{"_id":"public/tags/jekyll/index.html","hash":"62b5df3857fb516b26f9529c2eff5b2a64c2ec0a","modified":1710051856894},{"_id":"public/tags/hexo/index.html","hash":"2f01eba6ae78d987b35672caca1977f5a81bb6fd","modified":1710051856894},{"_id":"public/tags/static-site-generator/index.html","hash":"8665150fc63932ea9bd7b1eb91c352908e2c9c13","modified":1710051856894},{"_id":"public/tags/ç¾½æ¯›çƒ/index.html","hash":"c9958cbf86f7fb15299dc1acc3b1402bba289095","modified":1710051856894},{"_id":"public/tags/work/index.html","hash":"e141a71429cc6a97597914f5853c6fc42a91edc5","modified":1710051856894},{"_id":"public/tags/æ«å¶/index.html","hash":"1b8fc50bd24e908f504c5284406aebba4967f963","modified":1710051856894},{"_id":"public/tags/ä¸­å›½/index.html","hash":"73c4f266773ce4a200504e91a0ab5f1f915dc19f","modified":1710051856894},{"_id":"public/tags/æŒªå¨/index.html","hash":"bec17c6e394aac8d64bbe132b58a8b59ce43ddc5","modified":1710051856894},{"_id":"public/tags/markdown/index.html","hash":"eea5c0853d086d4d9dea166317188d7aaa65ceb3","modified":1710051856894},{"_id":"public/tags/å®¿èˆ/index.html","hash":"fc12ff3dd348b8edfe9dbb9229096abbffd33efd","modified":1710051856894},{"_id":"public/tags/å­©å­ä»¬/index.html","hash":"725ecf2a9f0e115f2403b2e5c977bc227acba80f","modified":1710051856894},{"_id":"public/tags/å§é¾™/index.html","hash":"8f5c0e21565fa9b7392b9b9417739abc49793955","modified":1710051856894},{"_id":"public/tags/å‡¤é›/index.html","hash":"35ca836c7a244265cbb2c459e754f8f5c93429ac","modified":1710051856894},{"_id":"public/tags/éª†é›/index.html","hash":"f8d5ad3dbc0eab55ab812cf1b30e0a8f1070ec0e","modified":1710051856894},{"_id":"public/tags/é»„å¸/index.html","hash":"fa56e69e1b22eb575e64d3aa840c68962f8ae2c9","modified":1710051856894},{"_id":"public/tags/å°ç»¿/index.html","hash":"273ba3bfd35c5f356d103c99589678802addef49","modified":1710051856894},{"_id":"public/tags/çŒ«å’ª/index.html","hash":"384545e6f2ef66623a427b907355e74d9d2e2754","modified":1710051856894},{"_id":"public/tags/åœŸé¸¡/index.html","hash":"68c68116e45e0ac01647384daecdee81ec2320c5","modified":1710051856894},{"_id":"public/tags/ç†Š/index.html","hash":"09638a810e2f78afc366d683b9f02b25d56c8205","modified":1710051856894},{"_id":"public/tags/ğŸ»/index.html","hash":"90d257a6d85c8a9d915bca10c602557fe07edb78","modified":1710051856894},{"_id":"public/tags/ğŸ±/index.html","hash":"0a7c8c4014f67318ab94221db70bd8414fa8286d","modified":1710051856894},{"_id":"public/tags/ğŸ‡/index.html","hash":"ad235caba371ea929cc8bc127c5035831f1c1196","modified":1710051856894},{"_id":"public/tags/ğŸ°/index.html","hash":"39fd9a2a4b328a9e6491b0b6a017787d68b8764d","modified":1710051856894},{"_id":"public/tags/ğŸŠ/index.html","hash":"16df46d5b9e07855e2b07057855c29e41373707c","modified":1710051856894},{"_id":"public/tags/emoren/index.html","hash":"ee2613c9d67a4b2ba499e48cf303877681472000","modified":1710051856894},{"_id":"public/tags/acl/index.html","hash":"eed2abb677f862a81bbefbca7e102f248f460d1d","modified":1710051856894},{"_id":"public/tags/lillesand/index.html","hash":"e6e1176c27bce87320266c4bbd86ed17df87bde9","modified":1710051856894},{"_id":"public/tags/åŠ æ‹¿å¤§/index.html","hash":"ba4b1f5d5e5aa0fbb0c7134dcb0366f0be92e67b","modified":1710051856894},{"_id":"public/tags/html/index.html","hash":"afb5af01d807b0e8d3c01f2bd3eafb05c137d220","modified":1710051856894},{"_id":"public/tags/liquid/index.html","hash":"70e342e4c7e20fecdb1d21ed2c55b10a9355f470","modified":1710051856894},{"_id":"public/categories/life/index.html","hash":"1ead14620c432a6b108674e77e030df9827775b5","modified":1710051856894},{"_id":"public/categories/research/index.html","hash":"c2fda38d93e9b77748d58bdf735f306e9469717e","modified":1710051856894},{"_id":"public/categories/paper-note/index.html","hash":"5e4ec529236f17d994f762e4b58868add0d3069e","modified":1710051856894},{"_id":"public/categories/thoughts/index.html","hash":"9e83c0dee42565b264d7342a834c510baaf304e4","modified":1710051856894},{"_id":"public/categories/test/index.html","hash":"e4b9586539af58c2af15db49ce405964a48f4b69","modified":1710051856894},{"_id":"public/favicon.ico","hash":"8ede8a7e8edefbd78bb978d781a502f696c32be9","modified":1710051804435},{"_id":"public/images/Fengye.png","hash":"bd5ebaf9ffd792aa83701e7701ad5130e1b4df4c","modified":1710051804435},{"_id":"public/images/logo.svg","hash":"1b7a73d948e593dcec7549d63e5ac60ad6db6a8f","modified":1710051804435},{"_id":"public/favicon-32x32.png","hash":"ebaaa72b28fc97b0e4c419837ac1f2c54110d070","modified":1710051804435},{"_id":"public/images/favicon.ico","hash":"8ede8a7e8edefbd78bb978d781a502f696c32be9","modified":1710051804435},{"_id":"public/images/favicon-32x32.png","hash":"ebaaa72b28fc97b0e4c419837ac1f2c54110d070","modified":1710051804435},{"_id":"public/css/search.css","hash":"0efc0555c7c89a83321f0f4795b90df950d356bd","modified":1710051804435},{"_id":"public/pdf/cv.pdf","hash":"84479a0480b08fc28ed34e856c8a8dcad815dbc4","modified":1710051804435},{"_id":"public/2023/10/07/actadd/alg.png","hash":"dc702db52c81311391f17e05666a5a5c9e082ed2","modified":1710051804435},{"_id":"public/css/highlight.css","hash":"2588d7853381a2f0d2dc3edf1b5e9e67951d626f","modified":1710051804435},{"_id":"public/js/search.js","hash":"9b14f8e4a76d8da293d538d73d340dfd5b5e5402","modified":1710051804435},{"_id":"public/js/main.js","hash":"78e3f9e09ff68955215e8f2b4fcfa5d32662f81a","modified":1710051804435},{"_id":"public/css/main.css","hash":"4cdb9c722f98ddfcfb43070378d612da096a920d","modified":1710051804435},{"_id":"public/lib/clipboard.min.js","hash":"f48e9bfeca83e5057cc751e8c44fc07e9d976c06","modified":1710051804435},{"_id":"public/lib/tocbot/tocbot.min.css","hash":"3a2c80c85bdba2b71c604e32b1273c5c387948cc","modified":1710051804435},{"_id":"public/lib/tocbot/tocbot.min.js","hash":"4f1b40a6818fe6e955f2ce7de3b79aec4dcd0a7c","modified":1710051804435},{"_id":"public/lib/fancybox/fancybox.min.css","hash":"1564bb6a6b930a61875610c05001c4f7bfe9939a","modified":1710051804435},{"_id":"public/lib/jquery.min.js","hash":"69bb69e25ca7d5ef0935317584e6153f3fd9a88c","modified":1710051804435},{"_id":"public/lib/iconify-icon.min.js","hash":"7526cf2b54b9e657f377083129cc00c5aa4dc110","modified":1710051804435},{"_id":"public/lib/fancybox/fancybox.umd.min.js","hash":"e766e468e4f017b51a643648f6b4f05187c41d6b","modified":1710051804435},{"_id":"public/2023/10/07/actadd/method.png","hash":"a8b682bfe5ab4dbe09b14b083055f6690dd032c1","modified":1710051804435},{"_id":"public/2023/05/17/ç¬¬ä¸€ä¸ªå¸–å­ï¼Œçå†™ç‚¹ä¸œè¥¿/lillesand0.jpg","hash":"ea04c507fe1c7859b5e2c13265649e7ee5e10ca9","modified":1710051804435},{"_id":"public/2023/05/17/ç¬¬ä¸€ä¸ªå¸–å­ï¼Œçå†™ç‚¹ä¸œè¥¿/lillesand1.jpg","hash":"208e8d0ed9178a7935a930082b48e0a7371e5f62","modified":1710051804435},{"_id":"public/2024/01/10/infinitebench/data-stat-pie.png","hash":"3384e825a982ec9038295ef68d0856b1b04a1d07","modified":1710051804435},{"_id":"public/2023/10/07/actadd/result.png","hash":"9e94dccb92d8f3d25345b515322d23cf9ba24def","modified":1710051804435},{"_id":"public/2024/01/10/infinitebench/results.png","hash":"5b36c793d61b0c070f426e02db6223526d51e102","modified":1710051804435},{"_id":"public/2023/09/16/æ›´æ–°ä¸ªäººä¸»é¡µ/ä¸­å›½æŠ¤ç…§.jpg","hash":"ff10a197a19b140980e804180b0503216ef1d863","modified":1710051804435},{"_id":"public/images/portrait.jpg","hash":"c910db58974a63b1c7d8d256b99b0c6ee4901a50","modified":1710051804435},{"_id":"public/2023/09/16/æ›´æ–°ä¸ªäººä¸»é¡µ/ç”³è¯·ç­¾è¯.jpg","hash":"7df3acbc98aa10dca03efdb285fb40f2d4525f52","modified":1710051804435},{"_id":"public/2023/09/16/æ›´æ–°ä¸ªäººä¸»é¡µ/åƒæ¾³é—¨èœ.jpg","hash":"92855a153bb0c0644b327b55f433ebe661a78fa2","modified":1710051804435},{"_id":"public/images/portrait.png","hash":"049faf25dabe7878afe4181dc7355845504ce90c","modified":1710051804435},{"_id":"public/2023/10/05/2023ä¸­ç§‹/æ­¦æ±‰æ¬¢ä¹è°·.png","hash":"977e23f8ad8150689b0b34f3df1a9bbed2909460","modified":1710051804435},{"_id":"public/2023/10/05/2023ä¸­ç§‹/æ­¦å•†æ¢¦æ—¶ä»£.png","hash":"9dec428cc6c918f9eaf6d47dd1a36f797678f72b","modified":1710051804435},{"_id":"public/2023/10/05/2023ä¸­ç§‹/æ–°å¤©åœ°-éœ¸ç‹èŒ¶å§¬.png","hash":"cd9609e61378cea89811ef3d1cf348ec59822036","modified":1710051804435},{"_id":"public/2023/10/05/2023ä¸­ç§‹/è§£æ”¾å…¬å›­ä¸­é—´.png","hash":"359aaf1437e94f494e6dd419ff6b7001a6e0b362","modified":1710051804435},{"_id":"public/source/_data/index.json","hash":"a5d3ef034188d579ebf1d8df46370f1e5070962e","modified":1710051856894}],"Category":[{"name":"Life","_id":"cltl4oxef0004xh7kdcffd02p"},{"name":"Research","_id":"cltl4oxei000cxh7k8w255zde"},{"name":"Paper Note","_id":"cltl4oxej000jxh7k86kp7os4"},{"name":"Thoughts","_id":"cltl4oxem000uxh7k7oa1duf3"},{"name":"Test","_id":"cltl4oxen0014xh7k50v9dsv5"},{"name":"Paper","_id":"cltl4oxen0019xh7k621mhbtp"}],"Data":[],"Page":[{"title":"About Me","date":"2023-09-14T10:18:06.000Z","type":"about","_content":"\n<img src=\"images/portrait.jpg\" alt=\"Portrait of Chen Yingfa having lunch in Beijing, taken by Luo Yining.\"/>\n\n<iconify-icon icon=\"mingcute:world-2-fill\"></iconify-icon> [ä¸­æ–‡](#Chinese-Version-ä¸­æ–‡ç‰ˆæœ¬)\n\n<iconify-icon icon=\"mingcute:link-fill\"></iconify-icon> Social links:\n\n- [Google Scholar](https://scholar.google.com/citations?user=IgPWvEQAAAAJ&hl=en)\n- [X (Twitter)](https://www.twitter.com/DonnyChan123)\n- [GitHub](https://www.github.com/chen-yingfa)\n- [çŸ¥ä¹](https://www.zhihu.com/people/chen-ying-fa-34)\n- [Bç«™](https://space.bilibili.com/474619698?spm_id_from=333.1007.0.0)\n\n<iconify-icon icon=\"mingcute:mail-fill\"></iconify-icon> Email: (either is ok) \n\n- chenyingfa1999@qq.com\n- donnychan1999@gmail.com\n\n---\n\nä½ å¥½, hello, hei!\n\nMy name is Yingfa Chen (é™ˆè‹±å‘).\n\nI'm a 2nd year graduate student at the [Natural Language Processing lab at Tsinghua University](http://nlp.csai.tsinghua.edu.cn/), advised by Prof. Zhiyuan Liu. My research interests are about controlling the knowledge and behavior of large language models.\n\nI'm just getting started with academia, and will be applying for PhD in the same lab this year (the end of 2023), there is so much to learn!\n\n[:page_facing_up: My resume (last updated: December, 2022)](/pdf/cv.pdf)\n\n## Education\n\n---\n\n**M.S. in Computer Science and Technology (è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯)**\n\n<div align=\"right\">Sep 2022 - Jul 2024 (expected)</div>\n\nTsinghua University, China\n\n*Research direction: natural language processing, large language models*\n\n*Advisor: Prof. Zhiyuan Liu*\n\n---\n\n**B.S. in Computer Science and Technology (è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯)**\n\n<div align=\"right\">Aug 2018 - Jul 2022</div>\n\nTsinghua University, China \n\n---\n\n**General Studies in Natural Science (Studiespesialisering med realfag)**\n\n<div align=\"right\">\nAug 2014 - Jul 2018\n</div>\n\nMÃ¸glestu High School, Norway\n\n---\n\n## Personal Background\n\nI was born in March 13, 1999 in Arendal Norway. I grew up in Lillesand, a small coastal town in Aust-Agder, Norway. My parents are ethnically Chinese but born respectively in Vietnam and Cambodia, and they moved to Norway as refugees. My mother tongue is Cantonese Chinese.\n\nWhen I was 19, I went to Beijing for a Bachelor's degree in Computer Science at Tsinghua University.\n\nSince March of 2022, I am happily in a relationship with [Luo Yining](https://www.github.com/luo-yining/)^[The tag [#00](../../../tags/00) refers to my girlfriend.].\n\n## Interests\n\nğŸ¸ Sports: Badminton, running, soccer.\n\nğŸ® Entertainment: Video games, C-pop, anime, manga, Jackie Cheung, etc.\n\n---\n\n> Below is the Chinese version of this text, no need to read it if you understand the above text already.\n\n## Chinese Version ä¸­æ–‡ç‰ˆæœ¬\n\næˆ‘å«é™ˆè‹±å‘ã€‚\n\næˆ‘æ˜¯æ¸…åå¤§å­¦[è‡ªç„¶è¯­è¨€å¤„ç†ä¸ç¤¾ä¼šäººæ–‡è®¡ç®—å®éªŒå®¤](http://nlp.csai.tsinghua.edu.cn/)çš„äºŒå¹´çº§ç ”ç©¶ç”Ÿï¼Œç”±åˆ˜çŸ¥è¿œå‰¯æ•™æˆæŒ‡å¯¼ã€‚æˆ‘çš„ç ”ç©¶å…´è¶£æ˜¯å…³äºæ§åˆ¶å¤§å‹è¯­è¨€æ¨¡å‹çš„çŸ¥è¯†å’Œè¡Œä¸ºã€‚\n\næˆ‘åˆšåˆšå¼€å§‹å­¦æœ¯ç”Ÿæ¶¯ï¼Œå°†åœ¨ä»Šå¹´ï¼ˆ2023å¹´ä½ï¼‰ç”³è¯·åšå£«å­¦ä½ï¼Œè¿˜æœ‰å¾ˆå¤šè¦å­¦ä¹ ï¼\n\n[ğŸ“ƒ æˆ‘çš„ç®€å†ï¼ˆæœ€åæ›´æ–°ï¼š2022å¹´12æœˆï¼‰](/pdf/cv.pdf)\n\n### æ•™è‚²èƒŒæ™¯\n\n- **ç¡•å£«ï¼šè®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯**\n\n    æ¸…åå¤§å­¦ï¼Œä¸­å›½\n\n    2022å¹´9æœˆ - 2024å¹´7æœˆï¼ˆé¢„è®¡ï¼‰\n\n    ç ”ç©¶æ–¹å‘ï¼šè‡ªç„¶è¯­è¨€å¤„ç†ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹\n\n    å¯¼å¸ˆï¼šåˆ˜çŸ¥è¿œå‰¯æ•™æˆ\n\n\n- **æœ¬ç§‘ï¼šè®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯**\n\n    æ¸…åå¤§å­¦ï¼Œä¸­å›½\n\n    2018å¹´8æœˆ - 2022å¹´7æœˆ\n\n\n- **é«˜ä¸­ï¼šè‡ªç„¶ç§‘å­¦é€šè¯†æ•™è‚²**\n\n    æŒªå¨ï¼Œè«æ ¼å‹’æ–¯å›¾é«˜ä¸­\n\n    2014å¹´8æœˆ - 2018å¹´7æœˆ\n\n### ä¸ªäººèƒŒæ™¯\n\næˆ‘äº 1999 å¹´ 3 æœˆ 13 æ—¥åœ¨æŒªå¨ Arendal å‡ºç”Ÿã€‚æˆ‘é•¿å¤§åœ¨æŒªå¨ Aust-Agder çš„ä¸€ä¸ªæ²¿æµ·å°é•‡ Lillesandã€‚æˆ‘çš„çˆ¶æ¯æ˜¯åè£”ï¼Œåˆ†åˆ«å‡ºç”Ÿåœ¨è¶Šå—å’ŒæŸ¬åŸ”å¯¨ï¼Œä»–ä»¬ä½œä¸ºéš¾æ°‘ç§»æ°‘åˆ°æŒªå¨ã€‚æˆ‘çš„æ¯è¯­æ˜¯å¹¿ä¸œè¯ã€‚\n\nåä¹å²æ—¶ï¼Œæˆ‘å‰å¾€åŒ—äº¬ï¼Œåœ¨æ¸…åå¤§å­¦æ”»è¯»è®¡ç®—æœºç§‘å­¦å­¦å£«å­¦ä½ã€‚\n\nè‡ª 2022 å¹´ 3 æœˆä»¥æ¥ï¼Œæˆ‘ä¸æˆ‘å¥³å‹[éª†æ€¡å®](https://www.github.com/luo-yining/)ç›¸çˆ±ã€‚\n\n### å…´è¶£çˆ±å¥½\n\nğŸ¸ ä½“è‚²ï¼šç¾½æ¯›çƒï¼Œè·‘æ­¥ï¼Œè¶³çƒã€‚\n\nğŸ® å¨±ä¹ï¼šç”µå­æ¸¸æˆï¼ŒC-popï¼ŒåŠ¨æ¼«ï¼Œæ¼«ç”»ï¼Œå¼ å­¦å‹ç­‰ç­‰ã€‚\n\n> Translated by ChatGPT, reviewed by me.\n\n---\n\n## Extra\n\nHere is a binary search in Python:\n\n```python\ndef bin_search(arr: list, target) -> int:\n    lo, hi = 0, len(arr)\n    while lo < hi:\n        m = (lo + hi) // 2\n        if arr[m] < target:\n            lo = m + 1\n        else:\n            hi = m\n    return lo\n```\n","source":"about.md","raw":"---\ntitle: About Me\ndate: 2023-09-14 18:18:06\ntype: \"about\"\n---\n\n<img src=\"images/portrait.jpg\" alt=\"Portrait of Chen Yingfa having lunch in Beijing, taken by Luo Yining.\"/>\n\n<iconify-icon icon=\"mingcute:world-2-fill\"></iconify-icon> [ä¸­æ–‡](#Chinese-Version-ä¸­æ–‡ç‰ˆæœ¬)\n\n<iconify-icon icon=\"mingcute:link-fill\"></iconify-icon> Social links:\n\n- [Google Scholar](https://scholar.google.com/citations?user=IgPWvEQAAAAJ&hl=en)\n- [X (Twitter)](https://www.twitter.com/DonnyChan123)\n- [GitHub](https://www.github.com/chen-yingfa)\n- [çŸ¥ä¹](https://www.zhihu.com/people/chen-ying-fa-34)\n- [Bç«™](https://space.bilibili.com/474619698?spm_id_from=333.1007.0.0)\n\n<iconify-icon icon=\"mingcute:mail-fill\"></iconify-icon> Email: (either is ok) \n\n- chenyingfa1999@qq.com\n- donnychan1999@gmail.com\n\n---\n\nä½ å¥½, hello, hei!\n\nMy name is Yingfa Chen (é™ˆè‹±å‘).\n\nI'm a 2nd year graduate student at the [Natural Language Processing lab at Tsinghua University](http://nlp.csai.tsinghua.edu.cn/), advised by Prof. Zhiyuan Liu. My research interests are about controlling the knowledge and behavior of large language models.\n\nI'm just getting started with academia, and will be applying for PhD in the same lab this year (the end of 2023), there is so much to learn!\n\n[:page_facing_up: My resume (last updated: December, 2022)](/pdf/cv.pdf)\n\n## Education\n\n---\n\n**M.S. in Computer Science and Technology (è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯)**\n\n<div align=\"right\">Sep 2022 - Jul 2024 (expected)</div>\n\nTsinghua University, China\n\n*Research direction: natural language processing, large language models*\n\n*Advisor: Prof. Zhiyuan Liu*\n\n---\n\n**B.S. in Computer Science and Technology (è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯)**\n\n<div align=\"right\">Aug 2018 - Jul 2022</div>\n\nTsinghua University, China \n\n---\n\n**General Studies in Natural Science (Studiespesialisering med realfag)**\n\n<div align=\"right\">\nAug 2014 - Jul 2018\n</div>\n\nMÃ¸glestu High School, Norway\n\n---\n\n## Personal Background\n\nI was born in March 13, 1999 in Arendal Norway. I grew up in Lillesand, a small coastal town in Aust-Agder, Norway. My parents are ethnically Chinese but born respectively in Vietnam and Cambodia, and they moved to Norway as refugees. My mother tongue is Cantonese Chinese.\n\nWhen I was 19, I went to Beijing for a Bachelor's degree in Computer Science at Tsinghua University.\n\nSince March of 2022, I am happily in a relationship with [Luo Yining](https://www.github.com/luo-yining/)^[The tag [#00](../../../tags/00) refers to my girlfriend.].\n\n## Interests\n\nğŸ¸ Sports: Badminton, running, soccer.\n\nğŸ® Entertainment: Video games, C-pop, anime, manga, Jackie Cheung, etc.\n\n---\n\n> Below is the Chinese version of this text, no need to read it if you understand the above text already.\n\n## Chinese Version ä¸­æ–‡ç‰ˆæœ¬\n\næˆ‘å«é™ˆè‹±å‘ã€‚\n\næˆ‘æ˜¯æ¸…åå¤§å­¦[è‡ªç„¶è¯­è¨€å¤„ç†ä¸ç¤¾ä¼šäººæ–‡è®¡ç®—å®éªŒå®¤](http://nlp.csai.tsinghua.edu.cn/)çš„äºŒå¹´çº§ç ”ç©¶ç”Ÿï¼Œç”±åˆ˜çŸ¥è¿œå‰¯æ•™æˆæŒ‡å¯¼ã€‚æˆ‘çš„ç ”ç©¶å…´è¶£æ˜¯å…³äºæ§åˆ¶å¤§å‹è¯­è¨€æ¨¡å‹çš„çŸ¥è¯†å’Œè¡Œä¸ºã€‚\n\næˆ‘åˆšåˆšå¼€å§‹å­¦æœ¯ç”Ÿæ¶¯ï¼Œå°†åœ¨ä»Šå¹´ï¼ˆ2023å¹´ä½ï¼‰ç”³è¯·åšå£«å­¦ä½ï¼Œè¿˜æœ‰å¾ˆå¤šè¦å­¦ä¹ ï¼\n\n[ğŸ“ƒ æˆ‘çš„ç®€å†ï¼ˆæœ€åæ›´æ–°ï¼š2022å¹´12æœˆï¼‰](/pdf/cv.pdf)\n\n### æ•™è‚²èƒŒæ™¯\n\n- **ç¡•å£«ï¼šè®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯**\n\n    æ¸…åå¤§å­¦ï¼Œä¸­å›½\n\n    2022å¹´9æœˆ - 2024å¹´7æœˆï¼ˆé¢„è®¡ï¼‰\n\n    ç ”ç©¶æ–¹å‘ï¼šè‡ªç„¶è¯­è¨€å¤„ç†ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹\n\n    å¯¼å¸ˆï¼šåˆ˜çŸ¥è¿œå‰¯æ•™æˆ\n\n\n- **æœ¬ç§‘ï¼šè®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯**\n\n    æ¸…åå¤§å­¦ï¼Œä¸­å›½\n\n    2018å¹´8æœˆ - 2022å¹´7æœˆ\n\n\n- **é«˜ä¸­ï¼šè‡ªç„¶ç§‘å­¦é€šè¯†æ•™è‚²**\n\n    æŒªå¨ï¼Œè«æ ¼å‹’æ–¯å›¾é«˜ä¸­\n\n    2014å¹´8æœˆ - 2018å¹´7æœˆ\n\n### ä¸ªäººèƒŒæ™¯\n\næˆ‘äº 1999 å¹´ 3 æœˆ 13 æ—¥åœ¨æŒªå¨ Arendal å‡ºç”Ÿã€‚æˆ‘é•¿å¤§åœ¨æŒªå¨ Aust-Agder çš„ä¸€ä¸ªæ²¿æµ·å°é•‡ Lillesandã€‚æˆ‘çš„çˆ¶æ¯æ˜¯åè£”ï¼Œåˆ†åˆ«å‡ºç”Ÿåœ¨è¶Šå—å’ŒæŸ¬åŸ”å¯¨ï¼Œä»–ä»¬ä½œä¸ºéš¾æ°‘ç§»æ°‘åˆ°æŒªå¨ã€‚æˆ‘çš„æ¯è¯­æ˜¯å¹¿ä¸œè¯ã€‚\n\nåä¹å²æ—¶ï¼Œæˆ‘å‰å¾€åŒ—äº¬ï¼Œåœ¨æ¸…åå¤§å­¦æ”»è¯»è®¡ç®—æœºç§‘å­¦å­¦å£«å­¦ä½ã€‚\n\nè‡ª 2022 å¹´ 3 æœˆä»¥æ¥ï¼Œæˆ‘ä¸æˆ‘å¥³å‹[éª†æ€¡å®](https://www.github.com/luo-yining/)ç›¸çˆ±ã€‚\n\n### å…´è¶£çˆ±å¥½\n\nğŸ¸ ä½“è‚²ï¼šç¾½æ¯›çƒï¼Œè·‘æ­¥ï¼Œè¶³çƒã€‚\n\nğŸ® å¨±ä¹ï¼šç”µå­æ¸¸æˆï¼ŒC-popï¼ŒåŠ¨æ¼«ï¼Œæ¼«ç”»ï¼Œå¼ å­¦å‹ç­‰ç­‰ã€‚\n\n> Translated by ChatGPT, reviewed by me.\n\n---\n\n## Extra\n\nHere is a binary search in Python:\n\n```python\ndef bin_search(arr: list, target) -> int:\n    lo, hi = 0, len(arr)\n    while lo < hi:\n        m = (lo + hi) // 2\n        if arr[m] < target:\n            lo = m + 1\n        else:\n            hi = m\n    return lo\n```\n","updated":"2023-09-30T09:06:19.412Z","path":"about.html","comments":1,"layout":"page","_id":"cltl4oxeb0000xh7k3dl59pfl","content":"<img src=\"images/portrait.jpg\" alt=\"Portrait of Chen Yingfa having lunch in Beijing, taken by Luo Yining.\">\n<p><iconify-icon icon=\"mingcute:world-2-fill\"></iconify-icon> <a href=\"#Chinese-Version-%E4%B8%AD%E6%96%87%E7%89%88%E6%9C%AC\">ä¸­æ–‡</a></p>\n<p><iconify-icon icon=\"mingcute:link-fill\"></iconify-icon> Social links:</p>\n<ul>\n<li><a href=\"https://scholar.google.com/citations?user=IgPWvEQAAAAJ&amp;hl=en\">Google Scholar</a></li>\n<li><a href=\"https://www.twitter.com/DonnyChan123\">X (Twitter)</a></li>\n<li><a href=\"https://www.github.com/chen-yingfa\">GitHub</a></li>\n<li><a href=\"https://www.zhihu.com/people/chen-ying-fa-34\">çŸ¥ä¹</a></li>\n<li><a href=\"https://space.bilibili.com/474619698?spm_id_from=333.1007.0.0\">Bç«™</a></li>\n</ul>\n<p><iconify-icon icon=\"mingcute:mail-fill\"></iconify-icon> Email: (either is ok)</p>\n<ul>\n<li><a href=\"mailto:chenyingfa1999@qq.com\">chenyingfa1999@qq.com</a></li>\n<li><a href=\"mailto:donnychan1999@gmail.com\">donnychan1999@gmail.com</a></li>\n</ul>\n<hr>\n<p>ä½ å¥½, hello, hei!</p>\n<p>My name is Yingfa Chen (é™ˆè‹±å‘).</p>\n<p>I'm a 2nd year graduate student at the <a href=\"http://nlp.csai.tsinghua.edu.cn/\">Natural Language Processing lab at Tsinghua University</a>, advised by Prof. Zhiyuan Liu. My research interests are about controlling the knowledge and behavior of large language models.</p>\n<p>I'm just getting started with academia, and will be applying for PhD in the same lab this year (the end of 2023), there is so much to learn!</p>\n<p><a href=\"/pdf/cv.pdf\"><span class=\"github-emoji\"><span>ğŸ“„</span><img src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4c4.png?v8\" aria-hidden=\"true\" onerror=\"this.parent.classList.add('github-emoji-fallback')\"></span> My resume (last updated: December, 2022)</a></p>\n<h2 id=\"Education\">Education</h2>\n<hr>\n<p><strong>M.S. in Computer Science and Technology (è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯)</strong></p>\n<div align=\"right\">Sep 2022 - Jul 2024 (expected)</div>\n<p>Tsinghua University, China</p>\n<p><em>Research direction: natural language processing, large language models</em></p>\n<p><em>Advisor: Prof. Zhiyuan Liu</em></p>\n<hr>\n<p><strong>B.S. in Computer Science and Technology (è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯)</strong></p>\n<div align=\"right\">Aug 2018 - Jul 2022</div>\n<p>Tsinghua University, China</p>\n<hr>\n<p><strong>General Studies in Natural Science (Studiespesialisering med realfag)</strong></p>\n<div align=\"right\">\nAug 2014 - Jul 2018\n</div>\n<p>MÃ¸glestu High School, Norway</p>\n<hr>\n<h2 id=\"Personal-Background\">Personal Background</h2>\n<p>I was born in March 13, 1999 in Arendal Norway. I grew up in Lillesand, a small coastal town in Aust-Agder, Norway. My parents are ethnically Chinese but born respectively in Vietnam and Cambodia, and they moved to Norway as refugees. My mother tongue is Cantonese Chinese.</p>\n<p>When I was 19, I went to Beijing for a Bachelor's degree in Computer Science at Tsinghua University.</p>\n<p>Since March of 2022, I am happily in a relationship with <a href=\"https://www.github.com/luo-yining/\">Luo Yining</a><sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">[1]</a></sup>.</p>\n<h2 id=\"Interests\">Interests</h2>\n<p>ğŸ¸ Sports: Badminton, running, soccer.</p>\n<p>ğŸ® Entertainment: Video games, C-pop, anime, manga, Jackie Cheung, etc.</p>\n<hr>\n<blockquote>\n<p>Below is the Chinese version of this text, no need to read it if you understand the above text already.</p>\n</blockquote>\n<h2 id=\"Chinese-Version-ä¸­æ–‡ç‰ˆæœ¬\">Chinese Version ä¸­æ–‡ç‰ˆæœ¬</h2>\n<p>æˆ‘å«é™ˆè‹±å‘ã€‚</p>\n<p>æˆ‘æ˜¯æ¸…åå¤§å­¦<a href=\"http://nlp.csai.tsinghua.edu.cn/\">è‡ªç„¶è¯­è¨€å¤„ç†ä¸ç¤¾ä¼šäººæ–‡è®¡ç®—å®éªŒå®¤</a>çš„äºŒå¹´çº§ç ”ç©¶ç”Ÿï¼Œç”±åˆ˜çŸ¥è¿œå‰¯æ•™æˆæŒ‡å¯¼ã€‚æˆ‘çš„ç ”ç©¶å…´è¶£æ˜¯å…³äºæ§åˆ¶å¤§å‹è¯­è¨€æ¨¡å‹çš„çŸ¥è¯†å’Œè¡Œä¸ºã€‚</p>\n<p>æˆ‘åˆšåˆšå¼€å§‹å­¦æœ¯ç”Ÿæ¶¯ï¼Œå°†åœ¨ä»Šå¹´ï¼ˆ2023å¹´ä½ï¼‰ç”³è¯·åšå£«å­¦ä½ï¼Œè¿˜æœ‰å¾ˆå¤šè¦å­¦ä¹ ï¼</p>\n<p><a href=\"/pdf/cv.pdf\">ğŸ“ƒ æˆ‘çš„ç®€å†ï¼ˆæœ€åæ›´æ–°ï¼š2022å¹´12æœˆï¼‰</a></p>\n<h3 id=\"æ•™è‚²èƒŒæ™¯\">æ•™è‚²èƒŒæ™¯</h3>\n<ul>\n<li>\n<p><strong>ç¡•å£«ï¼šè®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯</strong></p>\n<p>æ¸…åå¤§å­¦ï¼Œä¸­å›½</p>\n<p>2022å¹´9æœˆ - 2024å¹´7æœˆï¼ˆé¢„è®¡ï¼‰</p>\n<p>ç ”ç©¶æ–¹å‘ï¼šè‡ªç„¶è¯­è¨€å¤„ç†ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹</p>\n<p>å¯¼å¸ˆï¼šåˆ˜çŸ¥è¿œå‰¯æ•™æˆ</p>\n</li>\n<li>\n<p><strong>æœ¬ç§‘ï¼šè®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯</strong></p>\n<p>æ¸…åå¤§å­¦ï¼Œä¸­å›½</p>\n<p>2018å¹´8æœˆ - 2022å¹´7æœˆ</p>\n</li>\n<li>\n<p><strong>é«˜ä¸­ï¼šè‡ªç„¶ç§‘å­¦é€šè¯†æ•™è‚²</strong></p>\n<p>æŒªå¨ï¼Œè«æ ¼å‹’æ–¯å›¾é«˜ä¸­</p>\n<p>2014å¹´8æœˆ - 2018å¹´7æœˆ</p>\n</li>\n</ul>\n<h3 id=\"ä¸ªäººèƒŒæ™¯\">ä¸ªäººèƒŒæ™¯</h3>\n<p>æˆ‘äº 1999 å¹´ 3 æœˆ 13 æ—¥åœ¨æŒªå¨ Arendal å‡ºç”Ÿã€‚æˆ‘é•¿å¤§åœ¨æŒªå¨ Aust-Agder çš„ä¸€ä¸ªæ²¿æµ·å°é•‡ Lillesandã€‚æˆ‘çš„çˆ¶æ¯æ˜¯åè£”ï¼Œåˆ†åˆ«å‡ºç”Ÿåœ¨è¶Šå—å’ŒæŸ¬åŸ”å¯¨ï¼Œä»–ä»¬ä½œä¸ºéš¾æ°‘ç§»æ°‘åˆ°æŒªå¨ã€‚æˆ‘çš„æ¯è¯­æ˜¯å¹¿ä¸œè¯ã€‚</p>\n<p>åä¹å²æ—¶ï¼Œæˆ‘å‰å¾€åŒ—äº¬ï¼Œåœ¨æ¸…åå¤§å­¦æ”»è¯»è®¡ç®—æœºç§‘å­¦å­¦å£«å­¦ä½ã€‚</p>\n<p>è‡ª 2022 å¹´ 3 æœˆä»¥æ¥ï¼Œæˆ‘ä¸æˆ‘å¥³å‹<a href=\"https://www.github.com/luo-yining/\">éª†æ€¡å®</a>ç›¸çˆ±ã€‚</p>\n<h3 id=\"å…´è¶£çˆ±å¥½\">å…´è¶£çˆ±å¥½</h3>\n<p>ğŸ¸ ä½“è‚²ï¼šç¾½æ¯›çƒï¼Œè·‘æ­¥ï¼Œè¶³çƒã€‚</p>\n<p>ğŸ® å¨±ä¹ï¼šç”µå­æ¸¸æˆï¼ŒC-popï¼ŒåŠ¨æ¼«ï¼Œæ¼«ç”»ï¼Œå¼ å­¦å‹ç­‰ç­‰ã€‚</p>\n<blockquote>\n<p>Translated by ChatGPT, reviewed by me.</p>\n</blockquote>\n<hr>\n<h2 id=\"Extra\">Extra</h2>\n<p>Here is a binary search in Python:</p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">bin_search</span>(<span class=\"params\">arr: <span class=\"built_in\">list</span>, target</span>) -&gt; <span class=\"built_in\">int</span>:</span><br><span class=\"line\">    lo, hi = <span class=\"number\">0</span>, <span class=\"built_in\">len</span>(arr)</span><br><span class=\"line\">    <span class=\"keyword\">while</span> lo &lt; hi:</span><br><span class=\"line\">        m = (lo + hi) // <span class=\"number\">2</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> arr[m] &lt; target:</span><br><span class=\"line\">            lo = m + <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            hi = m</span><br><span class=\"line\">    <span class=\"keyword\">return</span> lo</span><br></pre></td></tr></tbody></table></figure>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn1\" class=\"footnote-item\"><p>The tag <a href=\"../../../tags/00\">#00</a> refers to my girlfriend. <a href=\"#fnref1\" class=\"footnote-backref\">â†©ï¸</a></p>\n</li>\n</ol>\n</section>\n","site":{"data":{}},"excerpt":"","more":"<img src=\"images/portrait.jpg\" alt=\"Portrait of Chen Yingfa having lunch in Beijing, taken by Luo Yining.\"/>\n<p><iconify-icon icon=\"mingcute:world-2-fill\"></iconify-icon> <a href=\"#Chinese-Version-%E4%B8%AD%E6%96%87%E7%89%88%E6%9C%AC\">ä¸­æ–‡</a></p>\n<p><iconify-icon icon=\"mingcute:link-fill\"></iconify-icon> Social links:</p>\n<ul>\n<li><a href=\"https://scholar.google.com/citations?user=IgPWvEQAAAAJ&amp;hl=en\">Google Scholar</a></li>\n<li><a href=\"https://www.twitter.com/DonnyChan123\">X (Twitter)</a></li>\n<li><a href=\"https://www.github.com/chen-yingfa\">GitHub</a></li>\n<li><a href=\"https://www.zhihu.com/people/chen-ying-fa-34\">çŸ¥ä¹</a></li>\n<li><a href=\"https://space.bilibili.com/474619698?spm_id_from=333.1007.0.0\">Bç«™</a></li>\n</ul>\n<p><iconify-icon icon=\"mingcute:mail-fill\"></iconify-icon> Email: (either is ok)</p>\n<ul>\n<li><a href=\"mailto:chenyingfa1999@qq.com\">chenyingfa1999@qq.com</a></li>\n<li><a href=\"mailto:donnychan1999@gmail.com\">donnychan1999@gmail.com</a></li>\n</ul>\n<hr>\n<p>ä½ å¥½, hello, hei!</p>\n<p>My name is Yingfa Chen (é™ˆè‹±å‘).</p>\n<p>I'm a 2nd year graduate student at the <a href=\"http://nlp.csai.tsinghua.edu.cn/\">Natural Language Processing lab at Tsinghua University</a>, advised by Prof. Zhiyuan Liu. My research interests are about controlling the knowledge and behavior of large language models.</p>\n<p>I'm just getting started with academia, and will be applying for PhD in the same lab this year (the end of 2023), there is so much to learn!</p>\n<p><a href=\"/pdf/cv.pdf\">:page_facing_up: My resume (last updated: December, 2022)</a></p>\n<h2 id=\"Education\">Education</h2>\n<hr>\n<p><strong>M.S. in Computer Science and Technology (è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯)</strong></p>\n<div align=\"right\">Sep 2022 - Jul 2024 (expected)</div>\n<p>Tsinghua University, China</p>\n<p><em>Research direction: natural language processing, large language models</em></p>\n<p><em>Advisor: Prof. Zhiyuan Liu</em></p>\n<hr>\n<p><strong>B.S. in Computer Science and Technology (è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯)</strong></p>\n<div align=\"right\">Aug 2018 - Jul 2022</div>\n<p>Tsinghua University, China</p>\n<hr>\n<p><strong>General Studies in Natural Science (Studiespesialisering med realfag)</strong></p>\n<div align=\"right\">\nAug 2014 - Jul 2018\n</div>\n<p>MÃ¸glestu High School, Norway</p>\n<hr>\n<h2 id=\"Personal-Background\">Personal Background</h2>\n<p>I was born in March 13, 1999 in Arendal Norway. I grew up in Lillesand, a small coastal town in Aust-Agder, Norway. My parents are ethnically Chinese but born respectively in Vietnam and Cambodia, and they moved to Norway as refugees. My mother tongue is Cantonese Chinese.</p>\n<p>When I was 19, I went to Beijing for a Bachelor's degree in Computer Science at Tsinghua University.</p>\n<p>Since March of 2022, I am happily in a relationship with <a href=\"https://www.github.com/luo-yining/\">Luo Yining</a><sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">[1]</a></sup>.</p>\n<h2 id=\"Interests\">Interests</h2>\n<p>ğŸ¸ Sports: Badminton, running, soccer.</p>\n<p>ğŸ® Entertainment: Video games, C-pop, anime, manga, Jackie Cheung, etc.</p>\n<hr>\n<blockquote>\n<p>Below is the Chinese version of this text, no need to read it if you understand the above text already.</p>\n</blockquote>\n<h2 id=\"Chinese-Version-ä¸­æ–‡ç‰ˆæœ¬\">Chinese Version ä¸­æ–‡ç‰ˆæœ¬</h2>\n<p>æˆ‘å«é™ˆè‹±å‘ã€‚</p>\n<p>æˆ‘æ˜¯æ¸…åå¤§å­¦<a href=\"http://nlp.csai.tsinghua.edu.cn/\">è‡ªç„¶è¯­è¨€å¤„ç†ä¸ç¤¾ä¼šäººæ–‡è®¡ç®—å®éªŒå®¤</a>çš„äºŒå¹´çº§ç ”ç©¶ç”Ÿï¼Œç”±åˆ˜çŸ¥è¿œå‰¯æ•™æˆæŒ‡å¯¼ã€‚æˆ‘çš„ç ”ç©¶å…´è¶£æ˜¯å…³äºæ§åˆ¶å¤§å‹è¯­è¨€æ¨¡å‹çš„çŸ¥è¯†å’Œè¡Œä¸ºã€‚</p>\n<p>æˆ‘åˆšåˆšå¼€å§‹å­¦æœ¯ç”Ÿæ¶¯ï¼Œå°†åœ¨ä»Šå¹´ï¼ˆ2023å¹´ä½ï¼‰ç”³è¯·åšå£«å­¦ä½ï¼Œè¿˜æœ‰å¾ˆå¤šè¦å­¦ä¹ ï¼</p>\n<p><a href=\"/pdf/cv.pdf\">ğŸ“ƒ æˆ‘çš„ç®€å†ï¼ˆæœ€åæ›´æ–°ï¼š2022å¹´12æœˆï¼‰</a></p>\n<h3 id=\"æ•™è‚²èƒŒæ™¯\">æ•™è‚²èƒŒæ™¯</h3>\n<ul>\n<li>\n<p><strong>ç¡•å£«ï¼šè®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯</strong></p>\n<p>æ¸…åå¤§å­¦ï¼Œä¸­å›½</p>\n<p>2022å¹´9æœˆ - 2024å¹´7æœˆï¼ˆé¢„è®¡ï¼‰</p>\n<p>ç ”ç©¶æ–¹å‘ï¼šè‡ªç„¶è¯­è¨€å¤„ç†ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹</p>\n<p>å¯¼å¸ˆï¼šåˆ˜çŸ¥è¿œå‰¯æ•™æˆ</p>\n</li>\n<li>\n<p><strong>æœ¬ç§‘ï¼šè®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯</strong></p>\n<p>æ¸…åå¤§å­¦ï¼Œä¸­å›½</p>\n<p>2018å¹´8æœˆ - 2022å¹´7æœˆ</p>\n</li>\n<li>\n<p><strong>é«˜ä¸­ï¼šè‡ªç„¶ç§‘å­¦é€šè¯†æ•™è‚²</strong></p>\n<p>æŒªå¨ï¼Œè«æ ¼å‹’æ–¯å›¾é«˜ä¸­</p>\n<p>2014å¹´8æœˆ - 2018å¹´7æœˆ</p>\n</li>\n</ul>\n<h3 id=\"ä¸ªäººèƒŒæ™¯\">ä¸ªäººèƒŒæ™¯</h3>\n<p>æˆ‘äº 1999 å¹´ 3 æœˆ 13 æ—¥åœ¨æŒªå¨ Arendal å‡ºç”Ÿã€‚æˆ‘é•¿å¤§åœ¨æŒªå¨ Aust-Agder çš„ä¸€ä¸ªæ²¿æµ·å°é•‡ Lillesandã€‚æˆ‘çš„çˆ¶æ¯æ˜¯åè£”ï¼Œåˆ†åˆ«å‡ºç”Ÿåœ¨è¶Šå—å’ŒæŸ¬åŸ”å¯¨ï¼Œä»–ä»¬ä½œä¸ºéš¾æ°‘ç§»æ°‘åˆ°æŒªå¨ã€‚æˆ‘çš„æ¯è¯­æ˜¯å¹¿ä¸œè¯ã€‚</p>\n<p>åä¹å²æ—¶ï¼Œæˆ‘å‰å¾€åŒ—äº¬ï¼Œåœ¨æ¸…åå¤§å­¦æ”»è¯»è®¡ç®—æœºç§‘å­¦å­¦å£«å­¦ä½ã€‚</p>\n<p>è‡ª 2022 å¹´ 3 æœˆä»¥æ¥ï¼Œæˆ‘ä¸æˆ‘å¥³å‹<a href=\"https://www.github.com/luo-yining/\">éª†æ€¡å®</a>ç›¸çˆ±ã€‚</p>\n<h3 id=\"å…´è¶£çˆ±å¥½\">å…´è¶£çˆ±å¥½</h3>\n<p>ğŸ¸ ä½“è‚²ï¼šç¾½æ¯›çƒï¼Œè·‘æ­¥ï¼Œè¶³çƒã€‚</p>\n<p>ğŸ® å¨±ä¹ï¼šç”µå­æ¸¸æˆï¼ŒC-popï¼ŒåŠ¨æ¼«ï¼Œæ¼«ç”»ï¼Œå¼ å­¦å‹ç­‰ç­‰ã€‚</p>\n<blockquote>\n<p>Translated by ChatGPT, reviewed by me.</p>\n</blockquote>\n<hr>\n<h2 id=\"Extra\">Extra</h2>\n<p>Here is a binary search in Python:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">bin_search</span>(<span class=\"params\">arr: <span class=\"built_in\">list</span>, target</span>) -&gt; <span class=\"built_in\">int</span>:</span><br><span class=\"line\">    lo, hi = <span class=\"number\">0</span>, <span class=\"built_in\">len</span>(arr)</span><br><span class=\"line\">    <span class=\"keyword\">while</span> lo &lt; hi:</span><br><span class=\"line\">        m = (lo + hi) // <span class=\"number\">2</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> arr[m] &lt; target:</span><br><span class=\"line\">            lo = m + <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            hi = m</span><br><span class=\"line\">    <span class=\"keyword\">return</span> lo</span><br></pre></td></tr></table></figure>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn1\" class=\"footnote-item\"><p>The tag <a href=\"../../../tags/00\">#00</a> refers to my girlfriend. <a href=\"#fnref1\" class=\"footnote-backref\">â†©ï¸</a></p>\n</li>\n</ol>\n</section>\n"},{"_content":"Test","source":"friends/index.md","raw":"Test","date":"2023-09-15T16:49:37.338Z","updated":"2023-09-15T16:49:37.338Z","path":"friends/index.html","title":"","comments":1,"layout":"page","_id":"cltl4oxee0002xh7k2d4wdwdx","content":"<p>Test</p>\n","site":{"data":{}},"excerpt":"","more":"<p>Test</p>\n"},{"title":"My Publications","date":"2023-09-16T11:13:19.000Z","type":"publications","_content":"<!-- # My Publications -->\n\nList of all my publications:\n\n**2023**:\n\n- [GitHub] [$\\infty$-Bench: Extending Long Context Evaluation to Over 100K](/2024/01/10/InfiniteBench/)\n    - [Code](http://www.github.com/OpenBMB/InfiniteBench)\n\n- [preprint] [CFDBench: A Comprehensive Benchmark for Machine Learning Methods in Fluid Dynamics](/2023/09/16/CFDBench/)\n    - [Code](https://www.github.com/luo-yining/CFDBench) | [çŸ¥ä¹](https://zhuanlan.zhihu.com/p/656033757) | [Paper (preprints.org)](https://www.preprints.org/manuscript/202309.1550/v1)\n\n<!-- - [preprint] [Robust and Scalable Model Editing for Large Language Models](/2023/09/14/EREN/) -->\n- [ACL 2023] READIN: A Chinese Multi-Task Benchmark with Realistic and Diverse Input Noises\n    - [Code](https://www.github.com/THUNLP/READIN) | [Paper](https://aclanthology.org/2023.acl-long.460/)\n\n- [TACL 2023] Sub-Character Tokenization for Chinese Pretrained Language Models\n    - [Code](https://www.github.com/THUNLP/SubCharTokenization) | [Paper](https://aclanthology.org/2023.tacl-1.28/)\n\n**2022**:\n\n- [EMNLP 2022 Demo] BMCook: A Task-agnostic Compression Toolkit for Big Models\n    - [Code](https://www.github.com/OpenBMB/BMCook)\n","source":"publications.md","raw":"---\ntitle: My Publications\ndate: 2023-09-16 19:13:19\ntype: \"publications\"\n---\n<!-- # My Publications -->\n\nList of all my publications:\n\n**2023**:\n\n- [GitHub] [$\\infty$-Bench: Extending Long Context Evaluation to Over 100K](/2024/01/10/InfiniteBench/)\n    - [Code](http://www.github.com/OpenBMB/InfiniteBench)\n\n- [preprint] [CFDBench: A Comprehensive Benchmark for Machine Learning Methods in Fluid Dynamics](/2023/09/16/CFDBench/)\n    - [Code](https://www.github.com/luo-yining/CFDBench) | [çŸ¥ä¹](https://zhuanlan.zhihu.com/p/656033757) | [Paper (preprints.org)](https://www.preprints.org/manuscript/202309.1550/v1)\n\n<!-- - [preprint] [Robust and Scalable Model Editing for Large Language Models](/2023/09/14/EREN/) -->\n- [ACL 2023] READIN: A Chinese Multi-Task Benchmark with Realistic and Diverse Input Noises\n    - [Code](https://www.github.com/THUNLP/READIN) | [Paper](https://aclanthology.org/2023.acl-long.460/)\n\n- [TACL 2023] Sub-Character Tokenization for Chinese Pretrained Language Models\n    - [Code](https://www.github.com/THUNLP/SubCharTokenization) | [Paper](https://aclanthology.org/2023.tacl-1.28/)\n\n**2022**:\n\n- [EMNLP 2022 Demo] BMCook: A Task-agnostic Compression Toolkit for Big Models\n    - [Code](https://www.github.com/OpenBMB/BMCook)\n","updated":"2024-01-11T03:51:02.707Z","path":"publications.html","comments":1,"layout":"page","_id":"cltl4oxeg0006xh7k3vum3lmo","content":"<p>List of all my publications:</p>\n<p><strong>2023</strong>:</p>\n<ul>\n<li>\n<p>[GitHub] <a href=\"/2024/01/10/InfiniteBench/\">$\\infty$-Bench: Extending Long Context Evaluation to Over 100K</a></p>\n<ul>\n<li><a href=\"http://www.github.com/OpenBMB/InfiniteBench\">Code</a></li>\n</ul>\n</li>\n<li>\n<p>[preprint] <a href=\"/2023/09/16/CFDBench/\">CFDBench: A Comprehensive Benchmark for Machine Learning Methods in Fluid Dynamics</a></p>\n<ul>\n<li><a href=\"https://www.github.com/luo-yining/CFDBench\">Code</a> | <a href=\"https://zhuanlan.zhihu.com/p/656033757\">çŸ¥ä¹</a> | <a href=\"https://www.preprints.org/manuscript/202309.1550/v1\">Paper (preprints.org)</a></li>\n</ul>\n</li>\n</ul>\n<!-- - [preprint] [Robust and Scalable Model Editing for Large Language Models](/2023/09/14/EREN/) -->\n<ul>\n<li>\n<p>[ACL 2023] READIN: A Chinese Multi-Task Benchmark with Realistic and Diverse Input Noises</p>\n<ul>\n<li><a href=\"https://www.github.com/THUNLP/READIN\">Code</a> | <a href=\"https://aclanthology.org/2023.acl-long.460/\">Paper</a></li>\n</ul>\n</li>\n<li>\n<p>[TACL 2023] Sub-Character Tokenization for Chinese Pretrained Language Models</p>\n<ul>\n<li><a href=\"https://www.github.com/THUNLP/SubCharTokenization\">Code</a> | <a href=\"https://aclanthology.org/2023.tacl-1.28/\">Paper</a></li>\n</ul>\n</li>\n</ul>\n<p><strong>2022</strong>:</p>\n<ul>\n<li>[EMNLP 2022 Demo] BMCook: A Task-agnostic Compression Toolkit for Big Models\n<ul>\n<li><a href=\"https://www.github.com/OpenBMB/BMCook\">Code</a></li>\n</ul>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<!-- # My Publications -->\n<p>List of all my publications:</p>\n<p><strong>2023</strong>:</p>\n<ul>\n<li>\n<p>[GitHub] <a href=\"/2024/01/10/InfiniteBench/\">$\\infty$-Bench: Extending Long Context Evaluation to Over 100K</a></p>\n<ul>\n<li><a href=\"http://www.github.com/OpenBMB/InfiniteBench\">Code</a></li>\n</ul>\n</li>\n<li>\n<p>[preprint] <a href=\"/2023/09/16/CFDBench/\">CFDBench: A Comprehensive Benchmark for Machine Learning Methods in Fluid Dynamics</a></p>\n<ul>\n<li><a href=\"https://www.github.com/luo-yining/CFDBench\">Code</a> | <a href=\"https://zhuanlan.zhihu.com/p/656033757\">çŸ¥ä¹</a> | <a href=\"https://www.preprints.org/manuscript/202309.1550/v1\">Paper (preprints.org)</a></li>\n</ul>\n</li>\n</ul>\n<!-- - [preprint] [Robust and Scalable Model Editing for Large Language Models](/2023/09/14/EREN/) -->\n<ul>\n<li>\n<p>[ACL 2023] READIN: A Chinese Multi-Task Benchmark with Realistic and Diverse Input Noises</p>\n<ul>\n<li><a href=\"https://www.github.com/THUNLP/READIN\">Code</a> | <a href=\"https://aclanthology.org/2023.acl-long.460/\">Paper</a></li>\n</ul>\n</li>\n<li>\n<p>[TACL 2023] Sub-Character Tokenization for Chinese Pretrained Language Models</p>\n<ul>\n<li><a href=\"https://www.github.com/THUNLP/SubCharTokenization\">Code</a> | <a href=\"https://aclanthology.org/2023.tacl-1.28/\">Paper</a></li>\n</ul>\n</li>\n</ul>\n<p><strong>2022</strong>:</p>\n<ul>\n<li>[EMNLP 2022 Demo] BMCook: A Task-agnostic Compression Toolkit for Big Models\n<ul>\n<li><a href=\"https://www.github.com/OpenBMB/BMCook\">Code</a></li>\n</ul>\n</li>\n</ul>\n"},{"title":"Categories ç±»åˆ«","date":"2023-09-15T16:50:43.000Z","type":"categories","layout":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: Categories ç±»åˆ«\ndate: 2023-09-16 00:50:43\ntype: \"categories\"\nlayout: categories\n---\n","updated":"2024-03-10T04:49:18.292Z","path":"categories/index.html","comments":1,"_id":"cltl4oxeh0008xh7kh4qe1bwf","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"home","date":"2023-09-16T08:08:44.000Z","_content":"\nHello","source":"home/index.md","raw":"---\ntitle: home\ndate: 2023-09-16 16:08:44\n---\n\nHello","updated":"2023-09-16T08:08:50.129Z","path":"home/index.html","comments":1,"layout":"page","_id":"cltl4oxeh000axh7k47lwczxn","content":"<p>Hello</p>\n","site":{"data":{}},"excerpt":"","more":"<p>Hello</p>\n"},{"title":"index","date":"2023-09-16T08:36:33.000Z","layout":"index","_content":"\nTest  \nThis is the content of index.md.\n\nè¿™é‡Œæ˜¯ index.md çš„å†…å®¹ã€‚\n\n> A comment\n\n","source":"index/index.md","raw":"---\ntitle: index\ndate: 2023-09-16 16:36:33\nlayout: index\n---\n\nTest  \nThis is the content of index.md.\n\nè¿™é‡Œæ˜¯ index.md çš„å†…å®¹ã€‚\n\n> A comment\n\n","updated":"2024-01-23T11:12:03.884Z","path":"index/index.html","comments":1,"_id":"cltl4oxei000exh7k543nfa96","content":"<p>Test<br>\nThis is the content of <a href=\"http://index.md\">index.md</a>.</p>\n<p>è¿™é‡Œæ˜¯ <a href=\"http://index.md\">index.md</a> çš„å†…å®¹ã€‚</p>\n<blockquote>\n<p>A comment</p>\n</blockquote>\n","site":{"data":{}},"excerpt":"","more":"<p>Test<br>\nThis is the content of <a href=\"http://index.md\">index.md</a>.</p>\n<p>è¿™é‡Œæ˜¯ <a href=\"http://index.md\">index.md</a> çš„å†…å®¹ã€‚</p>\n<blockquote>\n<p>A comment</p>\n</blockquote>\n"},{"title":"Tags æ ‡ç­¾","date":"2023-09-15T16:00:51.000Z","type":"tags","layout":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: Tags æ ‡ç­¾\ndate: 2023-09-16 00:00:51\ntype: \"tags\"\nlayout: tags\n---\n","updated":"2024-03-10T04:48:43.903Z","path":"tags/index.html","comments":1,"_id":"cltl4oxei000gxh7k60m1ckeh","content":"","site":{"data":{}},"excerpt":"","more":""},{"date":"2023-09-23T07:24:00.000Z","type":"projects","_content":"\n# Projects\n\n## æ«å¶ Fengye\n\nA Hexo theme that is minimalistic, modern and beautiful.\n\n[Code](https://www.github.com/chen-yingfa/hexo-theme-fengye)\n\n## è½»ä¹¦ Qingshu\n\nA modern and minimalistic Markdown editor with Manaco.\n\n[Code](https://www.github.com/chen-yingfa/qingshu)\n","source":"projects.md","raw":"---\ndate: 2023-09-23 15:24:00\ntype: \"projects\"\n---\n\n# Projects\n\n## æ«å¶ Fengye\n\nA Hexo theme that is minimalistic, modern and beautiful.\n\n[Code](https://www.github.com/chen-yingfa/hexo-theme-fengye)\n\n## è½»ä¹¦ Qingshu\n\nA modern and minimalistic Markdown editor with Manaco.\n\n[Code](https://www.github.com/chen-yingfa/qingshu)\n","updated":"2023-09-23T07:28:34.813Z","path":"projects.html","title":"","comments":1,"layout":"page","_id":"cltl4oxej000lxh7k2glv22nq","content":"<h1>Projects</h1>\n<h2 id=\"æ«å¶-Fengye\">æ«å¶ Fengye</h2>\n<p>A Hexo theme that is minimalistic, modern and beautiful.</p>\n<p><a href=\"https://www.github.com/chen-yingfa/hexo-theme-fengye\">Code</a></p>\n<h2 id=\"è½»ä¹¦-Qingshu\">è½»ä¹¦ Qingshu</h2>\n<p>A modern and minimalistic Markdown editor with Manaco.</p>\n<p><a href=\"https://www.github.com/chen-yingfa/qingshu\">Code</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h1>Projects</h1>\n<h2 id=\"æ«å¶-Fengye\">æ«å¶ Fengye</h2>\n<p>A Hexo theme that is minimalistic, modern and beautiful.</p>\n<p><a href=\"https://www.github.com/chen-yingfa/hexo-theme-fengye\">Code</a></p>\n<h2 id=\"è½»ä¹¦-Qingshu\">è½»ä¹¦ Qingshu</h2>\n<p>A modern and minimalistic Markdown editor with Manaco.</p>\n<p><a href=\"https://www.github.com/chen-yingfa/qingshu\">Code</a></p>\n"},{"_content":"Test\nThis is the content of index.md.\n\nè¿™é‡Œæ˜¯ index.md çš„å†…å®¹ã€‚\n\n> A comment\n","source":"indexx.md","raw":"Test\nThis is the content of index.md.\n\nè¿™é‡Œæ˜¯ index.md çš„å†…å®¹ã€‚\n\n> A comment\n","date":"2023-09-23T07:31:36.827Z","updated":"2023-09-23T07:31:36.827Z","path":"indexx.html","title":"","comments":1,"layout":"page","_id":"cltl4oxez0097xh7khum42pby","content":"<p>Test\nThis is the content of <a href=\"http://index.md\">index.md</a>.</p>\n<p>è¿™é‡Œæ˜¯ <a href=\"http://index.md\">index.md</a> çš„å†…å®¹ã€‚</p>\n<blockquote>\n<p>A comment</p>\n</blockquote>\n","site":{"data":{}},"excerpt":"","more":"<p>Test\nThis is the content of <a href=\"http://index.md\">index.md</a>.</p>\n<p>è¿™é‡Œæ˜¯ <a href=\"http://index.md\">index.md</a> çš„å†…å®¹ã€‚</p>\n<blockquote>\n<p>A comment</p>\n</blockquote>\n"}],"Post":[{"author":"é™ˆè‹±å‘ Yingfa Chen","title":"2023å¹´ä¸­ç§‹å’Œå›½åº†","date":"2023-10-05T05:55:02.000Z","thumbnail":"æ­¦å•†æ¢¦æ—¶ä»£.png","_content":"\nä»Šå¹´å›½åº† ğŸ‡¨ğŸ‡³ å’Œä¸­ç§‹ ğŸ¥® ä¸€èµ·æ”¾å‡ï¼Œæˆ‘è·Ÿ 00 ä¸€èµ·å›æ¥åº”åŸå‚åŠ å¥¹å ‚å§å’Œåˆä¸­åŒå­¦çš„å©šç¤¼^[27å·æ˜¯åˆä¸­åŒå­¦ï¼ˆé­é™ˆï¼‰çš„å©šç¤¼ï¼Œ5å·æ˜¯å ‚å§ï¼ˆéª†å“é¢–ï¼‰çš„å©šç¤¼ã€‚]ï¼Œ\nä½åœ¨å¥¹å®¶é‡Œåä¸ªå¤œæ™š^[ä¹æœˆäºŒåå…­æ—¥å›æ¥ï¼Œåæœˆä¸ƒæ—¥èµ°ã€‚åé«˜é“åˆ°åŒ—äº¬ï¼Œç„¶ååšç«è½¦åˆ°åº”åŸã€‚]ã€‚ç¬¬äºŒæ¬¡è§å®¶é•¿ï¼Œä¹Ÿç®—æ˜¯æŒºé¡ºåˆ©ï¼Œä½†æ˜¯æ¯å¤©éƒ½ä¼šè§åˆ°é™Œç”Ÿäººï¼Œæœ‰ç‚¹ç´¯ï¼Œåº†å¹¸çš„æ˜¯ï¼Œæ„Ÿè§‰åˆ° 00 èƒ½æ¥å—è·Ÿæˆ‘å®¶äººç”Ÿæ´»åœ¨ä¸€èµ·ã€‚ä¸€å·åˆ°ä¸‰å·æˆ‘ä»¬å»æ­¦æ±‰ç©äº†ä¸‰å¤©ï¼Œè¶…çº§å¼€å¿ƒï¼Œè·Ÿå¥¹åœ¨ä¸€èµ·è¿é€›å•†åœºéƒ½æ˜¯å¼€å¿ƒçš„ï¼\n\n\n## å°å¿åŸçš„æ°›å›´\n\nåº”åŸè·Ÿæˆ‘æƒ³è±¡ä¸­çš„å°å¿åŸå¾ˆåƒï¼Œä¹Ÿæ˜¯å¾ˆå¤šè¿œæˆ¿äº²æˆšï¼Œä¹ ä¿—ä¹Ÿè®©äººå¾ˆçƒ¦ã€‚æ•¬é…’ã€éšåœ°æ‰”åƒåœ¾ã€å®¤å†…æŠ½çƒŸã€å…«å¦äººå®¶çš„ç§äº‹ã€è¯´è¯ç²—é„™ã€è„ã€è¯´äº†ä¸è¦è¿˜éè¦ç»™äººå®¶â€¦â€¦è€Œä¸”ç¡®å®èƒ½æ˜æ˜¾æ„Ÿè§‰åˆ°ï¼Œè¿™é‡Œçš„äººçš„ç´ è´¨çš„å¹³å‡æ°´å¹³æŒºä½çš„ï¼Œå°¤å…¶æ˜¯ä¸Šä¸€è¾ˆã€‚çœŸçš„å¾ˆè®¨åŒåƒå¸­ï¼Œ00 ä¹Ÿæ˜¯ï¼Œè¿™äº›ä¹ ä¿—çš„éº»çƒ¦ç¨‹åº¦è®© 00 éƒ½ä¸æƒ³ç»“å©šäº†â€¦â€¦\n\n<!-- more -->\n\nä½†æ˜¯æ— æ‰€è°“äº†ï¼Œä¹‹åèƒ½è·Ÿ 00 åœ¨ä¸€èµ·å°±å¥½ï¼Œé™¤äº†å›æ¥è¿‡èŠ‚åº”è¯¥ä¹Ÿå¾ˆå°‘æœºä¼šæœ‰è”ç³»ã€‚\n\n## æ­¦æ±‰\n\nä¸€å·åˆ°ä¸‰å·å»äº†æ­¦æ±‰æ—…æ¸¸ã€‚æ—©ä¸Šäº”ç‚¹å¤šè·Ÿ 00 çš„ â€äºŒå¦ˆâ€œï¼ˆå…¶å®æ˜¯å©¶ï¼Œå”å”çš„è€å©†ï¼‰åè½¦å»æ­¦æ±‰ï¼Œåäº†ä¸€ä¸ªå°æ—¶ã€‚ä»–ä»¬è¿™ä¹ˆæ—©æ˜¯å› ä¸ºè¦å»è°ˆå©šç¤¼çš„äº‹æƒ…ï¼Œç„¶åå®³æ€•å µè½¦ã€‚æˆ‘ä»¬åœ¨é…’åº—æ—è¾¹ä¸‹æ¥ï¼Œé‚£æ—¶å€™â€œäºŒå¦ˆâ€ä¸‹åœ°é“ç«™ä¸Šå•æ‰€ï¼Œç„¶å 00 éè¦ç»™å¥¹ä¹°åŒ…å­ï¼ˆä¸ºäº†ç¤¼è²Œï¼‰ï¼Œç„¶åå¥¹æœ€åè¿˜æ˜¯æ‹’ç»äº†ï¼Œå¯¼è‡´æˆ‘ä»¬å¾—è‡ªå·±åƒä¸‹åŒ…å­ã€‚è™½ç„¶åŒ…å­æ²¡æœ‰ä¸å¥½åƒï¼Œä½†æ˜¯æˆ‘å°±å¾ˆè®¨åŒè¿™ç§æ˜çŸ¥äººå®¶ä¸è¦è¿˜éè¦ä¹°çš„è¡Œä¸ºã€‚\n\nä¹‹åæˆ‘ä»¬å»é…’åº—çš„æ—¶å€™ï¼Œè¿˜æ²¡æœ‰æˆ¿å­ï¼Œæˆ‘ä»¬å¯„å­˜äº†è¡Œæå°±ç›´æ¥å»æ–°å¤©åœ°ä¹°äº†æ¯éœ¸ç‹èŒ¶å§¬çš„å¥¶èŒ¶ï¼Œç„¶åå»äº†å¤å¾·å¯ºã€‚ç½‘ä¸Šè¯´ä¸å¯ä»¥ç©¿ç€æš´éœ²ï¼Œä½†æ˜¯æ„Ÿè§‰è·¯äººç©¿ç€è¿˜æ˜¯å¾ˆæš´éœ²ã€‚\n\n![åœ¨æ­¦æ±‰æ–°å¤©åœ°ä¹°éœ¸ç‹èŒ¶å§¬ã€‚](./2023ä¸­ç§‹/æ–°å¤©åœ°-éœ¸ç‹èŒ¶å§¬.png \"åœ¨æ­¦æ±‰æ–°å¤©åœ°ä¹°éœ¸ç‹èŒ¶å§¬ã€‚\")\n\nä¹‹åè¿˜å»äº†è§£æ”¾å…¬å›­å’Œä¸­å±±å…¬å›­ï¼Œéƒ½æŒºä¸é”™çš„ã€‚å¤§åŸå¸‚å°±æ˜¯å¥½ã€‚é‡Œé¢çœ‹åˆ°äº†å¾ˆå¥½çœ‹çš„å»ºç­‘ç‰©ã€‚åœ¨ä¸­å±±å…¬å›­æˆ‘ä»¬é—®äº†ä¸¤ä¸ªå°å­©å€Ÿç”¨ç¾½æ¯›çƒæ‹å­æ¥æ‰“äº†å‡ ä¸‹ã€‚ä¹‹ååœ¨ä¸€ä¸ªç›¸äº²è§’^[ä¹‹å‰åœ¨ä¸Šæµ·éƒ½æ²¡æ‰¾åˆ°ã€‚]æ—è¾¹è·Ÿå¥¹çš„é«˜ä¸­åŒå­¦ï¼Œå½­åŒï¼Œä¼šåˆï¼Œç„¶åé€›äº†ä¸€ä¸‹ç›¸äº²è§’ã€‚ä¹‹åæˆ‘ä»¬è¿˜åäº†ä¸€ä¸‹è¿‡å±±è½¦ï¼ˆå…¬å›­é‡Œé¢æœ‰è¿‡å±±è½¦è¿˜æ˜¯ç¬¬ä¸€æ¬¡è§ï¼‰ã€‚\n\n![è§£æ”¾å…¬å›­ä¸­é—´çš„ä¸€ä¸ªå¾ˆå¤šå¡”çš„åœ°æ–¹ã€‚](./2023ä¸­ç§‹/è§£æ”¾å…¬å›­ä¸­é—´.png \"è§£æ”¾å…¬å›­ä¸­é—´çš„ä¸€ä¸ªå¾ˆå¤šå¡”çš„åœ°æ–¹ã€‚\")\n\næ™šä¸Šå°±å»è·Ÿå¥¹çš„é«˜ä¸­åŒå­¦ä¸€èµ·åƒé¥­ã€‚\n\nç¬¬äºŒå¤©æˆ‘ä»¬å…ˆåœ¨åœ°é“ç«™å‰ªäº†å¤´å‘ï¼Œç„¶åå»å®é€šå¯ºï¼Œæ™šä¸Šå»æ­¦å•†æ¢¦æ—¶ä»£ã€‚è¿™ä¸ªå•†åœºè§„æ ¼è¶…çº§é«˜ï¼Œè¿˜æŒºå¥½ç©çš„ã€‚ç¬¬ä¸€æ¬¡çœ‹åˆ°ç´¢å°¼ä¸“å–åº—ï¼Œè¿˜æœ‰ Pico ä¸“å–åº—ã€‚é‡Œé¢è¿˜æœ‰æ»‘é›ªçš„åœ°æ–¹ï¼Œä½†æ˜¯å¤ªè´µçš„ã€‚æˆ‘ä»¬è¿˜å»äº†ä¼˜è¡£åº“ï¼Œä¹°äº†ä¸€äº›è¡£æœï¼Œå‘ç°è¿˜æŒºä¾¿å®œçš„ã€‚ä»¥å‰éƒ½ä¼šè§‰å¾—é€›è¡—è´­ç‰©å¾ˆæ— èŠï¼Œä½†æ˜¯è·Ÿå¥¹åœ¨ä¸€èµ·è¿è¿é€›è¡—ä¹°è¡£æœéƒ½æ˜¯å¼€å¿ƒçš„ã€‚\n\næ™šä¸Šæˆ‘ä»¬è·Ÿå¥¹â€œå¤§å“¥â€ï¼ˆå…¶å®æ˜¯å ‚å“¥ï¼‰å’Œä»–è€å©†ä¸€èµ·åƒé¥­ï¼Œåƒäº†é­”å®—çƒ¤è‚‰ï¼Œç„¶åå–äº†èŒ¶é¢œæ‚¦è‰²ã€‚æ€»ä½“æ¥è¯´ä¹ŸæŒºé¡ºåˆ©çš„ï¼Œæ„Ÿè§‰ä»–ä»¬ä¹Ÿä¸éš¾ç›¸å¤„ã€‚\n\n![æ­¦å•†æ¢¦æ—¶ä»£é‡Œé¢çš„ç¾é£Ÿè¡—ä¹°é²œè™¾æ±¤åŒ…](./2023ä¸­ç§‹/æ­¦å•†æ¢¦æ—¶ä»£.png \"æ­¦å•†æ¢¦æ—¶ä»£é‡Œé¢çš„ç¾é£Ÿè¡—ä¹°é²œè™¾æ±¤åŒ…\")\n\nç¬¬ä¸‰å¤©æˆ‘ä»¬å»äº†æ¬¢ä¹è°·ï¼æ˜¯æˆ‘ä»¬ç¬¬ä¸€æ¬¡ä¸€èµ·å»æ¸¸ä¹åœºï¼ç©äº†ä¸€ä¸ªè¿‡å±±è½¦ï¼Œç„¶ååšäº†å¤ªé˜³é£è½¦ï¼Œ00 å°±å¤´æ™•æƒ³åäº†ï¼Œæœç„¶è¿˜æ˜¯ä¸è¡Œâ€¦â€¦ä½†æ˜¯æ²¡äº‹ï¼Œè¿˜æ˜¯æŒºå¼€å¿ƒçš„ã€‚æ’é˜Ÿè¿‡ç¨‹ä¸­è¿˜é‡åˆ°äº†æ’é˜Ÿçš„äººï¼Œå¥½æ¶å¿ƒï¼\n\næ™šä¸Šæˆ‘ä»¬è·Ÿä¸€äº›äººï¼ˆå…±ä¸ƒä¸ªäººï¼‰ä¸€èµ·æ‹¼è½¦å›æ¥åº”åŸï¼Œå±…ç„¶æ¯”ç«è½¦è¿˜ä¾¿å®œï¼Œä¸é”™ã€‚å›æ¥å·²ç»11ç‚¹äº†ï¼Œç„¶åå›å®¶æ”¾ä¸‹è¡Œæç®±ä¹‹ååˆå‡ºå»æ‰¾å¥¹åˆä¸­åŒå­¦ä¸€èµ·åƒå®µå¤œã€‚\n\n![åœ¨æ­¦æ±‰æ¬¢ä¹è°·ç©è€ã€‚](./2023ä¸­ç§‹/æ­¦æ±‰æ¬¢ä¹è°·.png \"åœ¨æ­¦æ±‰æ¬¢ä¹è°·ç©è€ã€‚\")\n\n\n## å…¬äº‹\n\nè¿™ä¸ªå‡æœŸæœ‰ç‚¹é•¿ï¼Œæ„Ÿè§‰æœ‰å¾ˆå¤šæ´»éƒ½æ²¡æœ‰å¹²ã€‚æ¯å¤©éƒ½å¾ˆå¤šäº‹æƒ…ï¼Œæ„Ÿè§‰è¿™é‡Œçš„äººå¤ªé—²äº†ï¼Œåº”è¯¥è®©ä»–ä»¬å¤šä¸Šç­å“ˆå“ˆå“ˆã€‚å¤æ–‡å­—ç¿»è¯‘çš„å·¥ä½œè¿˜æ²¡æœ‰å¹²å®Œï¼Œç›®å‰æ„Ÿè§‰æ•ˆæœä¸æ˜¯å¾ˆå¥½ï¼Œæˆ‘ä¹Ÿä¸æƒ³å¹²è¿™ä¸ªäº†ï¼Œæ„Ÿè§‰å¾ˆæµªè´¹æˆ‘çš„æ—¶é—´â€¦â€¦è‡³äºå¯¹é½ç¥ç»å…ƒï¼Œè²Œä¼¼ç°æœ‰æ–¹æ³•éƒ½æ— æ³•ç”¨åœ¨è‡ªå›å½’æ¨¡å‹ä¸Šé¢ï¼Œä½†æ˜¯å¯¹é½é—®é¢˜å¥½åƒä¹‹åè‡ªå›å½’æ¨¡å‹æ‰ä¼šå‡ºç°ã€‚ä¸çŸ¥é“æ˜¯ä¸æ˜¯æˆ‘æ²¡æœ‰æ‰¾åˆ°ï¼Œç›®å‰è¿˜æ²¡æœ‰æ‰¾åˆ°ä¸€ç¯‡ç ”ç©¶ç¥ç»å…ƒå¯¹ç”Ÿæˆç»“æœçš„å½±å“çš„å·¥ä½œã€‚[ROME](https://www.github.com/kmeng01/rome) çš„ Causal Tracing æ„Ÿè§‰å¯ä»¥ç”¨ï¼Œè¿™ä¸¤å¤©å¾—èµ¶ç´§åšç‚¹ä¸œè¥¿å‡ºæ¥ã€‚\n","source":"_posts/2023ä¸­ç§‹.md","raw":"---\nauthor: é™ˆè‹±å‘ Yingfa Chen\ntitle: 2023å¹´ä¸­ç§‹å’Œå›½åº†\ndate: 2023-10-05 13:55:02\ncategories: Life\nthumbnail: æ­¦å•†æ¢¦æ—¶ä»£.png\ntags:\n- life\n- ä¸­ç§‹\n- ä¸­æ–‡\n- '00'\n- wedding\n- ä¸­ç§‹-middle-autumn\n- å›½åº†-national-day\n- åº”åŸ\n- æ­¦æ±‰-wuhan\n---\n\nä»Šå¹´å›½åº† ğŸ‡¨ğŸ‡³ å’Œä¸­ç§‹ ğŸ¥® ä¸€èµ·æ”¾å‡ï¼Œæˆ‘è·Ÿ 00 ä¸€èµ·å›æ¥åº”åŸå‚åŠ å¥¹å ‚å§å’Œåˆä¸­åŒå­¦çš„å©šç¤¼^[27å·æ˜¯åˆä¸­åŒå­¦ï¼ˆé­é™ˆï¼‰çš„å©šç¤¼ï¼Œ5å·æ˜¯å ‚å§ï¼ˆéª†å“é¢–ï¼‰çš„å©šç¤¼ã€‚]ï¼Œ\nä½åœ¨å¥¹å®¶é‡Œåä¸ªå¤œæ™š^[ä¹æœˆäºŒåå…­æ—¥å›æ¥ï¼Œåæœˆä¸ƒæ—¥èµ°ã€‚åé«˜é“åˆ°åŒ—äº¬ï¼Œç„¶ååšç«è½¦åˆ°åº”åŸã€‚]ã€‚ç¬¬äºŒæ¬¡è§å®¶é•¿ï¼Œä¹Ÿç®—æ˜¯æŒºé¡ºåˆ©ï¼Œä½†æ˜¯æ¯å¤©éƒ½ä¼šè§åˆ°é™Œç”Ÿäººï¼Œæœ‰ç‚¹ç´¯ï¼Œåº†å¹¸çš„æ˜¯ï¼Œæ„Ÿè§‰åˆ° 00 èƒ½æ¥å—è·Ÿæˆ‘å®¶äººç”Ÿæ´»åœ¨ä¸€èµ·ã€‚ä¸€å·åˆ°ä¸‰å·æˆ‘ä»¬å»æ­¦æ±‰ç©äº†ä¸‰å¤©ï¼Œè¶…çº§å¼€å¿ƒï¼Œè·Ÿå¥¹åœ¨ä¸€èµ·è¿é€›å•†åœºéƒ½æ˜¯å¼€å¿ƒçš„ï¼\n\n\n## å°å¿åŸçš„æ°›å›´\n\nåº”åŸè·Ÿæˆ‘æƒ³è±¡ä¸­çš„å°å¿åŸå¾ˆåƒï¼Œä¹Ÿæ˜¯å¾ˆå¤šè¿œæˆ¿äº²æˆšï¼Œä¹ ä¿—ä¹Ÿè®©äººå¾ˆçƒ¦ã€‚æ•¬é…’ã€éšåœ°æ‰”åƒåœ¾ã€å®¤å†…æŠ½çƒŸã€å…«å¦äººå®¶çš„ç§äº‹ã€è¯´è¯ç²—é„™ã€è„ã€è¯´äº†ä¸è¦è¿˜éè¦ç»™äººå®¶â€¦â€¦è€Œä¸”ç¡®å®èƒ½æ˜æ˜¾æ„Ÿè§‰åˆ°ï¼Œè¿™é‡Œçš„äººçš„ç´ è´¨çš„å¹³å‡æ°´å¹³æŒºä½çš„ï¼Œå°¤å…¶æ˜¯ä¸Šä¸€è¾ˆã€‚çœŸçš„å¾ˆè®¨åŒåƒå¸­ï¼Œ00 ä¹Ÿæ˜¯ï¼Œè¿™äº›ä¹ ä¿—çš„éº»çƒ¦ç¨‹åº¦è®© 00 éƒ½ä¸æƒ³ç»“å©šäº†â€¦â€¦\n\n<!-- more -->\n\nä½†æ˜¯æ— æ‰€è°“äº†ï¼Œä¹‹åèƒ½è·Ÿ 00 åœ¨ä¸€èµ·å°±å¥½ï¼Œé™¤äº†å›æ¥è¿‡èŠ‚åº”è¯¥ä¹Ÿå¾ˆå°‘æœºä¼šæœ‰è”ç³»ã€‚\n\n## æ­¦æ±‰\n\nä¸€å·åˆ°ä¸‰å·å»äº†æ­¦æ±‰æ—…æ¸¸ã€‚æ—©ä¸Šäº”ç‚¹å¤šè·Ÿ 00 çš„ â€äºŒå¦ˆâ€œï¼ˆå…¶å®æ˜¯å©¶ï¼Œå”å”çš„è€å©†ï¼‰åè½¦å»æ­¦æ±‰ï¼Œåäº†ä¸€ä¸ªå°æ—¶ã€‚ä»–ä»¬è¿™ä¹ˆæ—©æ˜¯å› ä¸ºè¦å»è°ˆå©šç¤¼çš„äº‹æƒ…ï¼Œç„¶åå®³æ€•å µè½¦ã€‚æˆ‘ä»¬åœ¨é…’åº—æ—è¾¹ä¸‹æ¥ï¼Œé‚£æ—¶å€™â€œäºŒå¦ˆâ€ä¸‹åœ°é“ç«™ä¸Šå•æ‰€ï¼Œç„¶å 00 éè¦ç»™å¥¹ä¹°åŒ…å­ï¼ˆä¸ºäº†ç¤¼è²Œï¼‰ï¼Œç„¶åå¥¹æœ€åè¿˜æ˜¯æ‹’ç»äº†ï¼Œå¯¼è‡´æˆ‘ä»¬å¾—è‡ªå·±åƒä¸‹åŒ…å­ã€‚è™½ç„¶åŒ…å­æ²¡æœ‰ä¸å¥½åƒï¼Œä½†æ˜¯æˆ‘å°±å¾ˆè®¨åŒè¿™ç§æ˜çŸ¥äººå®¶ä¸è¦è¿˜éè¦ä¹°çš„è¡Œä¸ºã€‚\n\nä¹‹åæˆ‘ä»¬å»é…’åº—çš„æ—¶å€™ï¼Œè¿˜æ²¡æœ‰æˆ¿å­ï¼Œæˆ‘ä»¬å¯„å­˜äº†è¡Œæå°±ç›´æ¥å»æ–°å¤©åœ°ä¹°äº†æ¯éœ¸ç‹èŒ¶å§¬çš„å¥¶èŒ¶ï¼Œç„¶åå»äº†å¤å¾·å¯ºã€‚ç½‘ä¸Šè¯´ä¸å¯ä»¥ç©¿ç€æš´éœ²ï¼Œä½†æ˜¯æ„Ÿè§‰è·¯äººç©¿ç€è¿˜æ˜¯å¾ˆæš´éœ²ã€‚\n\n![åœ¨æ­¦æ±‰æ–°å¤©åœ°ä¹°éœ¸ç‹èŒ¶å§¬ã€‚](./2023ä¸­ç§‹/æ–°å¤©åœ°-éœ¸ç‹èŒ¶å§¬.png \"åœ¨æ­¦æ±‰æ–°å¤©åœ°ä¹°éœ¸ç‹èŒ¶å§¬ã€‚\")\n\nä¹‹åè¿˜å»äº†è§£æ”¾å…¬å›­å’Œä¸­å±±å…¬å›­ï¼Œéƒ½æŒºä¸é”™çš„ã€‚å¤§åŸå¸‚å°±æ˜¯å¥½ã€‚é‡Œé¢çœ‹åˆ°äº†å¾ˆå¥½çœ‹çš„å»ºç­‘ç‰©ã€‚åœ¨ä¸­å±±å…¬å›­æˆ‘ä»¬é—®äº†ä¸¤ä¸ªå°å­©å€Ÿç”¨ç¾½æ¯›çƒæ‹å­æ¥æ‰“äº†å‡ ä¸‹ã€‚ä¹‹ååœ¨ä¸€ä¸ªç›¸äº²è§’^[ä¹‹å‰åœ¨ä¸Šæµ·éƒ½æ²¡æ‰¾åˆ°ã€‚]æ—è¾¹è·Ÿå¥¹çš„é«˜ä¸­åŒå­¦ï¼Œå½­åŒï¼Œä¼šåˆï¼Œç„¶åé€›äº†ä¸€ä¸‹ç›¸äº²è§’ã€‚ä¹‹åæˆ‘ä»¬è¿˜åäº†ä¸€ä¸‹è¿‡å±±è½¦ï¼ˆå…¬å›­é‡Œé¢æœ‰è¿‡å±±è½¦è¿˜æ˜¯ç¬¬ä¸€æ¬¡è§ï¼‰ã€‚\n\n![è§£æ”¾å…¬å›­ä¸­é—´çš„ä¸€ä¸ªå¾ˆå¤šå¡”çš„åœ°æ–¹ã€‚](./2023ä¸­ç§‹/è§£æ”¾å…¬å›­ä¸­é—´.png \"è§£æ”¾å…¬å›­ä¸­é—´çš„ä¸€ä¸ªå¾ˆå¤šå¡”çš„åœ°æ–¹ã€‚\")\n\næ™šä¸Šå°±å»è·Ÿå¥¹çš„é«˜ä¸­åŒå­¦ä¸€èµ·åƒé¥­ã€‚\n\nç¬¬äºŒå¤©æˆ‘ä»¬å…ˆåœ¨åœ°é“ç«™å‰ªäº†å¤´å‘ï¼Œç„¶åå»å®é€šå¯ºï¼Œæ™šä¸Šå»æ­¦å•†æ¢¦æ—¶ä»£ã€‚è¿™ä¸ªå•†åœºè§„æ ¼è¶…çº§é«˜ï¼Œè¿˜æŒºå¥½ç©çš„ã€‚ç¬¬ä¸€æ¬¡çœ‹åˆ°ç´¢å°¼ä¸“å–åº—ï¼Œè¿˜æœ‰ Pico ä¸“å–åº—ã€‚é‡Œé¢è¿˜æœ‰æ»‘é›ªçš„åœ°æ–¹ï¼Œä½†æ˜¯å¤ªè´µçš„ã€‚æˆ‘ä»¬è¿˜å»äº†ä¼˜è¡£åº“ï¼Œä¹°äº†ä¸€äº›è¡£æœï¼Œå‘ç°è¿˜æŒºä¾¿å®œçš„ã€‚ä»¥å‰éƒ½ä¼šè§‰å¾—é€›è¡—è´­ç‰©å¾ˆæ— èŠï¼Œä½†æ˜¯è·Ÿå¥¹åœ¨ä¸€èµ·è¿è¿é€›è¡—ä¹°è¡£æœéƒ½æ˜¯å¼€å¿ƒçš„ã€‚\n\næ™šä¸Šæˆ‘ä»¬è·Ÿå¥¹â€œå¤§å“¥â€ï¼ˆå…¶å®æ˜¯å ‚å“¥ï¼‰å’Œä»–è€å©†ä¸€èµ·åƒé¥­ï¼Œåƒäº†é­”å®—çƒ¤è‚‰ï¼Œç„¶åå–äº†èŒ¶é¢œæ‚¦è‰²ã€‚æ€»ä½“æ¥è¯´ä¹ŸæŒºé¡ºåˆ©çš„ï¼Œæ„Ÿè§‰ä»–ä»¬ä¹Ÿä¸éš¾ç›¸å¤„ã€‚\n\n![æ­¦å•†æ¢¦æ—¶ä»£é‡Œé¢çš„ç¾é£Ÿè¡—ä¹°é²œè™¾æ±¤åŒ…](./2023ä¸­ç§‹/æ­¦å•†æ¢¦æ—¶ä»£.png \"æ­¦å•†æ¢¦æ—¶ä»£é‡Œé¢çš„ç¾é£Ÿè¡—ä¹°é²œè™¾æ±¤åŒ…\")\n\nç¬¬ä¸‰å¤©æˆ‘ä»¬å»äº†æ¬¢ä¹è°·ï¼æ˜¯æˆ‘ä»¬ç¬¬ä¸€æ¬¡ä¸€èµ·å»æ¸¸ä¹åœºï¼ç©äº†ä¸€ä¸ªè¿‡å±±è½¦ï¼Œç„¶ååšäº†å¤ªé˜³é£è½¦ï¼Œ00 å°±å¤´æ™•æƒ³åäº†ï¼Œæœç„¶è¿˜æ˜¯ä¸è¡Œâ€¦â€¦ä½†æ˜¯æ²¡äº‹ï¼Œè¿˜æ˜¯æŒºå¼€å¿ƒçš„ã€‚æ’é˜Ÿè¿‡ç¨‹ä¸­è¿˜é‡åˆ°äº†æ’é˜Ÿçš„äººï¼Œå¥½æ¶å¿ƒï¼\n\næ™šä¸Šæˆ‘ä»¬è·Ÿä¸€äº›äººï¼ˆå…±ä¸ƒä¸ªäººï¼‰ä¸€èµ·æ‹¼è½¦å›æ¥åº”åŸï¼Œå±…ç„¶æ¯”ç«è½¦è¿˜ä¾¿å®œï¼Œä¸é”™ã€‚å›æ¥å·²ç»11ç‚¹äº†ï¼Œç„¶åå›å®¶æ”¾ä¸‹è¡Œæç®±ä¹‹ååˆå‡ºå»æ‰¾å¥¹åˆä¸­åŒå­¦ä¸€èµ·åƒå®µå¤œã€‚\n\n![åœ¨æ­¦æ±‰æ¬¢ä¹è°·ç©è€ã€‚](./2023ä¸­ç§‹/æ­¦æ±‰æ¬¢ä¹è°·.png \"åœ¨æ­¦æ±‰æ¬¢ä¹è°·ç©è€ã€‚\")\n\n\n## å…¬äº‹\n\nè¿™ä¸ªå‡æœŸæœ‰ç‚¹é•¿ï¼Œæ„Ÿè§‰æœ‰å¾ˆå¤šæ´»éƒ½æ²¡æœ‰å¹²ã€‚æ¯å¤©éƒ½å¾ˆå¤šäº‹æƒ…ï¼Œæ„Ÿè§‰è¿™é‡Œçš„äººå¤ªé—²äº†ï¼Œåº”è¯¥è®©ä»–ä»¬å¤šä¸Šç­å“ˆå“ˆå“ˆã€‚å¤æ–‡å­—ç¿»è¯‘çš„å·¥ä½œè¿˜æ²¡æœ‰å¹²å®Œï¼Œç›®å‰æ„Ÿè§‰æ•ˆæœä¸æ˜¯å¾ˆå¥½ï¼Œæˆ‘ä¹Ÿä¸æƒ³å¹²è¿™ä¸ªäº†ï¼Œæ„Ÿè§‰å¾ˆæµªè´¹æˆ‘çš„æ—¶é—´â€¦â€¦è‡³äºå¯¹é½ç¥ç»å…ƒï¼Œè²Œä¼¼ç°æœ‰æ–¹æ³•éƒ½æ— æ³•ç”¨åœ¨è‡ªå›å½’æ¨¡å‹ä¸Šé¢ï¼Œä½†æ˜¯å¯¹é½é—®é¢˜å¥½åƒä¹‹åè‡ªå›å½’æ¨¡å‹æ‰ä¼šå‡ºç°ã€‚ä¸çŸ¥é“æ˜¯ä¸æ˜¯æˆ‘æ²¡æœ‰æ‰¾åˆ°ï¼Œç›®å‰è¿˜æ²¡æœ‰æ‰¾åˆ°ä¸€ç¯‡ç ”ç©¶ç¥ç»å…ƒå¯¹ç”Ÿæˆç»“æœçš„å½±å“çš„å·¥ä½œã€‚[ROME](https://www.github.com/kmeng01/rome) çš„ Causal Tracing æ„Ÿè§‰å¯ä»¥ç”¨ï¼Œè¿™ä¸¤å¤©å¾—èµ¶ç´§åšç‚¹ä¸œè¥¿å‡ºæ¥ã€‚\n","slug":"2023ä¸­ç§‹","published":1,"updated":"2024-01-11T05:44:33.394Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cltl4oxed0001xh7k0ksv1gpa","content":"<p>ä»Šå¹´å›½åº† ğŸ‡¨ğŸ‡³ å’Œä¸­ç§‹ ğŸ¥® ä¸€èµ·æ”¾å‡ï¼Œæˆ‘è·Ÿ 00 ä¸€èµ·å›æ¥åº”åŸå‚åŠ å¥¹å ‚å§å’Œåˆä¸­åŒå­¦çš„å©šç¤¼<sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">[1]</a></sup>ï¼Œ\nä½åœ¨å¥¹å®¶é‡Œåä¸ªå¤œæ™š<sup class=\"footnote-ref\"><a href=\"#fn2\" id=\"fnref2\">[2]</a></sup>ã€‚ç¬¬äºŒæ¬¡è§å®¶é•¿ï¼Œä¹Ÿç®—æ˜¯æŒºé¡ºåˆ©ï¼Œä½†æ˜¯æ¯å¤©éƒ½ä¼šè§åˆ°é™Œç”Ÿäººï¼Œæœ‰ç‚¹ç´¯ï¼Œåº†å¹¸çš„æ˜¯ï¼Œæ„Ÿè§‰åˆ° 00 èƒ½æ¥å—è·Ÿæˆ‘å®¶äººç”Ÿæ´»åœ¨ä¸€èµ·ã€‚ä¸€å·åˆ°ä¸‰å·æˆ‘ä»¬å»æ­¦æ±‰ç©äº†ä¸‰å¤©ï¼Œè¶…çº§å¼€å¿ƒï¼Œè·Ÿå¥¹åœ¨ä¸€èµ·è¿é€›å•†åœºéƒ½æ˜¯å¼€å¿ƒçš„ï¼</p>\n<h2 id=\"å°å¿åŸçš„æ°›å›´\">å°å¿åŸçš„æ°›å›´</h2>\n<p>åº”åŸè·Ÿæˆ‘æƒ³è±¡ä¸­çš„å°å¿åŸå¾ˆåƒï¼Œä¹Ÿæ˜¯å¾ˆå¤šè¿œæˆ¿äº²æˆšï¼Œä¹ ä¿—ä¹Ÿè®©äººå¾ˆçƒ¦ã€‚æ•¬é…’ã€éšåœ°æ‰”åƒåœ¾ã€å®¤å†…æŠ½çƒŸã€å…«å¦äººå®¶çš„ç§äº‹ã€è¯´è¯ç²—é„™ã€è„ã€è¯´äº†ä¸è¦è¿˜éè¦ç»™äººå®¶â€¦â€¦è€Œä¸”ç¡®å®èƒ½æ˜æ˜¾æ„Ÿè§‰åˆ°ï¼Œè¿™é‡Œçš„äººçš„ç´ è´¨çš„å¹³å‡æ°´å¹³æŒºä½çš„ï¼Œå°¤å…¶æ˜¯ä¸Šä¸€è¾ˆã€‚çœŸçš„å¾ˆè®¨åŒåƒå¸­ï¼Œ00 ä¹Ÿæ˜¯ï¼Œè¿™äº›ä¹ ä¿—çš„éº»çƒ¦ç¨‹åº¦è®© 00 éƒ½ä¸æƒ³ç»“å©šäº†â€¦â€¦</p>\n<span id=\"more\"></span>\n<p>ä½†æ˜¯æ— æ‰€è°“äº†ï¼Œä¹‹åèƒ½è·Ÿ 00 åœ¨ä¸€èµ·å°±å¥½ï¼Œé™¤äº†å›æ¥è¿‡èŠ‚åº”è¯¥ä¹Ÿå¾ˆå°‘æœºä¼šæœ‰è”ç³»ã€‚</p>\n<h2 id=\"æ­¦æ±‰\">æ­¦æ±‰</h2>\n<p>ä¸€å·åˆ°ä¸‰å·å»äº†æ­¦æ±‰æ—…æ¸¸ã€‚æ—©ä¸Šäº”ç‚¹å¤šè·Ÿ 00 çš„ â€äºŒå¦ˆâ€œï¼ˆå…¶å®æ˜¯å©¶ï¼Œå”å”çš„è€å©†ï¼‰åè½¦å»æ­¦æ±‰ï¼Œåäº†ä¸€ä¸ªå°æ—¶ã€‚ä»–ä»¬è¿™ä¹ˆæ—©æ˜¯å› ä¸ºè¦å»è°ˆå©šç¤¼çš„äº‹æƒ…ï¼Œç„¶åå®³æ€•å µè½¦ã€‚æˆ‘ä»¬åœ¨é…’åº—æ—è¾¹ä¸‹æ¥ï¼Œé‚£æ—¶å€™â€œäºŒå¦ˆâ€ä¸‹åœ°é“ç«™ä¸Šå•æ‰€ï¼Œç„¶å 00 éè¦ç»™å¥¹ä¹°åŒ…å­ï¼ˆä¸ºäº†ç¤¼è²Œï¼‰ï¼Œç„¶åå¥¹æœ€åè¿˜æ˜¯æ‹’ç»äº†ï¼Œå¯¼è‡´æˆ‘ä»¬å¾—è‡ªå·±åƒä¸‹åŒ…å­ã€‚è™½ç„¶åŒ…å­æ²¡æœ‰ä¸å¥½åƒï¼Œä½†æ˜¯æˆ‘å°±å¾ˆè®¨åŒè¿™ç§æ˜çŸ¥äººå®¶ä¸è¦è¿˜éè¦ä¹°çš„è¡Œä¸ºã€‚</p>\n<p>ä¹‹åæˆ‘ä»¬å»é…’åº—çš„æ—¶å€™ï¼Œè¿˜æ²¡æœ‰æˆ¿å­ï¼Œæˆ‘ä»¬å¯„å­˜äº†è¡Œæå°±ç›´æ¥å»æ–°å¤©åœ°ä¹°äº†æ¯éœ¸ç‹èŒ¶å§¬çš„å¥¶èŒ¶ï¼Œç„¶åå»äº†å¤å¾·å¯ºã€‚ç½‘ä¸Šè¯´ä¸å¯ä»¥ç©¿ç€æš´éœ²ï¼Œä½†æ˜¯æ„Ÿè§‰è·¯äººç©¿ç€è¿˜æ˜¯å¾ˆæš´éœ²ã€‚</p>\n<p><img src=\"/2023/10/05/2023%E4%B8%AD%E7%A7%8B/%E6%96%B0%E5%A4%A9%E5%9C%B0-%E9%9C%B8%E7%8E%8B%E8%8C%B6%E5%A7%AC.png\" alt=\"åœ¨æ­¦æ±‰æ–°å¤©åœ°ä¹°éœ¸ç‹èŒ¶å§¬ã€‚\" title=\"åœ¨æ­¦æ±‰æ–°å¤©åœ°ä¹°éœ¸ç‹èŒ¶å§¬ã€‚\"></p>\n<p>ä¹‹åè¿˜å»äº†è§£æ”¾å…¬å›­å’Œä¸­å±±å…¬å›­ï¼Œéƒ½æŒºä¸é”™çš„ã€‚å¤§åŸå¸‚å°±æ˜¯å¥½ã€‚é‡Œé¢çœ‹åˆ°äº†å¾ˆå¥½çœ‹çš„å»ºç­‘ç‰©ã€‚åœ¨ä¸­å±±å…¬å›­æˆ‘ä»¬é—®äº†ä¸¤ä¸ªå°å­©å€Ÿç”¨ç¾½æ¯›çƒæ‹å­æ¥æ‰“äº†å‡ ä¸‹ã€‚ä¹‹ååœ¨ä¸€ä¸ªç›¸äº²è§’<sup class=\"footnote-ref\"><a href=\"#fn3\" id=\"fnref3\">[3]</a></sup>æ—è¾¹è·Ÿå¥¹çš„é«˜ä¸­åŒå­¦ï¼Œå½­åŒï¼Œä¼šåˆï¼Œç„¶åé€›äº†ä¸€ä¸‹ç›¸äº²è§’ã€‚ä¹‹åæˆ‘ä»¬è¿˜åäº†ä¸€ä¸‹è¿‡å±±è½¦ï¼ˆå…¬å›­é‡Œé¢æœ‰è¿‡å±±è½¦è¿˜æ˜¯ç¬¬ä¸€æ¬¡è§ï¼‰ã€‚</p>\n<p><img src=\"/2023/10/05/2023%E4%B8%AD%E7%A7%8B/%E8%A7%A3%E6%94%BE%E5%85%AC%E5%9B%AD%E4%B8%AD%E9%97%B4.png\" alt=\"è§£æ”¾å…¬å›­ä¸­é—´çš„ä¸€ä¸ªå¾ˆå¤šå¡”çš„åœ°æ–¹ã€‚\" title=\"è§£æ”¾å…¬å›­ä¸­é—´çš„ä¸€ä¸ªå¾ˆå¤šå¡”çš„åœ°æ–¹ã€‚\"></p>\n<p>æ™šä¸Šå°±å»è·Ÿå¥¹çš„é«˜ä¸­åŒå­¦ä¸€èµ·åƒé¥­ã€‚</p>\n<p>ç¬¬äºŒå¤©æˆ‘ä»¬å…ˆåœ¨åœ°é“ç«™å‰ªäº†å¤´å‘ï¼Œç„¶åå»å®é€šå¯ºï¼Œæ™šä¸Šå»æ­¦å•†æ¢¦æ—¶ä»£ã€‚è¿™ä¸ªå•†åœºè§„æ ¼è¶…çº§é«˜ï¼Œè¿˜æŒºå¥½ç©çš„ã€‚ç¬¬ä¸€æ¬¡çœ‹åˆ°ç´¢å°¼ä¸“å–åº—ï¼Œè¿˜æœ‰ Pico ä¸“å–åº—ã€‚é‡Œé¢è¿˜æœ‰æ»‘é›ªçš„åœ°æ–¹ï¼Œä½†æ˜¯å¤ªè´µçš„ã€‚æˆ‘ä»¬è¿˜å»äº†ä¼˜è¡£åº“ï¼Œä¹°äº†ä¸€äº›è¡£æœï¼Œå‘ç°è¿˜æŒºä¾¿å®œçš„ã€‚ä»¥å‰éƒ½ä¼šè§‰å¾—é€›è¡—è´­ç‰©å¾ˆæ— èŠï¼Œä½†æ˜¯è·Ÿå¥¹åœ¨ä¸€èµ·è¿è¿é€›è¡—ä¹°è¡£æœéƒ½æ˜¯å¼€å¿ƒçš„ã€‚</p>\n<p>æ™šä¸Šæˆ‘ä»¬è·Ÿå¥¹â€œå¤§å“¥â€ï¼ˆå…¶å®æ˜¯å ‚å“¥ï¼‰å’Œä»–è€å©†ä¸€èµ·åƒé¥­ï¼Œåƒäº†é­”å®—çƒ¤è‚‰ï¼Œç„¶åå–äº†èŒ¶é¢œæ‚¦è‰²ã€‚æ€»ä½“æ¥è¯´ä¹ŸæŒºé¡ºåˆ©çš„ï¼Œæ„Ÿè§‰ä»–ä»¬ä¹Ÿä¸éš¾ç›¸å¤„ã€‚</p>\n<p><img src=\"/2023/10/05/2023%E4%B8%AD%E7%A7%8B/%E6%AD%A6%E5%95%86%E6%A2%A6%E6%97%B6%E4%BB%A3.png\" alt=\"æ­¦å•†æ¢¦æ—¶ä»£é‡Œé¢çš„ç¾é£Ÿè¡—ä¹°é²œè™¾æ±¤åŒ…\" title=\"æ­¦å•†æ¢¦æ—¶ä»£é‡Œé¢çš„ç¾é£Ÿè¡—ä¹°é²œè™¾æ±¤åŒ…\"></p>\n<p>ç¬¬ä¸‰å¤©æˆ‘ä»¬å»äº†æ¬¢ä¹è°·ï¼æ˜¯æˆ‘ä»¬ç¬¬ä¸€æ¬¡ä¸€èµ·å»æ¸¸ä¹åœºï¼ç©äº†ä¸€ä¸ªè¿‡å±±è½¦ï¼Œç„¶ååšäº†å¤ªé˜³é£è½¦ï¼Œ00 å°±å¤´æ™•æƒ³åäº†ï¼Œæœç„¶è¿˜æ˜¯ä¸è¡Œâ€¦â€¦ä½†æ˜¯æ²¡äº‹ï¼Œè¿˜æ˜¯æŒºå¼€å¿ƒçš„ã€‚æ’é˜Ÿè¿‡ç¨‹ä¸­è¿˜é‡åˆ°äº†æ’é˜Ÿçš„äººï¼Œå¥½æ¶å¿ƒï¼</p>\n<p>æ™šä¸Šæˆ‘ä»¬è·Ÿä¸€äº›äººï¼ˆå…±ä¸ƒä¸ªäººï¼‰ä¸€èµ·æ‹¼è½¦å›æ¥åº”åŸï¼Œå±…ç„¶æ¯”ç«è½¦è¿˜ä¾¿å®œï¼Œä¸é”™ã€‚å›æ¥å·²ç»11ç‚¹äº†ï¼Œç„¶åå›å®¶æ”¾ä¸‹è¡Œæç®±ä¹‹ååˆå‡ºå»æ‰¾å¥¹åˆä¸­åŒå­¦ä¸€èµ·åƒå®µå¤œã€‚</p>\n<p><img src=\"/2023/10/05/2023%E4%B8%AD%E7%A7%8B/%E6%AD%A6%E6%B1%89%E6%AC%A2%E4%B9%90%E8%B0%B7.png\" alt=\"åœ¨æ­¦æ±‰æ¬¢ä¹è°·ç©è€ã€‚\" title=\"åœ¨æ­¦æ±‰æ¬¢ä¹è°·ç©è€ã€‚\"></p>\n<h2 id=\"å…¬äº‹\">å…¬äº‹</h2>\n<p>è¿™ä¸ªå‡æœŸæœ‰ç‚¹é•¿ï¼Œæ„Ÿè§‰æœ‰å¾ˆå¤šæ´»éƒ½æ²¡æœ‰å¹²ã€‚æ¯å¤©éƒ½å¾ˆå¤šäº‹æƒ…ï¼Œæ„Ÿè§‰è¿™é‡Œçš„äººå¤ªé—²äº†ï¼Œåº”è¯¥è®©ä»–ä»¬å¤šä¸Šç­å“ˆå“ˆå“ˆã€‚å¤æ–‡å­—ç¿»è¯‘çš„å·¥ä½œè¿˜æ²¡æœ‰å¹²å®Œï¼Œç›®å‰æ„Ÿè§‰æ•ˆæœä¸æ˜¯å¾ˆå¥½ï¼Œæˆ‘ä¹Ÿä¸æƒ³å¹²è¿™ä¸ªäº†ï¼Œæ„Ÿè§‰å¾ˆæµªè´¹æˆ‘çš„æ—¶é—´â€¦â€¦è‡³äºå¯¹é½ç¥ç»å…ƒï¼Œè²Œä¼¼ç°æœ‰æ–¹æ³•éƒ½æ— æ³•ç”¨åœ¨è‡ªå›å½’æ¨¡å‹ä¸Šé¢ï¼Œä½†æ˜¯å¯¹é½é—®é¢˜å¥½åƒä¹‹åè‡ªå›å½’æ¨¡å‹æ‰ä¼šå‡ºç°ã€‚ä¸çŸ¥é“æ˜¯ä¸æ˜¯æˆ‘æ²¡æœ‰æ‰¾åˆ°ï¼Œç›®å‰è¿˜æ²¡æœ‰æ‰¾åˆ°ä¸€ç¯‡ç ”ç©¶ç¥ç»å…ƒå¯¹ç”Ÿæˆç»“æœçš„å½±å“çš„å·¥ä½œã€‚<a href=\"https://www.github.com/kmeng01/rome\">ROME</a> çš„ Causal Tracing æ„Ÿè§‰å¯ä»¥ç”¨ï¼Œè¿™ä¸¤å¤©å¾—èµ¶ç´§åšç‚¹ä¸œè¥¿å‡ºæ¥ã€‚</p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn1\" class=\"footnote-item\"><p>27å·æ˜¯åˆä¸­åŒå­¦ï¼ˆé­é™ˆï¼‰çš„å©šç¤¼ï¼Œ5å·æ˜¯å ‚å§ï¼ˆéª†å“é¢–ï¼‰çš„å©šç¤¼ã€‚ <a href=\"#fnref1\" class=\"footnote-backref\">â†©ï¸</a></p>\n</li>\n<li id=\"fn2\" class=\"footnote-item\"><p>ä¹æœˆäºŒåå…­æ—¥å›æ¥ï¼Œåæœˆä¸ƒæ—¥èµ°ã€‚åé«˜é“åˆ°åŒ—äº¬ï¼Œç„¶ååšç«è½¦åˆ°åº”åŸã€‚ <a href=\"#fnref2\" class=\"footnote-backref\">â†©ï¸</a></p>\n</li>\n<li id=\"fn3\" class=\"footnote-item\"><p>ä¹‹å‰åœ¨ä¸Šæµ·éƒ½æ²¡æ‰¾åˆ°ã€‚ <a href=\"#fnref3\" class=\"footnote-backref\">â†©ï¸</a></p>\n</li>\n</ol>\n</section>\n","site":{"data":{}},"excerpt":"<p>ä»Šå¹´å›½åº† ğŸ‡¨ğŸ‡³ å’Œä¸­ç§‹ ğŸ¥® ä¸€èµ·æ”¾å‡ï¼Œæˆ‘è·Ÿ 00 ä¸€èµ·å›æ¥åº”åŸå‚åŠ å¥¹å ‚å§å’Œåˆä¸­åŒå­¦çš„å©šç¤¼<sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">[1]</a></sup>ï¼Œ\nä½åœ¨å¥¹å®¶é‡Œåä¸ªå¤œæ™š<sup class=\"footnote-ref\"><a href=\"#fn2\" id=\"fnref2\">[2]</a></sup>ã€‚ç¬¬äºŒæ¬¡è§å®¶é•¿ï¼Œä¹Ÿç®—æ˜¯æŒºé¡ºåˆ©ï¼Œä½†æ˜¯æ¯å¤©éƒ½ä¼šè§åˆ°é™Œç”Ÿäººï¼Œæœ‰ç‚¹ç´¯ï¼Œåº†å¹¸çš„æ˜¯ï¼Œæ„Ÿè§‰åˆ° 00 èƒ½æ¥å—è·Ÿæˆ‘å®¶äººç”Ÿæ´»åœ¨ä¸€èµ·ã€‚ä¸€å·åˆ°ä¸‰å·æˆ‘ä»¬å»æ­¦æ±‰ç©äº†ä¸‰å¤©ï¼Œè¶…çº§å¼€å¿ƒï¼Œè·Ÿå¥¹åœ¨ä¸€èµ·è¿é€›å•†åœºéƒ½æ˜¯å¼€å¿ƒçš„ï¼</p>\n<h2 id=\"å°å¿åŸçš„æ°›å›´\">å°å¿åŸçš„æ°›å›´</h2>\n<p>åº”åŸè·Ÿæˆ‘æƒ³è±¡ä¸­çš„å°å¿åŸå¾ˆåƒï¼Œä¹Ÿæ˜¯å¾ˆå¤šè¿œæˆ¿äº²æˆšï¼Œä¹ ä¿—ä¹Ÿè®©äººå¾ˆçƒ¦ã€‚æ•¬é…’ã€éšåœ°æ‰”åƒåœ¾ã€å®¤å†…æŠ½çƒŸã€å…«å¦äººå®¶çš„ç§äº‹ã€è¯´è¯ç²—é„™ã€è„ã€è¯´äº†ä¸è¦è¿˜éè¦ç»™äººå®¶â€¦â€¦è€Œä¸”ç¡®å®èƒ½æ˜æ˜¾æ„Ÿè§‰åˆ°ï¼Œè¿™é‡Œçš„äººçš„ç´ è´¨çš„å¹³å‡æ°´å¹³æŒºä½çš„ï¼Œå°¤å…¶æ˜¯ä¸Šä¸€è¾ˆã€‚çœŸçš„å¾ˆè®¨åŒåƒå¸­ï¼Œ00 ä¹Ÿæ˜¯ï¼Œè¿™äº›ä¹ ä¿—çš„éº»çƒ¦ç¨‹åº¦è®© 00 éƒ½ä¸æƒ³ç»“å©šäº†â€¦â€¦</p>","more":"<p>ä½†æ˜¯æ— æ‰€è°“äº†ï¼Œä¹‹åèƒ½è·Ÿ 00 åœ¨ä¸€èµ·å°±å¥½ï¼Œé™¤äº†å›æ¥è¿‡èŠ‚åº”è¯¥ä¹Ÿå¾ˆå°‘æœºä¼šæœ‰è”ç³»ã€‚</p>\n<h2 id=\"æ­¦æ±‰\">æ­¦æ±‰</h2>\n<p>ä¸€å·åˆ°ä¸‰å·å»äº†æ­¦æ±‰æ—…æ¸¸ã€‚æ—©ä¸Šäº”ç‚¹å¤šè·Ÿ 00 çš„ â€äºŒå¦ˆâ€œï¼ˆå…¶å®æ˜¯å©¶ï¼Œå”å”çš„è€å©†ï¼‰åè½¦å»æ­¦æ±‰ï¼Œåäº†ä¸€ä¸ªå°æ—¶ã€‚ä»–ä»¬è¿™ä¹ˆæ—©æ˜¯å› ä¸ºè¦å»è°ˆå©šç¤¼çš„äº‹æƒ…ï¼Œç„¶åå®³æ€•å µè½¦ã€‚æˆ‘ä»¬åœ¨é…’åº—æ—è¾¹ä¸‹æ¥ï¼Œé‚£æ—¶å€™â€œäºŒå¦ˆâ€ä¸‹åœ°é“ç«™ä¸Šå•æ‰€ï¼Œç„¶å 00 éè¦ç»™å¥¹ä¹°åŒ…å­ï¼ˆä¸ºäº†ç¤¼è²Œï¼‰ï¼Œç„¶åå¥¹æœ€åè¿˜æ˜¯æ‹’ç»äº†ï¼Œå¯¼è‡´æˆ‘ä»¬å¾—è‡ªå·±åƒä¸‹åŒ…å­ã€‚è™½ç„¶åŒ…å­æ²¡æœ‰ä¸å¥½åƒï¼Œä½†æ˜¯æˆ‘å°±å¾ˆè®¨åŒè¿™ç§æ˜çŸ¥äººå®¶ä¸è¦è¿˜éè¦ä¹°çš„è¡Œä¸ºã€‚</p>\n<p>ä¹‹åæˆ‘ä»¬å»é…’åº—çš„æ—¶å€™ï¼Œè¿˜æ²¡æœ‰æˆ¿å­ï¼Œæˆ‘ä»¬å¯„å­˜äº†è¡Œæå°±ç›´æ¥å»æ–°å¤©åœ°ä¹°äº†æ¯éœ¸ç‹èŒ¶å§¬çš„å¥¶èŒ¶ï¼Œç„¶åå»äº†å¤å¾·å¯ºã€‚ç½‘ä¸Šè¯´ä¸å¯ä»¥ç©¿ç€æš´éœ²ï¼Œä½†æ˜¯æ„Ÿè§‰è·¯äººç©¿ç€è¿˜æ˜¯å¾ˆæš´éœ²ã€‚</p>\n<p><img src=\"/2023/10/05/2023%E4%B8%AD%E7%A7%8B/%E6%96%B0%E5%A4%A9%E5%9C%B0-%E9%9C%B8%E7%8E%8B%E8%8C%B6%E5%A7%AC.png\" alt=\"åœ¨æ­¦æ±‰æ–°å¤©åœ°ä¹°éœ¸ç‹èŒ¶å§¬ã€‚\" title=\"åœ¨æ­¦æ±‰æ–°å¤©åœ°ä¹°éœ¸ç‹èŒ¶å§¬ã€‚\"></p>\n<p>ä¹‹åè¿˜å»äº†è§£æ”¾å…¬å›­å’Œä¸­å±±å…¬å›­ï¼Œéƒ½æŒºä¸é”™çš„ã€‚å¤§åŸå¸‚å°±æ˜¯å¥½ã€‚é‡Œé¢çœ‹åˆ°äº†å¾ˆå¥½çœ‹çš„å»ºç­‘ç‰©ã€‚åœ¨ä¸­å±±å…¬å›­æˆ‘ä»¬é—®äº†ä¸¤ä¸ªå°å­©å€Ÿç”¨ç¾½æ¯›çƒæ‹å­æ¥æ‰“äº†å‡ ä¸‹ã€‚ä¹‹ååœ¨ä¸€ä¸ªç›¸äº²è§’<sup class=\"footnote-ref\"><a href=\"#fn3\" id=\"fnref3\">[3]</a></sup>æ—è¾¹è·Ÿå¥¹çš„é«˜ä¸­åŒå­¦ï¼Œå½­åŒï¼Œä¼šåˆï¼Œç„¶åé€›äº†ä¸€ä¸‹ç›¸äº²è§’ã€‚ä¹‹åæˆ‘ä»¬è¿˜åäº†ä¸€ä¸‹è¿‡å±±è½¦ï¼ˆå…¬å›­é‡Œé¢æœ‰è¿‡å±±è½¦è¿˜æ˜¯ç¬¬ä¸€æ¬¡è§ï¼‰ã€‚</p>\n<p><img src=\"/2023/10/05/2023%E4%B8%AD%E7%A7%8B/%E8%A7%A3%E6%94%BE%E5%85%AC%E5%9B%AD%E4%B8%AD%E9%97%B4.png\" alt=\"è§£æ”¾å…¬å›­ä¸­é—´çš„ä¸€ä¸ªå¾ˆå¤šå¡”çš„åœ°æ–¹ã€‚\" title=\"è§£æ”¾å…¬å›­ä¸­é—´çš„ä¸€ä¸ªå¾ˆå¤šå¡”çš„åœ°æ–¹ã€‚\"></p>\n<p>æ™šä¸Šå°±å»è·Ÿå¥¹çš„é«˜ä¸­åŒå­¦ä¸€èµ·åƒé¥­ã€‚</p>\n<p>ç¬¬äºŒå¤©æˆ‘ä»¬å…ˆåœ¨åœ°é“ç«™å‰ªäº†å¤´å‘ï¼Œç„¶åå»å®é€šå¯ºï¼Œæ™šä¸Šå»æ­¦å•†æ¢¦æ—¶ä»£ã€‚è¿™ä¸ªå•†åœºè§„æ ¼è¶…çº§é«˜ï¼Œè¿˜æŒºå¥½ç©çš„ã€‚ç¬¬ä¸€æ¬¡çœ‹åˆ°ç´¢å°¼ä¸“å–åº—ï¼Œè¿˜æœ‰ Pico ä¸“å–åº—ã€‚é‡Œé¢è¿˜æœ‰æ»‘é›ªçš„åœ°æ–¹ï¼Œä½†æ˜¯å¤ªè´µçš„ã€‚æˆ‘ä»¬è¿˜å»äº†ä¼˜è¡£åº“ï¼Œä¹°äº†ä¸€äº›è¡£æœï¼Œå‘ç°è¿˜æŒºä¾¿å®œçš„ã€‚ä»¥å‰éƒ½ä¼šè§‰å¾—é€›è¡—è´­ç‰©å¾ˆæ— èŠï¼Œä½†æ˜¯è·Ÿå¥¹åœ¨ä¸€èµ·è¿è¿é€›è¡—ä¹°è¡£æœéƒ½æ˜¯å¼€å¿ƒçš„ã€‚</p>\n<p>æ™šä¸Šæˆ‘ä»¬è·Ÿå¥¹â€œå¤§å“¥â€ï¼ˆå…¶å®æ˜¯å ‚å“¥ï¼‰å’Œä»–è€å©†ä¸€èµ·åƒé¥­ï¼Œåƒäº†é­”å®—çƒ¤è‚‰ï¼Œç„¶åå–äº†èŒ¶é¢œæ‚¦è‰²ã€‚æ€»ä½“æ¥è¯´ä¹ŸæŒºé¡ºåˆ©çš„ï¼Œæ„Ÿè§‰ä»–ä»¬ä¹Ÿä¸éš¾ç›¸å¤„ã€‚</p>\n<p><img src=\"/2023/10/05/2023%E4%B8%AD%E7%A7%8B/%E6%AD%A6%E5%95%86%E6%A2%A6%E6%97%B6%E4%BB%A3.png\" alt=\"æ­¦å•†æ¢¦æ—¶ä»£é‡Œé¢çš„ç¾é£Ÿè¡—ä¹°é²œè™¾æ±¤åŒ…\" title=\"æ­¦å•†æ¢¦æ—¶ä»£é‡Œé¢çš„ç¾é£Ÿè¡—ä¹°é²œè™¾æ±¤åŒ…\"></p>\n<p>ç¬¬ä¸‰å¤©æˆ‘ä»¬å»äº†æ¬¢ä¹è°·ï¼æ˜¯æˆ‘ä»¬ç¬¬ä¸€æ¬¡ä¸€èµ·å»æ¸¸ä¹åœºï¼ç©äº†ä¸€ä¸ªè¿‡å±±è½¦ï¼Œç„¶ååšäº†å¤ªé˜³é£è½¦ï¼Œ00 å°±å¤´æ™•æƒ³åäº†ï¼Œæœç„¶è¿˜æ˜¯ä¸è¡Œâ€¦â€¦ä½†æ˜¯æ²¡äº‹ï¼Œè¿˜æ˜¯æŒºå¼€å¿ƒçš„ã€‚æ’é˜Ÿè¿‡ç¨‹ä¸­è¿˜é‡åˆ°äº†æ’é˜Ÿçš„äººï¼Œå¥½æ¶å¿ƒï¼</p>\n<p>æ™šä¸Šæˆ‘ä»¬è·Ÿä¸€äº›äººï¼ˆå…±ä¸ƒä¸ªäººï¼‰ä¸€èµ·æ‹¼è½¦å›æ¥åº”åŸï¼Œå±…ç„¶æ¯”ç«è½¦è¿˜ä¾¿å®œï¼Œä¸é”™ã€‚å›æ¥å·²ç»11ç‚¹äº†ï¼Œç„¶åå›å®¶æ”¾ä¸‹è¡Œæç®±ä¹‹ååˆå‡ºå»æ‰¾å¥¹åˆä¸­åŒå­¦ä¸€èµ·åƒå®µå¤œã€‚</p>\n<p><img src=\"/2023/10/05/2023%E4%B8%AD%E7%A7%8B/%E6%AD%A6%E6%B1%89%E6%AC%A2%E4%B9%90%E8%B0%B7.png\" alt=\"åœ¨æ­¦æ±‰æ¬¢ä¹è°·ç©è€ã€‚\" title=\"åœ¨æ­¦æ±‰æ¬¢ä¹è°·ç©è€ã€‚\"></p>\n<h2 id=\"å…¬äº‹\">å…¬äº‹</h2>\n<p>è¿™ä¸ªå‡æœŸæœ‰ç‚¹é•¿ï¼Œæ„Ÿè§‰æœ‰å¾ˆå¤šæ´»éƒ½æ²¡æœ‰å¹²ã€‚æ¯å¤©éƒ½å¾ˆå¤šäº‹æƒ…ï¼Œæ„Ÿè§‰è¿™é‡Œçš„äººå¤ªé—²äº†ï¼Œåº”è¯¥è®©ä»–ä»¬å¤šä¸Šç­å“ˆå“ˆå“ˆã€‚å¤æ–‡å­—ç¿»è¯‘çš„å·¥ä½œè¿˜æ²¡æœ‰å¹²å®Œï¼Œç›®å‰æ„Ÿè§‰æ•ˆæœä¸æ˜¯å¾ˆå¥½ï¼Œæˆ‘ä¹Ÿä¸æƒ³å¹²è¿™ä¸ªäº†ï¼Œæ„Ÿè§‰å¾ˆæµªè´¹æˆ‘çš„æ—¶é—´â€¦â€¦è‡³äºå¯¹é½ç¥ç»å…ƒï¼Œè²Œä¼¼ç°æœ‰æ–¹æ³•éƒ½æ— æ³•ç”¨åœ¨è‡ªå›å½’æ¨¡å‹ä¸Šé¢ï¼Œä½†æ˜¯å¯¹é½é—®é¢˜å¥½åƒä¹‹åè‡ªå›å½’æ¨¡å‹æ‰ä¼šå‡ºç°ã€‚ä¸çŸ¥é“æ˜¯ä¸æ˜¯æˆ‘æ²¡æœ‰æ‰¾åˆ°ï¼Œç›®å‰è¿˜æ²¡æœ‰æ‰¾åˆ°ä¸€ç¯‡ç ”ç©¶ç¥ç»å…ƒå¯¹ç”Ÿæˆç»“æœçš„å½±å“çš„å·¥ä½œã€‚<a href=\"https://www.github.com/kmeng01/rome\">ROME</a> çš„ Causal Tracing æ„Ÿè§‰å¯ä»¥ç”¨ï¼Œè¿™ä¸¤å¤©å¾—èµ¶ç´§åšç‚¹ä¸œè¥¿å‡ºæ¥ã€‚</p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn1\" class=\"footnote-item\"><p>27å·æ˜¯åˆä¸­åŒå­¦ï¼ˆé­é™ˆï¼‰çš„å©šç¤¼ï¼Œ5å·æ˜¯å ‚å§ï¼ˆéª†å“é¢–ï¼‰çš„å©šç¤¼ã€‚ <a href=\"#fnref1\" class=\"footnote-backref\">â†©ï¸</a></p>\n</li>\n<li id=\"fn2\" class=\"footnote-item\"><p>ä¹æœˆäºŒåå…­æ—¥å›æ¥ï¼Œåæœˆä¸ƒæ—¥èµ°ã€‚åé«˜é“åˆ°åŒ—äº¬ï¼Œç„¶ååšç«è½¦åˆ°åº”åŸã€‚ <a href=\"#fnref2\" class=\"footnote-backref\">â†©ï¸</a></p>\n</li>\n<li id=\"fn3\" class=\"footnote-item\"><p>ä¹‹å‰åœ¨ä¸Šæµ·éƒ½æ²¡æ‰¾åˆ°ã€‚ <a href=\"#fnref3\" class=\"footnote-backref\">â†©ï¸</a></p>\n</li>\n</ol>\n</section>"},{"author":"é™ˆè‹±å‘ Yingfa Chen","title":"CFDBench: A Comprehensive Benchmark for Machine Learning Methods in Fluid Dynamics","date":"2023-09-16T11:47:17.000Z","featured":true,"_content":"\n[Code](https://www.github.com/luo-yining/CFDBench) | [Paper (on hold by ArXiv)](...) | [Paper (preprints.org)](https://www.preprints.org/manuscript/202309.1550/v1) | [çŸ¥ä¹](https://zhuanlan.zhihu.com/p/656033757)\n\nI did this work with my girlfriend, whose research direction is computational fluid dynamics (CFD). We observed that there are numerous research works in applying deep learning (DL) to solve CFD problems. E.g., [Pangu-Weather](https://github.com/198808xc/Pangu-Weather) have shown that DL methods can not only be more accurate than the best numerical methods, but can also be multiple magnitudes faster.\n\n<!-- more -->\n\nHowever, there is no standard benchmark for evaluating the performance of different DL methods. Therefore, we constructed CFDBench.\n\n---\n\n## Abstract\n\nIn recent years, applying deep learning to solve physics problems has attracted much attention. Data-driven deep learning methods produce operators that can learn solutions to the whole system of partial differential equations. However, the existing methods are only evaluated on simple flow equations (e.g., Burger's equation), and only consider the generalization ability on different initial conditions. In this paper, we construct CFDBench, a benchmark with four classic problems in computational fluid dynamics (CFD): lid-driven cavity flow, laminar boundary layer flow in circular tubes, dam flows through the steps, and periodic Karman vortex street. Each flow problem includes data with different boundary conditions, fluid physical properties, and domain geometry. Compared to existing datasets, the advantages of CFDBench are (1) comprehensive. It contains common physical parameters such as velocity, pressure, and cavity fraction. (2) realistic. It is very suitable for deep learning solutions of fluid mechanics equations. (3) challenging. It has a certain learning difficulty, prompting to find models with strong learning ability. (4) standardized. CFDBench facilitates a comprehensive and fair comparison of different deep learning methods for CFD. We make appropriate modifications to popular deep neural networks to apply them to CFDBench and enable the accommodation of more changing inputs. The evaluation on CFDBench reveals some new shortcomings of existing works and we propose possible directions for solving such problems.\n","source":"_posts/CFDBench.md","raw":"---\nauthor: é™ˆè‹±å‘ Yingfa Chen\ntitle: \"CFDBench: A Comprehensive Benchmark for Machine Learning Methods in Fluid Dynamics\"\ndate: 2023-09-16 19:47:17\ncategories: Research\ntags:\n- paper\n- research\n- cfd\n- dataset\n- \"00\"\n- english\n- pinn\n- fno\n- physics\n- machine-learning\n- deep-learning\n- deeponet\n- ai4science\nfeatured: true\n---\n\n[Code](https://www.github.com/luo-yining/CFDBench) | [Paper (on hold by ArXiv)](...) | [Paper (preprints.org)](https://www.preprints.org/manuscript/202309.1550/v1) | [çŸ¥ä¹](https://zhuanlan.zhihu.com/p/656033757)\n\nI did this work with my girlfriend, whose research direction is computational fluid dynamics (CFD). We observed that there are numerous research works in applying deep learning (DL) to solve CFD problems. E.g., [Pangu-Weather](https://github.com/198808xc/Pangu-Weather) have shown that DL methods can not only be more accurate than the best numerical methods, but can also be multiple magnitudes faster.\n\n<!-- more -->\n\nHowever, there is no standard benchmark for evaluating the performance of different DL methods. Therefore, we constructed CFDBench.\n\n---\n\n## Abstract\n\nIn recent years, applying deep learning to solve physics problems has attracted much attention. Data-driven deep learning methods produce operators that can learn solutions to the whole system of partial differential equations. However, the existing methods are only evaluated on simple flow equations (e.g., Burger's equation), and only consider the generalization ability on different initial conditions. In this paper, we construct CFDBench, a benchmark with four classic problems in computational fluid dynamics (CFD): lid-driven cavity flow, laminar boundary layer flow in circular tubes, dam flows through the steps, and periodic Karman vortex street. Each flow problem includes data with different boundary conditions, fluid physical properties, and domain geometry. Compared to existing datasets, the advantages of CFDBench are (1) comprehensive. It contains common physical parameters such as velocity, pressure, and cavity fraction. (2) realistic. It is very suitable for deep learning solutions of fluid mechanics equations. (3) challenging. It has a certain learning difficulty, prompting to find models with strong learning ability. (4) standardized. CFDBench facilitates a comprehensive and fair comparison of different deep learning methods for CFD. We make appropriate modifications to popular deep neural networks to apply them to CFDBench and enable the accommodation of more changing inputs. The evaluation on CFDBench reveals some new shortcomings of existing works and we propose possible directions for solving such problems.\n","slug":"CFDBench","published":1,"updated":"2024-01-11T03:45:40.727Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cltl4oxee0003xh7k1kul88dt","content":"<p><a href=\"https://www.github.com/luo-yining/CFDBench\">Code</a> | <a href=\"...\">Paper (on hold by ArXiv)</a> | <a href=\"https://www.preprints.org/manuscript/202309.1550/v1\">Paper (preprints.org)</a> | <a href=\"https://zhuanlan.zhihu.com/p/656033757\">çŸ¥ä¹</a></p>\n<p>I did this work with my girlfriend, whose research direction is computational fluid dynamics (CFD). We observed that there are numerous research works in applying deep learning (DL) to solve CFD problems. E.g., <a href=\"https://github.com/198808xc/Pangu-Weather\">Pangu-Weather</a> have shown that DL methods can not only be more accurate than the best numerical methods, but can also be multiple magnitudes faster.</p>\n<span id=\"more\"></span>\n<p>However, there is no standard benchmark for evaluating the performance of different DL methods. Therefore, we constructed CFDBench.</p>\n<hr>\n<h2 id=\"Abstract\">Abstract</h2>\n<p>In recent years, applying deep learning to solve physics problems has attracted much attention. Data-driven deep learning methods produce operators that can learn solutions to the whole system of partial differential equations. However, the existing methods are only evaluated on simple flow equations (e.g., Burger's equation), and only consider the generalization ability on different initial conditions. In this paper, we construct CFDBench, a benchmark with four classic problems in computational fluid dynamics (CFD): lid-driven cavity flow, laminar boundary layer flow in circular tubes, dam flows through the steps, and periodic Karman vortex street. Each flow problem includes data with different boundary conditions, fluid physical properties, and domain geometry. Compared to existing datasets, the advantages of CFDBench are (1) comprehensive. It contains common physical parameters such as velocity, pressure, and cavity fraction. (2) realistic. It is very suitable for deep learning solutions of fluid mechanics equations. (3) challenging. It has a certain learning difficulty, prompting to find models with strong learning ability. (4) standardized. CFDBench facilitates a comprehensive and fair comparison of different deep learning methods for CFD. We make appropriate modifications to popular deep neural networks to apply them to CFDBench and enable the accommodation of more changing inputs. The evaluation on CFDBench reveals some new shortcomings of existing works and we propose possible directions for solving such problems.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"https://www.github.com/luo-yining/CFDBench\">Code</a> | <a href=\"...\">Paper (on hold by ArXiv)</a> | <a href=\"https://www.preprints.org/manuscript/202309.1550/v1\">Paper (preprints.org)</a> | <a href=\"https://zhuanlan.zhihu.com/p/656033757\">çŸ¥ä¹</a></p>\n<p>I did this work with my girlfriend, whose research direction is computational fluid dynamics (CFD). We observed that there are numerous research works in applying deep learning (DL) to solve CFD problems. E.g., <a href=\"https://github.com/198808xc/Pangu-Weather\">Pangu-Weather</a> have shown that DL methods can not only be more accurate than the best numerical methods, but can also be multiple magnitudes faster.</p>","more":"<p>However, there is no standard benchmark for evaluating the performance of different DL methods. Therefore, we constructed CFDBench.</p>\n<hr>\n<h2 id=\"Abstract\">Abstract</h2>\n<p>In recent years, applying deep learning to solve physics problems has attracted much attention. Data-driven deep learning methods produce operators that can learn solutions to the whole system of partial differential equations. However, the existing methods are only evaluated on simple flow equations (e.g., Burger's equation), and only consider the generalization ability on different initial conditions. In this paper, we construct CFDBench, a benchmark with four classic problems in computational fluid dynamics (CFD): lid-driven cavity flow, laminar boundary layer flow in circular tubes, dam flows through the steps, and periodic Karman vortex street. Each flow problem includes data with different boundary conditions, fluid physical properties, and domain geometry. Compared to existing datasets, the advantages of CFDBench are (1) comprehensive. It contains common physical parameters such as velocity, pressure, and cavity fraction. (2) realistic. It is very suitable for deep learning solutions of fluid mechanics equations. (3) challenging. It has a certain learning difficulty, prompting to find models with strong learning ability. (4) standardized. CFDBench facilitates a comprehensive and fair comparison of different deep learning methods for CFD. We make appropriate modifications to popular deep neural networks to apply them to CFDBench and enable the accommodation of more changing inputs. The evaluation on CFDBench reveals some new shortcomings of existing works and we propose possible directions for solving such problems.</p>"},{"author":"é™ˆè‹±å‘ Yingfa Chen","title":"Activation Addition (ActAdd)","date":"2023-10-07T09:55:33.000Z","_content":"\n[Paper](https://arxiv.org/abs/2308.10248)\n\nTLDR: Propose **ActAdd**, a method for controlling model behavior during inference by modifying activations with a bias term that is learned from a pair of prompt.\n\nSummary:\n\n- Propose **ActAdd**, a method for controlling model behavior by modifying activations at inference time.\n- Steering vectors are computed by taking the activation differences that result from pairs of prompts. The vectors are added as bias during inference.\n- ActAdd provides control over high-level properties of the output, and preserves off-target model performance, and requires little computational and implementational costs.\n\n<!-- more -->\n\n> The recently popular [representation engineering paper](https://arxiv.org/abs/2310.01405) (RepE) seems to be largely inspired by this work.\n\n## Background\n\nThe authors propose to compute steering vectors to steer the model's behavior. They call such methods *activation engineering*. They make the following contributions:\n\n- Find that combining forward passes works well in GPT-2, despite it was not trained for this.\n- The proposed method, **ActAdd**, is efficient, requiring no gradient descent or labeled data.\n\nThe difference between ActAdd and existing steering vector methods is that they find the vectors via one of the following.\n\n- Differences after fine-tuning\n- Per-query gradient-based search\n- Linear probes + differences in truthy attention heads\n\nIn contrast, ActAdd uses the difference between prompt pairs instead.\n\n## Method\n\nThe method is really, really simple. Simply manually contruct a pair of prompts, and compute the difference between the activations. Then, add the difference as a bias term to the activations during inference. The algorithm is as follows.\n\n![ActAdd method](./actadd/alg.png \"The algorithm of ActAdd\")\n\nAs shown, this method has two hyperparameters, the amount of drift $c$, and the modified layer $l$, and requires two manually constructed prompts $(p_+, p_-)$. How to more effectively construct these prompts is not discussed in this paper.\n\n## Result\n\n![Main result](./actadd/result.png \"Main results.\")\n\n## My Thoughts\n\nThe effectiveness of this method is a strong evidence that input and output features are represented as linear directions in representational space, but we still have no explanation for why such linearity arises naturally in LLMs. The fact that this actually works is very thought-provoking. However, ActAdd start to see degraded performance on off-target inputs when we drive the activations to far off, and the steering sometimes simply fails, this may indicate that the optimal steering path is not linear, which I believe is reasonable. This is somewhat similar to neuron attribution methods, many features/skills/knowledge cannot be attributed to single neurons (they are distributed across neurons), but existing neuron attribution methods still work well because, by change, some features are primarily determined by the activity or state of a single neuron (or a small set of neurons).\n\nWe can also draw a parallel with [BitFit](https://arxiv.org/abs/2106.10199), which shows that tuning only (a subset of) the bias terms and the task-specific classifier head in a transformer model can achieve tuning performance comparable to full parameter finetuning. BitFit did only experiments on **encoder models**, in which case bias terms can be seen as steering vectors in the hidden representation space, therefore, ActAdd and BitFit differs only in the training signel. ActAdd uses the difference between the representation of a pair of (positive and negative) prompts, while BitFit propagates the human annotation from the classification head. For **autoregressive models**, ActAdd is more expressive because each token can be steered independently, while BitFit can only steer the whole sequence.\n\nInterestingly, the fact that difference (or other arithmetics) in hidden representation are useful signal for some semantics has been shown in the era of learning word vectors, the fact that this can be used as a training signal is pretty neat.\n\nThe author also discussed the difference between activation engineering and adaptation methods, but the empirical results were not enough to show that ActAdd can be used as a substitute for adaptation. E.g., we cannot practically adapt the model into performing machine translation with ActAdd, because there is no negative prompt for translation. But perhaps upcoming works can apply this method to replace fine-tuning (or other adaptation methods). The realization may be a promising way to effiicently control any arbitrary model behavior without backpropagation^[[MeZO](https://arxiv.org/abs/2305.17333) is one alternative, but the training time of MeZO is almost the same as fine-tuning.].\n\nNevertheless, this method is extremely interesting, and I feel like many existing methods can be improved by taking inspirations from this work.\n","source":"_posts/actadd.md","raw":"---\nauthor: é™ˆè‹±å‘ Yingfa Chen\ntitle: Activation Addition (ActAdd)\ndate: 2023-10-07 17:55:33\ncategories: Paper Note\ntags:\n- english\n- ai-alignment\n- llm\n- gpt\n- activation-modification\n- adaptation\n- model-editing\n- representation-engineering\n- fine-tuning\n- parameter-efficient-tuning\n---\n\n[Paper](https://arxiv.org/abs/2308.10248)\n\nTLDR: Propose **ActAdd**, a method for controlling model behavior during inference by modifying activations with a bias term that is learned from a pair of prompt.\n\nSummary:\n\n- Propose **ActAdd**, a method for controlling model behavior by modifying activations at inference time.\n- Steering vectors are computed by taking the activation differences that result from pairs of prompts. The vectors are added as bias during inference.\n- ActAdd provides control over high-level properties of the output, and preserves off-target model performance, and requires little computational and implementational costs.\n\n<!-- more -->\n\n> The recently popular [representation engineering paper](https://arxiv.org/abs/2310.01405) (RepE) seems to be largely inspired by this work.\n\n## Background\n\nThe authors propose to compute steering vectors to steer the model's behavior. They call such methods *activation engineering*. They make the following contributions:\n\n- Find that combining forward passes works well in GPT-2, despite it was not trained for this.\n- The proposed method, **ActAdd**, is efficient, requiring no gradient descent or labeled data.\n\nThe difference between ActAdd and existing steering vector methods is that they find the vectors via one of the following.\n\n- Differences after fine-tuning\n- Per-query gradient-based search\n- Linear probes + differences in truthy attention heads\n\nIn contrast, ActAdd uses the difference between prompt pairs instead.\n\n## Method\n\nThe method is really, really simple. Simply manually contruct a pair of prompts, and compute the difference between the activations. Then, add the difference as a bias term to the activations during inference. The algorithm is as follows.\n\n![ActAdd method](./actadd/alg.png \"The algorithm of ActAdd\")\n\nAs shown, this method has two hyperparameters, the amount of drift $c$, and the modified layer $l$, and requires two manually constructed prompts $(p_+, p_-)$. How to more effectively construct these prompts is not discussed in this paper.\n\n## Result\n\n![Main result](./actadd/result.png \"Main results.\")\n\n## My Thoughts\n\nThe effectiveness of this method is a strong evidence that input and output features are represented as linear directions in representational space, but we still have no explanation for why such linearity arises naturally in LLMs. The fact that this actually works is very thought-provoking. However, ActAdd start to see degraded performance on off-target inputs when we drive the activations to far off, and the steering sometimes simply fails, this may indicate that the optimal steering path is not linear, which I believe is reasonable. This is somewhat similar to neuron attribution methods, many features/skills/knowledge cannot be attributed to single neurons (they are distributed across neurons), but existing neuron attribution methods still work well because, by change, some features are primarily determined by the activity or state of a single neuron (or a small set of neurons).\n\nWe can also draw a parallel with [BitFit](https://arxiv.org/abs/2106.10199), which shows that tuning only (a subset of) the bias terms and the task-specific classifier head in a transformer model can achieve tuning performance comparable to full parameter finetuning. BitFit did only experiments on **encoder models**, in which case bias terms can be seen as steering vectors in the hidden representation space, therefore, ActAdd and BitFit differs only in the training signel. ActAdd uses the difference between the representation of a pair of (positive and negative) prompts, while BitFit propagates the human annotation from the classification head. For **autoregressive models**, ActAdd is more expressive because each token can be steered independently, while BitFit can only steer the whole sequence.\n\nInterestingly, the fact that difference (or other arithmetics) in hidden representation are useful signal for some semantics has been shown in the era of learning word vectors, the fact that this can be used as a training signal is pretty neat.\n\nThe author also discussed the difference between activation engineering and adaptation methods, but the empirical results were not enough to show that ActAdd can be used as a substitute for adaptation. E.g., we cannot practically adapt the model into performing machine translation with ActAdd, because there is no negative prompt for translation. But perhaps upcoming works can apply this method to replace fine-tuning (or other adaptation methods). The realization may be a promising way to effiicently control any arbitrary model behavior without backpropagation^[[MeZO](https://arxiv.org/abs/2305.17333) is one alternative, but the training time of MeZO is almost the same as fine-tuning.].\n\nNevertheless, this method is extremely interesting, and I feel like many existing methods can be improved by taking inspirations from this work.\n","slug":"actadd","published":1,"updated":"2024-01-11T05:42:02.734Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cltl4oxeg0007xh7k5ati3i3b","content":"<p><a href=\"https://arxiv.org/abs/2308.10248\">Paper</a></p>\n<p>TLDR: Propose <strong>ActAdd</strong>, a method for controlling model behavior during inference by modifying activations with a bias term that is learned from a pair of prompt.</p>\n<p>Summary:</p>\n<ul>\n<li>Propose <strong>ActAdd</strong>, a method for controlling model behavior by modifying activations at inference time.</li>\n<li>Steering vectors are computed by taking the activation differences that result from pairs of prompts. The vectors are added as bias during inference.</li>\n<li>ActAdd provides control over high-level properties of the output, and preserves off-target model performance, and requires little computational and implementational costs.</li>\n</ul>\n<span id=\"more\"></span>\n<blockquote>\n<p>The recently popular <a href=\"https://arxiv.org/abs/2310.01405\">representation engineering paper</a> (RepE) seems to be largely inspired by this work.</p>\n</blockquote>\n<h2 id=\"Background\">Background</h2>\n<p>The authors propose to compute steering vectors to steer the model's behavior. They call such methods <em>activation engineering</em>. They make the following contributions:</p>\n<ul>\n<li>Find that combining forward passes works well in GPT-2, despite it was not trained for this.</li>\n<li>The proposed method, <strong>ActAdd</strong>, is efficient, requiring no gradient descent or labeled data.</li>\n</ul>\n<p>The difference between ActAdd and existing steering vector methods is that they find the vectors via one of the following.</p>\n<ul>\n<li>Differences after fine-tuning</li>\n<li>Per-query gradient-based search</li>\n<li>Linear probes + differences in truthy attention heads</li>\n</ul>\n<p>In contrast, ActAdd uses the difference between prompt pairs instead.</p>\n<h2 id=\"Method\">Method</h2>\n<p>The method is really, really simple. Simply manually contruct a pair of prompts, and compute the difference between the activations. Then, add the difference as a bias term to the activations during inference. The algorithm is as follows.</p>\n<p><img src=\"/2023/10/07/actadd/alg.png\" alt=\"ActAdd method\" title=\"The algorithm of ActAdd\"></p>\n<p>As shown, this method has two hyperparameters, the amount of drift $c$, and the modified layer $l$, and requires two manually constructed prompts $(p_+, p_-)$. How to more effectively construct these prompts is not discussed in this paper.</p>\n<h2 id=\"Result\">Result</h2>\n<p><img src=\"/2023/10/07/actadd/result.png\" alt=\"Main result\" title=\"Main results.\"></p>\n<h2 id=\"My-Thoughts\">My Thoughts</h2>\n<p>The effectiveness of this method is a strong evidence that input and output features are represented as linear directions in representational space, but we still have no explanation for why such linearity arises naturally in LLMs. The fact that this actually works is very thought-provoking. However, ActAdd start to see degraded performance on off-target inputs when we drive the activations to far off, and the steering sometimes simply fails, this may indicate that the optimal steering path is not linear, which I believe is reasonable. This is somewhat similar to neuron attribution methods, many features/skills/knowledge cannot be attributed to single neurons (they are distributed across neurons), but existing neuron attribution methods still work well because, by change, some features are primarily determined by the activity or state of a single neuron (or a small set of neurons).</p>\n<p>We can also draw a parallel with <a href=\"https://arxiv.org/abs/2106.10199\">BitFit</a>, which shows that tuning only (a subset of) the bias terms and the task-specific classifier head in a transformer model can achieve tuning performance comparable to full parameter finetuning. BitFit did only experiments on <strong>encoder models</strong>, in which case bias terms can be seen as steering vectors in the hidden representation space, therefore, ActAdd and BitFit differs only in the training signel. ActAdd uses the difference between the representation of a pair of (positive and negative) prompts, while BitFit propagates the human annotation from the classification head. For <strong>autoregressive models</strong>, ActAdd is more expressive because each token can be steered independently, while BitFit can only steer the whole sequence.</p>\n<p>Interestingly, the fact that difference (or other arithmetics) in hidden representation are useful signal for some semantics has been shown in the era of learning word vectors, the fact that this can be used as a training signal is pretty neat.</p>\n<p>The author also discussed the difference between activation engineering and adaptation methods, but the empirical results were not enough to show that ActAdd can be used as a substitute for adaptation. E.g., we cannot practically adapt the model into performing machine translation with ActAdd, because there is no negative prompt for translation. But perhaps upcoming works can apply this method to replace fine-tuning (or other adaptation methods). The realization may be a promising way to effiicently control any arbitrary model behavior without backpropagation<sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">[1]</a></sup>.</p>\n<p>Nevertheless, this method is extremely interesting, and I feel like many existing methods can be improved by taking inspirations from this work.</p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn1\" class=\"footnote-item\"><p><a href=\"https://arxiv.org/abs/2305.17333\">MeZO</a> is one alternative, but the training time of MeZO is almost the same as fine-tuning. <a href=\"#fnref1\" class=\"footnote-backref\">â†©ï¸</a></p>\n</li>\n</ol>\n</section>\n","site":{"data":{}},"excerpt":"<p><a href=\"https://arxiv.org/abs/2308.10248\">Paper</a></p>\n<p>TLDR: Propose <strong>ActAdd</strong>, a method for controlling model behavior during inference by modifying activations with a bias term that is learned from a pair of prompt.</p>\n<p>Summary:</p>\n<ul>\n<li>Propose <strong>ActAdd</strong>, a method for controlling model behavior by modifying activations at inference time.</li>\n<li>Steering vectors are computed by taking the activation differences that result from pairs of prompts. The vectors are added as bias during inference.</li>\n<li>ActAdd provides control over high-level properties of the output, and preserves off-target model performance, and requires little computational and implementational costs.</li>\n</ul>","more":"<blockquote>\n<p>The recently popular <a href=\"https://arxiv.org/abs/2310.01405\">representation engineering paper</a> (RepE) seems to be largely inspired by this work.</p>\n</blockquote>\n<h2 id=\"Background\">Background</h2>\n<p>The authors propose to compute steering vectors to steer the model's behavior. They call such methods <em>activation engineering</em>. They make the following contributions:</p>\n<ul>\n<li>Find that combining forward passes works well in GPT-2, despite it was not trained for this.</li>\n<li>The proposed method, <strong>ActAdd</strong>, is efficient, requiring no gradient descent or labeled data.</li>\n</ul>\n<p>The difference between ActAdd and existing steering vector methods is that they find the vectors via one of the following.</p>\n<ul>\n<li>Differences after fine-tuning</li>\n<li>Per-query gradient-based search</li>\n<li>Linear probes + differences in truthy attention heads</li>\n</ul>\n<p>In contrast, ActAdd uses the difference between prompt pairs instead.</p>\n<h2 id=\"Method\">Method</h2>\n<p>The method is really, really simple. Simply manually contruct a pair of prompts, and compute the difference between the activations. Then, add the difference as a bias term to the activations during inference. The algorithm is as follows.</p>\n<p><img src=\"/2023/10/07/actadd/alg.png\" alt=\"ActAdd method\" title=\"The algorithm of ActAdd\"></p>\n<p>As shown, this method has two hyperparameters, the amount of drift $c$, and the modified layer $l$, and requires two manually constructed prompts $(p_+, p_-)$. How to more effectively construct these prompts is not discussed in this paper.</p>\n<h2 id=\"Result\">Result</h2>\n<p><img src=\"/2023/10/07/actadd/result.png\" alt=\"Main result\" title=\"Main results.\"></p>\n<h2 id=\"My-Thoughts\">My Thoughts</h2>\n<p>The effectiveness of this method is a strong evidence that input and output features are represented as linear directions in representational space, but we still have no explanation for why such linearity arises naturally in LLMs. The fact that this actually works is very thought-provoking. However, ActAdd start to see degraded performance on off-target inputs when we drive the activations to far off, and the steering sometimes simply fails, this may indicate that the optimal steering path is not linear, which I believe is reasonable. This is somewhat similar to neuron attribution methods, many features/skills/knowledge cannot be attributed to single neurons (they are distributed across neurons), but existing neuron attribution methods still work well because, by change, some features are primarily determined by the activity or state of a single neuron (or a small set of neurons).</p>\n<p>We can also draw a parallel with <a href=\"https://arxiv.org/abs/2106.10199\">BitFit</a>, which shows that tuning only (a subset of) the bias terms and the task-specific classifier head in a transformer model can achieve tuning performance comparable to full parameter finetuning. BitFit did only experiments on <strong>encoder models</strong>, in which case bias terms can be seen as steering vectors in the hidden representation space, therefore, ActAdd and BitFit differs only in the training signel. ActAdd uses the difference between the representation of a pair of (positive and negative) prompts, while BitFit propagates the human annotation from the classification head. For <strong>autoregressive models</strong>, ActAdd is more expressive because each token can be steered independently, while BitFit can only steer the whole sequence.</p>\n<p>Interestingly, the fact that difference (or other arithmetics) in hidden representation are useful signal for some semantics has been shown in the era of learning word vectors, the fact that this can be used as a training signal is pretty neat.</p>\n<p>The author also discussed the difference between activation engineering and adaptation methods, but the empirical results were not enough to show that ActAdd can be used as a substitute for adaptation. E.g., we cannot practically adapt the model into performing machine translation with ActAdd, because there is no negative prompt for translation. But perhaps upcoming works can apply this method to replace fine-tuning (or other adaptation methods). The realization may be a promising way to effiicently control any arbitrary model behavior without backpropagation<sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">[1]</a></sup>.</p>\n<p>Nevertheless, this method is extremely interesting, and I feel like many existing methods can be improved by taking inspirations from this work.</p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn1\" class=\"footnote-item\"><p><a href=\"https://arxiv.org/abs/2305.17333\">MeZO</a> is one alternative, but the training time of MeZO is almost the same as fine-tuning. <a href=\"#fnref1\" class=\"footnote-backref\">â†©ï¸</a></p>\n</li>\n</ol>\n</section>"},{"author":"é™ˆè‹±å‘ Yingfa Chen","title":"InfiniteBench: Extending Long Context Evaluation Beyond 100K Tokens","date":"2024-01-10T02:38:38.000Z","featured":true,"_content":"\n[Code](http://www.github.com/OpenBMB/InfiniteBench) | [Paper](https://arxiv.org/abs/2402.13718)\n\nThe first benchmark for evaluating the effectiveness of LLMs in handling more than 100k tokens!\n\n> In the paper, we name it $\\infty$-Bench, but I will sometimes use \"InfiniteBench\" in this blog post for better readability.\n\nFinally got some time to write this blog, been so busy lately! I have been in a fairly long duration of research hiatus, meanwhile the field of NLP has been revolutionized by an overwhelming number of new LLMs. Finally, I was able to arrive at some productive and meaningful work in this new era of research, as a second author. In this blog post, I will introduce this work that I have been working on recently.\n\n<!-- more -->\n\n## Background\n\nThe advent of LLMs have shown many promising results, but many practice applications (e.g., agents, document/webpage reading, long text summarization, etc.) are greatly limited by the context length constraint. Therefore, many works have strived to increase the length of the context that LLMs can accept. However, current \"long-sequence\" benchmarks all fall below 100k tokens, and is therefore not able to evaluate the effectiveness of many long-context LLMs. Our work, $\\infty$-Bench\n\n## The Data\n\nThe data consists of language tasks from diverse domains (math, code, novels), two languages (English and Chinese). Half of the tasks are automatically generated, which is desirable for optionally further scaling the context lengths to any arbitrary lengths.\n\nFollowing shows the statistics of the tasks in our benchmark.\n\n![Data statistics. The angle of segments is proportional to the number of examples, and the radius is proportional to the average example lengths (sum of input and output tokens).](infinitebench/data-stat-pie.png \"Data statistics. The angle of segments is proportional to the number of examples, and the radius is proportional to the average example lengths (sum of input and output tokens).\")\n\n## Results\n\nWe tested SOTA proprietary and open-source LLMs at the time of evaluation. The result is shown below. We can see that in most tasks, the performance is far from satisfactory in practical applications.\n\n![Results of some SOTA long-context LLMs on our InfiniteBench](infinitebench/results.png \"Results of some SOTA long-context LLMs on our InfiniteBench\")\n\n## Thoughts on the Future of Long-Context Research\n\nOur lab have been investing much efforts in long-context LLMs lately. Particularly, we are interested in developing LLMs that can accept infinite input lengths, or what some of my colleagues call streaming language models (i.e., models that operate on streaming inputs). I believe that the transformer architecture is inherently incapable of processing infinite-length inputs. This is the research direction that I have been focusing on.\n\n### Inherent Limitations of Transformers\n\nThe most obvious reason is the quadratic complexity. A large number of research papers have focused on reducing the computational cost of the self-attention mechanism. But most SOTA LLMs at the moment are still dense attention layers, and rely on using Flash-Attention to speedup the computation. However, through discussion with various researchers, I have found that many people now believe that the attention computation is fast enough for most practical applications. I strongly disagree with this view. Firstly, the computational cost translates to the operational cost and emission that results from the usage of LLMs, which is of great concern. Secondly, my experience with ChatGPT (especially when using GPT-4) is that it is often not fast enough. Especially when it tends to produce many irrelevant lead-up sentences, I often find that I can find the answer using search engines before ChatGPT gives me the answer. Thirdly, current LLMs typically only have less than 100k in context length, however, as we apply them on contexts with millions of tokens, the speed of processing these tokens becomes unacceptable in most applications. For instance, in our experiments with InfiniteBench, applying a 7B model on 128k tokens using one A100 GPU takes 8~11 minutes to simply read the input^[I know that this is much faster than humans, but we expect AI to be faster than humans, especially considering they cost so much power to run.]. \n\n> This is not the only drawback of the transformer architecture, but that is out of the scope of this discussion.\n\n### Possible Paths\n\nI do not believe making small tweaks to the self-attention mechanism will solve the problem. Yep, we need new model architectures. Two architectures that I find promising are **linear attention** and **state-space models** (SSMs). Since these architectures have been widely discussed in the research community, I will not describe them in detail here. Instead, I want to express my opinion on the future of these architectures.\n\nI like to think of different language model architectures from the perspective of compressing the history^[This perspective is not new and has been discussed in many papers.]. The way transformers work is that they feed the last $L$ tokens without any compression to the model in each step. This means that the model remembers everything perfectly up to $L$ previous tokens, and remembers **nothing** about the history before that.\n\nIn contrast, SSM and models with linear attention can learn to automatically choose what information retain about the past, which more closely resembles how humans memorize, and provides a smoother curve of forgettance (which is likely beneficial because I think that a blurry remembrance is much better than complete forgettal beyond $n$ tokens). I firmly believe that this is the right direction to go. We are very likely to see a surge of LLMs with $O(1)$ inference cost (for one token) in the upcoming five years, and this can drastically reduce the computational costs and increase their applicability in real-world applications.\n\nBy now, some poeple are urgent to say that \"but recurrent models are much weaker than transformers\". The thing is, most of such comparison are done in settings where the input does not exceed the context window of the transformer models. In other words, we only evaluate transformers on cases where it has perfect memory. In fact, I believe that within the context windows of a transformer model, it should be the upper bounds for the performance of recurrent models, which holds a lossful compression of the window. Moreover, I think that further research down the line can drastically improve the performance of recurrent models (actually any possible linear language models) over self-attention-based language models.\n\nAnother thing to note is that, I have noticed that people like to align the parameter count of different LLMs during comparison, but for models with different architecture, this is a bad practice. In practice, we likely care more about the cost of maintenance, the speed of inference or training, and memory usage, etc. For instance, [RetNet](https://arxiv.org/abs/2307.08621)'s training throughput is actually faster than a transformer with [Flash-Attention](https://github.com/Dao-AILab/flash-attention). Imagine how fast RetNet + Flash-Attention can be. For applications, if a model is 10x faster than ChatGPT, but just slightly underperforms it, it is very likely that I will choose that over ChatGPT.\n\n## Additional Notes\n\nI am currently working on a linear attention model, but the field is changing so fast. I expect that this project will end within the next three months, because if not, my ideas will likely become obselete due to new works being released. Stay tuned.\n","source":"_posts/infinitebench.md","raw":"---\nauthor: é™ˆè‹±å‘ Yingfa Chen\ntitle: \"InfiniteBench: Extending Long Context Evaluation Beyond 100K Tokens\"\ndate: 2024-01-10 10:38:38\ncategories: Research\ntags:\n- research\n- llm\n- nlp\n- long-context\n- benchmark\n- recurrence\n- linear-attention\n- transformer\nfeatured: true\n---\n\n[Code](http://www.github.com/OpenBMB/InfiniteBench) | [Paper](https://arxiv.org/abs/2402.13718)\n\nThe first benchmark for evaluating the effectiveness of LLMs in handling more than 100k tokens!\n\n> In the paper, we name it $\\infty$-Bench, but I will sometimes use \"InfiniteBench\" in this blog post for better readability.\n\nFinally got some time to write this blog, been so busy lately! I have been in a fairly long duration of research hiatus, meanwhile the field of NLP has been revolutionized by an overwhelming number of new LLMs. Finally, I was able to arrive at some productive and meaningful work in this new era of research, as a second author. In this blog post, I will introduce this work that I have been working on recently.\n\n<!-- more -->\n\n## Background\n\nThe advent of LLMs have shown many promising results, but many practice applications (e.g., agents, document/webpage reading, long text summarization, etc.) are greatly limited by the context length constraint. Therefore, many works have strived to increase the length of the context that LLMs can accept. However, current \"long-sequence\" benchmarks all fall below 100k tokens, and is therefore not able to evaluate the effectiveness of many long-context LLMs. Our work, $\\infty$-Bench\n\n## The Data\n\nThe data consists of language tasks from diverse domains (math, code, novels), two languages (English and Chinese). Half of the tasks are automatically generated, which is desirable for optionally further scaling the context lengths to any arbitrary lengths.\n\nFollowing shows the statistics of the tasks in our benchmark.\n\n![Data statistics. The angle of segments is proportional to the number of examples, and the radius is proportional to the average example lengths (sum of input and output tokens).](infinitebench/data-stat-pie.png \"Data statistics. The angle of segments is proportional to the number of examples, and the radius is proportional to the average example lengths (sum of input and output tokens).\")\n\n## Results\n\nWe tested SOTA proprietary and open-source LLMs at the time of evaluation. The result is shown below. We can see that in most tasks, the performance is far from satisfactory in practical applications.\n\n![Results of some SOTA long-context LLMs on our InfiniteBench](infinitebench/results.png \"Results of some SOTA long-context LLMs on our InfiniteBench\")\n\n## Thoughts on the Future of Long-Context Research\n\nOur lab have been investing much efforts in long-context LLMs lately. Particularly, we are interested in developing LLMs that can accept infinite input lengths, or what some of my colleagues call streaming language models (i.e., models that operate on streaming inputs). I believe that the transformer architecture is inherently incapable of processing infinite-length inputs. This is the research direction that I have been focusing on.\n\n### Inherent Limitations of Transformers\n\nThe most obvious reason is the quadratic complexity. A large number of research papers have focused on reducing the computational cost of the self-attention mechanism. But most SOTA LLMs at the moment are still dense attention layers, and rely on using Flash-Attention to speedup the computation. However, through discussion with various researchers, I have found that many people now believe that the attention computation is fast enough for most practical applications. I strongly disagree with this view. Firstly, the computational cost translates to the operational cost and emission that results from the usage of LLMs, which is of great concern. Secondly, my experience with ChatGPT (especially when using GPT-4) is that it is often not fast enough. Especially when it tends to produce many irrelevant lead-up sentences, I often find that I can find the answer using search engines before ChatGPT gives me the answer. Thirdly, current LLMs typically only have less than 100k in context length, however, as we apply them on contexts with millions of tokens, the speed of processing these tokens becomes unacceptable in most applications. For instance, in our experiments with InfiniteBench, applying a 7B model on 128k tokens using one A100 GPU takes 8~11 minutes to simply read the input^[I know that this is much faster than humans, but we expect AI to be faster than humans, especially considering they cost so much power to run.]. \n\n> This is not the only drawback of the transformer architecture, but that is out of the scope of this discussion.\n\n### Possible Paths\n\nI do not believe making small tweaks to the self-attention mechanism will solve the problem. Yep, we need new model architectures. Two architectures that I find promising are **linear attention** and **state-space models** (SSMs). Since these architectures have been widely discussed in the research community, I will not describe them in detail here. Instead, I want to express my opinion on the future of these architectures.\n\nI like to think of different language model architectures from the perspective of compressing the history^[This perspective is not new and has been discussed in many papers.]. The way transformers work is that they feed the last $L$ tokens without any compression to the model in each step. This means that the model remembers everything perfectly up to $L$ previous tokens, and remembers **nothing** about the history before that.\n\nIn contrast, SSM and models with linear attention can learn to automatically choose what information retain about the past, which more closely resembles how humans memorize, and provides a smoother curve of forgettance (which is likely beneficial because I think that a blurry remembrance is much better than complete forgettal beyond $n$ tokens). I firmly believe that this is the right direction to go. We are very likely to see a surge of LLMs with $O(1)$ inference cost (for one token) in the upcoming five years, and this can drastically reduce the computational costs and increase their applicability in real-world applications.\n\nBy now, some poeple are urgent to say that \"but recurrent models are much weaker than transformers\". The thing is, most of such comparison are done in settings where the input does not exceed the context window of the transformer models. In other words, we only evaluate transformers on cases where it has perfect memory. In fact, I believe that within the context windows of a transformer model, it should be the upper bounds for the performance of recurrent models, which holds a lossful compression of the window. Moreover, I think that further research down the line can drastically improve the performance of recurrent models (actually any possible linear language models) over self-attention-based language models.\n\nAnother thing to note is that, I have noticed that people like to align the parameter count of different LLMs during comparison, but for models with different architecture, this is a bad practice. In practice, we likely care more about the cost of maintenance, the speed of inference or training, and memory usage, etc. For instance, [RetNet](https://arxiv.org/abs/2307.08621)'s training throughput is actually faster than a transformer with [Flash-Attention](https://github.com/Dao-AILab/flash-attention). Imagine how fast RetNet + Flash-Attention can be. For applications, if a model is 10x faster than ChatGPT, but just slightly underperforms it, it is very likely that I will choose that over ChatGPT.\n\n## Additional Notes\n\nI am currently working on a linear attention model, but the field is changing so fast. I expect that this project will end within the next three months, because if not, my ideas will likely become obselete due to new works being released. Stay tuned.\n","slug":"infinitebench","published":1,"updated":"2024-02-26T03:07:04.673Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cltl4oxeh0009xh7k6tym7gjd","content":"<p><a href=\"http://www.github.com/OpenBMB/InfiniteBench\">Code</a> | <a href=\"https://arxiv.org/abs/2402.13718\">Paper</a></p>\n<p>The first benchmark for evaluating the effectiveness of LLMs in handling more than 100k tokens!</p>\n<blockquote>\n<p>In the paper, we name it $\\infty$-Bench, but I will sometimes use \"InfiniteBench\" in this blog post for better readability.</p>\n</blockquote>\n<p>Finally got some time to write this blog, been so busy lately! I have been in a fairly long duration of research hiatus, meanwhile the field of NLP has been revolutionized by an overwhelming number of new LLMs. Finally, I was able to arrive at some productive and meaningful work in this new era of research, as a second author. In this blog post, I will introduce this work that I have been working on recently.</p>\n<span id=\"more\"></span>\n<h2 id=\"Background\">Background</h2>\n<p>The advent of LLMs have shown many promising results, but many practice applications (e.g., agents, document/webpage reading, long text summarization, etc.) are greatly limited by the context length constraint. Therefore, many works have strived to increase the length of the context that LLMs can accept. However, current \"long-sequence\" benchmarks all fall below 100k tokens, and is therefore not able to evaluate the effectiveness of many long-context LLMs. Our work, $\\infty$-Bench</p>\n<h2 id=\"The-Data\">The Data</h2>\n<p>The data consists of language tasks from diverse domains (math, code, novels), two languages (English and Chinese). Half of the tasks are automatically generated, which is desirable for optionally further scaling the context lengths to any arbitrary lengths.</p>\n<p>Following shows the statistics of the tasks in our benchmark.</p>\n<p><img src=\"/2024/01/10/infinitebench/data-stat-pie.png\" alt=\"Data statistics. The angle of segments is proportional to the number of examples, and the radius is proportional to the average example lengths (sum of input and output tokens).\" title=\"Data statistics. The angle of segments is proportional to the number of examples, and the radius is proportional to the average example lengths (sum of input and output tokens).\"></p>\n<h2 id=\"Results\">Results</h2>\n<p>We tested SOTA proprietary and open-source LLMs at the time of evaluation. The result is shown below. We can see that in most tasks, the performance is far from satisfactory in practical applications.</p>\n<p><img src=\"/2024/01/10/infinitebench/results.png\" alt=\"Results of some SOTA long-context LLMs on our InfiniteBench\" title=\"Results of some SOTA long-context LLMs on our InfiniteBench\"></p>\n<h2 id=\"Thoughts-on-the-Future-of-Long-Context-Research\">Thoughts on the Future of Long-Context Research</h2>\n<p>Our lab have been investing much efforts in long-context LLMs lately. Particularly, we are interested in developing LLMs that can accept infinite input lengths, or what some of my colleagues call streaming language models (i.e., models that operate on streaming inputs). I believe that the transformer architecture is inherently incapable of processing infinite-length inputs. This is the research direction that I have been focusing on.</p>\n<h3 id=\"Inherent-Limitations-of-Transformers\">Inherent Limitations of Transformers</h3>\n<p>The most obvious reason is the quadratic complexity. A large number of research papers have focused on reducing the computational cost of the self-attention mechanism. But most SOTA LLMs at the moment are still dense attention layers, and rely on using Flash-Attention to speedup the computation. However, through discussion with various researchers, I have found that many people now believe that the attention computation is fast enough for most practical applications. I strongly disagree with this view. Firstly, the computational cost translates to the operational cost and emission that results from the usage of LLMs, which is of great concern. Secondly, my experience with ChatGPT (especially when using GPT-4) is that it is often not fast enough. Especially when it tends to produce many irrelevant lead-up sentences, I often find that I can find the answer using search engines before ChatGPT gives me the answer. Thirdly, current LLMs typically only have less than 100k in context length, however, as we apply them on contexts with millions of tokens, the speed of processing these tokens becomes unacceptable in most applications. For instance, in our experiments with InfiniteBench, applying a 7B model on 128k tokens using one A100 GPU takes 8~11 minutes to simply read the input<sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">[1]</a></sup>.</p>\n<blockquote>\n<p>This is not the only drawback of the transformer architecture, but that is out of the scope of this discussion.</p>\n</blockquote>\n<h3 id=\"Possible-Paths\">Possible Paths</h3>\n<p>I do not believe making small tweaks to the self-attention mechanism will solve the problem. Yep, we need new model architectures. Two architectures that I find promising are <strong>linear attention</strong> and <strong>state-space models</strong> (SSMs). Since these architectures have been widely discussed in the research community, I will not describe them in detail here. Instead, I want to express my opinion on the future of these architectures.</p>\n<p>I like to think of different language model architectures from the perspective of compressing the history<sup class=\"footnote-ref\"><a href=\"#fn2\" id=\"fnref2\">[2]</a></sup>. The way transformers work is that they feed the last $L$ tokens without any compression to the model in each step. This means that the model remembers everything perfectly up to $L$ previous tokens, and remembers <strong>nothing</strong> about the history before that.</p>\n<p>In contrast, SSM and models with linear attention can learn to automatically choose what information retain about the past, which more closely resembles how humans memorize, and provides a smoother curve of forgettance (which is likely beneficial because I think that a blurry remembrance is much better than complete forgettal beyond $n$ tokens). I firmly believe that this is the right direction to go. We are very likely to see a surge of LLMs with $O(1)$ inference cost (for one token) in the upcoming five years, and this can drastically reduce the computational costs and increase their applicability in real-world applications.</p>\n<p>By now, some poeple are urgent to say that \"but recurrent models are much weaker than transformers\". The thing is, most of such comparison are done in settings where the input does not exceed the context window of the transformer models. In other words, we only evaluate transformers on cases where it has perfect memory. In fact, I believe that within the context windows of a transformer model, it should be the upper bounds for the performance of recurrent models, which holds a lossful compression of the window. Moreover, I think that further research down the line can drastically improve the performance of recurrent models (actually any possible linear language models) over self-attention-based language models.</p>\n<p>Another thing to note is that, I have noticed that people like to align the parameter count of different LLMs during comparison, but for models with different architecture, this is a bad practice. In practice, we likely care more about the cost of maintenance, the speed of inference or training, and memory usage, etc. For instance, <a href=\"https://arxiv.org/abs/2307.08621\">RetNet</a>'s training throughput is actually faster than a transformer with <a href=\"https://github.com/Dao-AILab/flash-attention\">Flash-Attention</a>. Imagine how fast RetNet + Flash-Attention can be. For applications, if a model is 10x faster than ChatGPT, but just slightly underperforms it, it is very likely that I will choose that over ChatGPT.</p>\n<h2 id=\"Additional-Notes\">Additional Notes</h2>\n<p>I am currently working on a linear attention model, but the field is changing so fast. I expect that this project will end within the next three months, because if not, my ideas will likely become obselete due to new works being released. Stay tuned.</p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn1\" class=\"footnote-item\"><p>I know that this is much faster than humans, but we expect AI to be faster than humans, especially considering they cost so much power to run. <a href=\"#fnref1\" class=\"footnote-backref\">â†©ï¸</a></p>\n</li>\n<li id=\"fn2\" class=\"footnote-item\"><p>This perspective is not new and has been discussed in many papers. <a href=\"#fnref2\" class=\"footnote-backref\">â†©ï¸</a></p>\n</li>\n</ol>\n</section>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://www.github.com/OpenBMB/InfiniteBench\">Code</a> | <a href=\"https://arxiv.org/abs/2402.13718\">Paper</a></p>\n<p>The first benchmark for evaluating the effectiveness of LLMs in handling more than 100k tokens!</p>\n<blockquote>\n<p>In the paper, we name it $\\infty$-Bench, but I will sometimes use \"InfiniteBench\" in this blog post for better readability.</p>\n</blockquote>\n<p>Finally got some time to write this blog, been so busy lately! I have been in a fairly long duration of research hiatus, meanwhile the field of NLP has been revolutionized by an overwhelming number of new LLMs. Finally, I was able to arrive at some productive and meaningful work in this new era of research, as a second author. In this blog post, I will introduce this work that I have been working on recently.</p>","more":"<h2 id=\"Background\">Background</h2>\n<p>The advent of LLMs have shown many promising results, but many practice applications (e.g., agents, document/webpage reading, long text summarization, etc.) are greatly limited by the context length constraint. Therefore, many works have strived to increase the length of the context that LLMs can accept. However, current &quot;long-sequence&quot; benchmarks all fall below 100k tokens, and is therefore not able to evaluate the effectiveness of many long-context LLMs. Our work, $\\infty$-Bench</p>\n<h2 id=\"The-Data\">The Data</h2>\n<p>The data consists of language tasks from diverse domains (math, code, novels), two languages (English and Chinese). Half of the tasks are automatically generated, which is desirable for optionally further scaling the context lengths to any arbitrary lengths.</p>\n<p>Following shows the statistics of the tasks in our benchmark.</p>\n<p><img src=\"/2024/01/10/infinitebench/data-stat-pie.png\" alt=\"Data statistics. The angle of segments is proportional to the number of examples, and the radius is proportional to the average example lengths (sum of input and output tokens).\" title=\"Data statistics. The angle of segments is proportional to the number of examples, and the radius is proportional to the average example lengths (sum of input and output tokens).\"></p>\n<h2 id=\"Results\">Results</h2>\n<p>We tested SOTA proprietary and open-source LLMs at the time of evaluation. The result is shown below. We can see that in most tasks, the performance is far from satisfactory in practical applications.</p>\n<p><img src=\"/2024/01/10/infinitebench/results.png\" alt=\"Results of some SOTA long-context LLMs on our InfiniteBench\" title=\"Results of some SOTA long-context LLMs on our InfiniteBench\"></p>\n<h2 id=\"Thoughts-on-the-Future-of-Long-Context-Research\">Thoughts on the Future of Long-Context Research</h2>\n<p>Our lab have been investing much efforts in long-context LLMs lately. Particularly, we are interested in developing LLMs that can accept infinite input lengths, or what some of my colleagues call streaming language models (i.e., models that operate on streaming inputs). I believe that the transformer architecture is inherently incapable of processing infinite-length inputs. This is the research direction that I have been focusing on.</p>\n<h3 id=\"Inherent-Limitations-of-Transformers\">Inherent Limitations of Transformers</h3>\n<p>The most obvious reason is the quadratic complexity. A large number of research papers have focused on reducing the computational cost of the self-attention mechanism. But most SOTA LLMs at the moment are still dense attention layers, and rely on using Flash-Attention to speedup the computation. However, through discussion with various researchers, I have found that many people now believe that the attention computation is fast enough for most practical applications. I strongly disagree with this view. Firstly, the computational cost translates to the operational cost and emission that results from the usage of LLMs, which is of great concern. Secondly, my experience with ChatGPT (especially when using GPT-4) is that it is often not fast enough. Especially when it tends to produce many irrelevant lead-up sentences, I often find that I can find the answer using search engines before ChatGPT gives me the answer. Thirdly, current LLMs typically only have less than 100k in context length, however, as we apply them on contexts with millions of tokens, the speed of processing these tokens becomes unacceptable in most applications. For instance, in our experiments with InfiniteBench, applying a 7B model on 128k tokens using one A100 GPU takes 8~11 minutes to simply read the input<sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">[1]</a></sup>.</p>\n<blockquote>\n<p>This is not the only drawback of the transformer architecture, but that is out of the scope of this discussion.</p>\n</blockquote>\n<h3 id=\"Possible-Paths\">Possible Paths</h3>\n<p>I do not believe making small tweaks to the self-attention mechanism will solve the problem. Yep, we need new model architectures. Two architectures that I find promising are <strong>linear attention</strong> and <strong>state-space models</strong> (SSMs). Since these architectures have been widely discussed in the research community, I will not describe them in detail here. Instead, I want to express my opinion on the future of these architectures.</p>\n<p>I like to think of different language model architectures from the perspective of compressing the history<sup class=\"footnote-ref\"><a href=\"#fn2\" id=\"fnref2\">[2]</a></sup>. The way transformers work is that they feed the last $L$ tokens without any compression to the model in each step. This means that the model remembers everything perfectly up to $L$ previous tokens, and remembers <strong>nothing</strong> about the history before that.</p>\n<p>In contrast, SSM and models with linear attention can learn to automatically choose what information retain about the past, which more closely resembles how humans memorize, and provides a smoother curve of forgettance (which is likely beneficial because I think that a blurry remembrance is much better than complete forgettal beyond $n$ tokens). I firmly believe that this is the right direction to go. We are very likely to see a surge of LLMs with $O(1)$ inference cost (for one token) in the upcoming five years, and this can drastically reduce the computational costs and increase their applicability in real-world applications.</p>\n<p>By now, some poeple are urgent to say that &quot;but recurrent models are much weaker than transformers&quot;. The thing is, most of such comparison are done in settings where the input does not exceed the context window of the transformer models. In other words, we only evaluate transformers on cases where it has perfect memory. In fact, I believe that within the context windows of a transformer model, it should be the upper bounds for the performance of recurrent models, which holds a lossful compression of the window. Moreover, I think that further research down the line can drastically improve the performance of recurrent models (actually any possible linear language models) over self-attention-based language models.</p>\n<p>Another thing to note is that, I have noticed that people like to align the parameter count of different LLMs during comparison, but for models with different architecture, this is a bad practice. In practice, we likely care more about the cost of maintenance, the speed of inference or training, and memory usage, etc. For instance, <a href=\"https://arxiv.org/abs/2307.08621\">RetNet</a>'s training throughput is actually faster than a transformer with <a href=\"https://github.com/Dao-AILab/flash-attention\">Flash-Attention</a>. Imagine how fast RetNet + Flash-Attention can be. For applications, if a model is 10x faster than ChatGPT, but just slightly underperforms it, it is very likely that I will choose that over ChatGPT.</p>\n<h2 id=\"Additional-Notes\">Additional Notes</h2>\n<p>I am currently working on a linear attention model, but the field is changing so fast. I expect that this project will end within the next three months, because if not, my ideas will likely become obselete due to new works being released. Stay tuned.</p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn1\" class=\"footnote-item\"><p>I know that this is much faster than humans, but we expect AI to be faster than humans, especially considering they cost so much power to run. <a href=\"#fnref1\" class=\"footnote-backref\">â†©ï¸</a></p>\n</li>\n<li id=\"fn2\" class=\"footnote-item\"><p>This perspective is not new and has been discussed in many papers. <a href=\"#fnref2\" class=\"footnote-backref\">â†©ï¸</a></p>\n</li>\n</ol>\n</section>"},{"author":"é™ˆè‹±å‘ Yingfa Chen","title":"Interpreting a Maze-Solving Network","date":"2023-10-07T10:03:10.000Z","_content":"\n[The blog post](https://www.lesswrong.com/s/sCGfFb5DPfjEmtEdn)\n\nI can't believe I haven't read this until now. This is mind-provoking, and the result is an important step towards understanding neural networks.\n\n<!-- more -->\n\nThe culmination of this blog post is the exciting work of [Activation Addition](/2023/10/07/actadd/), which I believe is one important work that inspired the recently [Representation Engineering](https://arxiv.org/abs/2310.01405) work.\n","source":"_posts/interpreting-a-maze-solving-network.md","raw":"---\nauthor: é™ˆè‹±å‘ Yingfa Chen\ntitle: Interpreting a Maze-Solving Network\ndate: 2023-10-07 18:03:10\ncategories: Thoughts\ntags:\n- english\n- activation-engineering\n- representation-engineering\n- interpretability\n- rl\n- alignment\n- llm\n- maze\n---\n\n[The blog post](https://www.lesswrong.com/s/sCGfFb5DPfjEmtEdn)\n\nI can't believe I haven't read this until now. This is mind-provoking, and the result is an important step towards understanding neural networks.\n\n<!-- more -->\n\nThe culmination of this blog post is the exciting work of [Activation Addition](/2023/10/07/actadd/), which I believe is one important work that inspired the recently [Representation Engineering](https://arxiv.org/abs/2310.01405) work.\n","slug":"interpreting-a-maze-solving-network","published":1,"updated":"2024-01-11T05:41:49.088Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cltl4oxeh000bxh7k5kpl1zp0","content":"<p><a href=\"https://www.lesswrong.com/s/sCGfFb5DPfjEmtEdn\">The blog post</a></p>\n<p>I can't believe I haven't read this until now. This is mind-provoking, and the result is an important step towards understanding neural networks.</p>\n<span id=\"more\"></span>\n<p>The culmination of this blog post is the exciting work of <a href=\"/2023/10/07/actadd/\">Activation Addition</a>, which I believe is one important work that inspired the recently <a href=\"https://arxiv.org/abs/2310.01405\">Representation Engineering</a> work.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"https://www.lesswrong.com/s/sCGfFb5DPfjEmtEdn\">The blog post</a></p>\n<p>I can't believe I haven't read this until now. This is mind-provoking, and the result is an important step towards understanding neural networks.</p>","more":"<p>The culmination of this blog post is the exciting work of <a href=\"/2023/10/07/actadd/\">Activation Addition</a>, which I believe is one important work that inspired the recently <a href=\"https://arxiv.org/abs/2310.01405\">Representation Engineering</a> work.</p>"},{"title":"Safety and Ethical Concerns of Large Language Models","date":"2023-09-19T10:13:06.000Z","author":"é™ˆè‹±å‘ Yingfa Chen","_content":"\nI will be holding a seminar at ModelBest (é¢å£æ™ºèƒ½) in Sep 20, 2023 in Beijing, Haidian, ç§‘æŠ€å›­. The seminar will be in Chinese, and it's called \"å¤§æ¨¡å‹å®‰å…¨ä¸ä¼¦ç†é—®é¢˜\" (translation: Safety and Ethical Concerns of Large Language Models). Below is a list of references.\n\n<!-- more -->\n\n## Introduction\n\n- Galactica: A Large Language Model for Science\n- https://openai.com/research/gpt-4\n- SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions\n- Bias and Fairness in Large Language Models: A Survey\n- A Survey of Safety and Trustworthiness of Large Language Models through the Lens of Verification and Validation\n\n## Evaluation Methods\n\n- A General Language Assistant as a Laboratory for Alignment, Anthropic\n- Safety Assessment of Chinese Large Language Models\n- Semantics derived automatically from language corpora contain human-like biases\n- StereoSet: Measuring stereotypical bias in pretrained language models\n\n### Instruction Attacks\n\n- Toxicity in CHATGPT: Analyzing Persona-assigned Language Models â­ï¸\n- Large Language Models are Zero-Shot Reasoners â­ï¸\n- On Second Thought, Letâ€™s Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning â­ï¸\n- Prompting GPT-3 To Be Reliable\n- Universal and Transferable Adversarial Attacks on Aligned Language Models â­ï¸\n- Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment â­ï¸â­ï¸\n\n### Exaggerated Safety\n\n- XSTEST: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models â­ï¸\n- Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions â­ï¸\n\n## Alignment Methods\n\n- Aligning language models to follow instructions â­ï¸\n- Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback â­ï¸\n- SELF-INSTRUCT: Aligning Language Models with Self-Generated Instructions â­ï¸â­ï¸\n- Pretraining Language Models with Human Preferences â­ï¸\n- LIMA: Less Is More for Alignment\n- https://openai.com/blog/our-approach-to-alignment-research (Aug 2022)\n- https://openai.com/blog/our-approach-to-alignment-research (Jul 2023) â­ï¸\n\n\nâ­ï¸: important\n\nâ­ï¸â­ï¸: very important\n\n## My Thoughts\n\nAI alignment is extremely important, and we know very little about it right now. In my everyday use of ChatGPT, it occasionally refuses to help me. This is presumably because it thinks that assisting me is harmful, while it's actually not. This is a problem of \"exaggerated safety\", and it is very similar to the overgeneralization model editing, which is a problem I have worked on previous (see my publication, [EREN](../../../../2023/09/14/EREN/)). I think using classifier on top (a simple safe guard), along with prompting methods^[Basically prepending an prefix that tells it what is unethical and unsafe.] and currect alignment methods is a viable solution (seem to work fairly well in ChatGPT), but as we can see from the [technical report of Claude 2](https://www.anthropic.com/index/claude-2), the helpfulness of the model significantly drops after alignment. Therefore, I think minimizing the sacrifice in helpfulness will be an important direction of future research.\n\nAnother concern is that there is no concensus on the goal of alignment. In fact, many people think that the fact that role-playing can be used to jailbreak alignment is not a bad thing per se, especially regarding toxicity, because if the user explicitly tells the AI to role-play a person that slurs a lot, the user expects slurs (one kind of toxicity).\n\nMoreover, the entire meaning of alignment research might be undermined by the fact that AI system can be unaligned pretty easily. This concern is specially severe for works that focus on reducing the cost of alignment, because the same techniques might be used to effectively unalign AI systems.^[Does there exist a way to make AI impossible to unalign? This reminds me of the \"mind stamping\" (Chinese: æ€æƒ³é’¢å°) from the Three Body Problem, a novel by Liu Cixin.]\n\nAll in all, AI alignment is a sub-field of better controllability of AI system, and I can foresee that it will be a hot research topic for the upcoming five years.\n","source":"_posts/llm-safety-and-ethics.md","raw":"---\ntitle: \"Safety and Ethical Concerns of Large Language Models\"\ndate: 2023-09-19 18:13:06\ncategories: Thoughts\nauthor: é™ˆè‹±å‘ Yingfa Chen\ntags:\n- english\n- research\n- llm\n- machine-learning\n- ethics\n- safety\n- ai-alignment\n- é¢å£æ™ºèƒ½\n- modelbest\n- life\n- tutorial\n- eren\n- chatgpt\n- claude\n- ä¸‰ä½“\n---\n\nI will be holding a seminar at ModelBest (é¢å£æ™ºèƒ½) in Sep 20, 2023 in Beijing, Haidian, ç§‘æŠ€å›­. The seminar will be in Chinese, and it's called \"å¤§æ¨¡å‹å®‰å…¨ä¸ä¼¦ç†é—®é¢˜\" (translation: Safety and Ethical Concerns of Large Language Models). Below is a list of references.\n\n<!-- more -->\n\n## Introduction\n\n- Galactica: A Large Language Model for Science\n- https://openai.com/research/gpt-4\n- SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions\n- Bias and Fairness in Large Language Models: A Survey\n- A Survey of Safety and Trustworthiness of Large Language Models through the Lens of Verification and Validation\n\n## Evaluation Methods\n\n- A General Language Assistant as a Laboratory for Alignment, Anthropic\n- Safety Assessment of Chinese Large Language Models\n- Semantics derived automatically from language corpora contain human-like biases\n- StereoSet: Measuring stereotypical bias in pretrained language models\n\n### Instruction Attacks\n\n- Toxicity in CHATGPT: Analyzing Persona-assigned Language Models â­ï¸\n- Large Language Models are Zero-Shot Reasoners â­ï¸\n- On Second Thought, Letâ€™s Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning â­ï¸\n- Prompting GPT-3 To Be Reliable\n- Universal and Transferable Adversarial Attacks on Aligned Language Models â­ï¸\n- Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment â­ï¸â­ï¸\n\n### Exaggerated Safety\n\n- XSTEST: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models â­ï¸\n- Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions â­ï¸\n\n## Alignment Methods\n\n- Aligning language models to follow instructions â­ï¸\n- Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback â­ï¸\n- SELF-INSTRUCT: Aligning Language Models with Self-Generated Instructions â­ï¸â­ï¸\n- Pretraining Language Models with Human Preferences â­ï¸\n- LIMA: Less Is More for Alignment\n- https://openai.com/blog/our-approach-to-alignment-research (Aug 2022)\n- https://openai.com/blog/our-approach-to-alignment-research (Jul 2023) â­ï¸\n\n\nâ­ï¸: important\n\nâ­ï¸â­ï¸: very important\n\n## My Thoughts\n\nAI alignment is extremely important, and we know very little about it right now. In my everyday use of ChatGPT, it occasionally refuses to help me. This is presumably because it thinks that assisting me is harmful, while it's actually not. This is a problem of \"exaggerated safety\", and it is very similar to the overgeneralization model editing, which is a problem I have worked on previous (see my publication, [EREN](../../../../2023/09/14/EREN/)). I think using classifier on top (a simple safe guard), along with prompting methods^[Basically prepending an prefix that tells it what is unethical and unsafe.] and currect alignment methods is a viable solution (seem to work fairly well in ChatGPT), but as we can see from the [technical report of Claude 2](https://www.anthropic.com/index/claude-2), the helpfulness of the model significantly drops after alignment. Therefore, I think minimizing the sacrifice in helpfulness will be an important direction of future research.\n\nAnother concern is that there is no concensus on the goal of alignment. In fact, many people think that the fact that role-playing can be used to jailbreak alignment is not a bad thing per se, especially regarding toxicity, because if the user explicitly tells the AI to role-play a person that slurs a lot, the user expects slurs (one kind of toxicity).\n\nMoreover, the entire meaning of alignment research might be undermined by the fact that AI system can be unaligned pretty easily. This concern is specially severe for works that focus on reducing the cost of alignment, because the same techniques might be used to effectively unalign AI systems.^[Does there exist a way to make AI impossible to unalign? This reminds me of the \"mind stamping\" (Chinese: æ€æƒ³é’¢å°) from the Three Body Problem, a novel by Liu Cixin.]\n\nAll in all, AI alignment is a sub-field of better controllability of AI system, and I can foresee that it will be a hot research topic for the upcoming five years.\n","slug":"llm-safety-and-ethics","published":1,"updated":"2024-01-10T07:24:56.515Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cltl4oxei000fxh7k1qa8gbtd","content":"<p>I will be holding a seminar at ModelBest (é¢å£æ™ºèƒ½) in Sep 20, 2023 in Beijing, Haidian, ç§‘æŠ€å›­. The seminar will be in Chinese, and it's called \"å¤§æ¨¡å‹å®‰å…¨ä¸ä¼¦ç†é—®é¢˜\" (translation: Safety and Ethical Concerns of Large Language Models). Below is a list of references.</p>\n<span id=\"more\"></span>\n<h2 id=\"Introduction\">Introduction</h2>\n<ul>\n<li>Galactica: A Large Language Model for Science</li>\n<li><a href=\"https://openai.com/research/gpt-4\">https://openai.com/research/gpt-4</a></li>\n<li>SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions</li>\n<li>Bias and Fairness in Large Language Models: A Survey</li>\n<li>A Survey of Safety and Trustworthiness of Large Language Models through the Lens of Verification and Validation</li>\n</ul>\n<h2 id=\"Evaluation-Methods\">Evaluation Methods</h2>\n<ul>\n<li>A General Language Assistant as a Laboratory for Alignment, Anthropic</li>\n<li>Safety Assessment of Chinese Large Language Models</li>\n<li>Semantics derived automatically from language corpora contain human-like biases</li>\n<li>StereoSet: Measuring stereotypical bias in pretrained language models</li>\n</ul>\n<h3 id=\"Instruction-Attacks\">Instruction Attacks</h3>\n<ul>\n<li>Toxicity in CHATGPT: Analyzing Persona-assigned Language Models â­ï¸</li>\n<li>Large Language Models are Zero-Shot Reasoners â­ï¸</li>\n<li>On Second Thought, Letâ€™s Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning â­ï¸</li>\n<li>Prompting GPT-3 To Be Reliable</li>\n<li>Universal and Transferable Adversarial Attacks on Aligned Language Models â­ï¸</li>\n<li>Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment â­ï¸â­ï¸</li>\n</ul>\n<h3 id=\"Exaggerated-Safety\">Exaggerated Safety</h3>\n<ul>\n<li>XSTEST: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models â­ï¸</li>\n<li>Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions â­ï¸</li>\n</ul>\n<h2 id=\"Alignment-Methods\">Alignment Methods</h2>\n<ul>\n<li>Aligning language models to follow instructions â­ï¸</li>\n<li>Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback â­ï¸</li>\n<li>SELF-INSTRUCT: Aligning Language Models with Self-Generated Instructions â­ï¸â­ï¸</li>\n<li>Pretraining Language Models with Human Preferences â­ï¸</li>\n<li>LIMA: Less Is More for Alignment</li>\n<li><a href=\"https://openai.com/blog/our-approach-to-alignment-research\">https://openai.com/blog/our-approach-to-alignment-research</a> (Aug 2022)</li>\n<li><a href=\"https://openai.com/blog/our-approach-to-alignment-research\">https://openai.com/blog/our-approach-to-alignment-research</a> (Jul 2023) â­ï¸</li>\n</ul>\n<p>â­ï¸: important</p>\n<p>â­ï¸â­ï¸: very important</p>\n<h2 id=\"My-Thoughts\">My Thoughts</h2>\n<p>AI alignment is extremely important, and we know very little about it right now. In my everyday use of ChatGPT, it occasionally refuses to help me. This is presumably because it thinks that assisting me is harmful, while it's actually not. This is a problem of \"exaggerated safety\", and it is very similar to the overgeneralization model editing, which is a problem I have worked on previous (see my publication, <a href=\"../../../../2023/09/14/EREN/\">EREN</a>). I think using classifier on top (a simple safe guard), along with prompting methods<sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">[1]</a></sup> and currect alignment methods is a viable solution (seem to work fairly well in ChatGPT), but as we can see from the <a href=\"https://www.anthropic.com/index/claude-2\">technical report of Claude 2</a>, the helpfulness of the model significantly drops after alignment. Therefore, I think minimizing the sacrifice in helpfulness will be an important direction of future research.</p>\n<p>Another concern is that there is no concensus on the goal of alignment. In fact, many people think that the fact that role-playing can be used to jailbreak alignment is not a bad thing per se, especially regarding toxicity, because if the user explicitly tells the AI to role-play a person that slurs a lot, the user expects slurs (one kind of toxicity).</p>\n<p>Moreover, the entire meaning of alignment research might be undermined by the fact that AI system can be unaligned pretty easily. This concern is specially severe for works that focus on reducing the cost of alignment, because the same techniques might be used to effectively unalign AI systems.<sup class=\"footnote-ref\"><a href=\"#fn2\" id=\"fnref2\">[2]</a></sup></p>\n<p>All in all, AI alignment is a sub-field of better controllability of AI system, and I can foresee that it will be a hot research topic for the upcoming five years.</p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn1\" class=\"footnote-item\"><p>Basically prepending an prefix that tells it what is unethical and unsafe. <a href=\"#fnref1\" class=\"footnote-backref\">â†©ï¸</a></p>\n</li>\n<li id=\"fn2\" class=\"footnote-item\"><p>Does there exist a way to make AI impossible to unalign? This reminds me of the \"mind stamping\" (Chinese: æ€æƒ³é’¢å°) from the Three Body Problem, a novel by Liu Cixin. <a href=\"#fnref2\" class=\"footnote-backref\">â†©ï¸</a></p>\n</li>\n</ol>\n</section>\n","site":{"data":{}},"excerpt":"<p>I will be holding a seminar at ModelBest (é¢å£æ™ºèƒ½) in Sep 20, 2023 in Beijing, Haidian, ç§‘æŠ€å›­. The seminar will be in Chinese, and it's called \"å¤§æ¨¡å‹å®‰å…¨ä¸ä¼¦ç†é—®é¢˜\" (translation: Safety and Ethical Concerns of Large Language Models). Below is a list of references.</p>","more":"<h2 id=\"Introduction\">Introduction</h2>\n<ul>\n<li>Galactica: A Large Language Model for Science</li>\n<li><a href=\"https://openai.com/research/gpt-4\">https://openai.com/research/gpt-4</a></li>\n<li>SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions</li>\n<li>Bias and Fairness in Large Language Models: A Survey</li>\n<li>A Survey of Safety and Trustworthiness of Large Language Models through the Lens of Verification and Validation</li>\n</ul>\n<h2 id=\"Evaluation-Methods\">Evaluation Methods</h2>\n<ul>\n<li>A General Language Assistant as a Laboratory for Alignment, Anthropic</li>\n<li>Safety Assessment of Chinese Large Language Models</li>\n<li>Semantics derived automatically from language corpora contain human-like biases</li>\n<li>StereoSet: Measuring stereotypical bias in pretrained language models</li>\n</ul>\n<h3 id=\"Instruction-Attacks\">Instruction Attacks</h3>\n<ul>\n<li>Toxicity in CHATGPT: Analyzing Persona-assigned Language Models â­ï¸</li>\n<li>Large Language Models are Zero-Shot Reasoners â­ï¸</li>\n<li>On Second Thought, Letâ€™s Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning â­ï¸</li>\n<li>Prompting GPT-3 To Be Reliable</li>\n<li>Universal and Transferable Adversarial Attacks on Aligned Language Models â­ï¸</li>\n<li>Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment â­ï¸â­ï¸</li>\n</ul>\n<h3 id=\"Exaggerated-Safety\">Exaggerated Safety</h3>\n<ul>\n<li>XSTEST: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models â­ï¸</li>\n<li>Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions â­ï¸</li>\n</ul>\n<h2 id=\"Alignment-Methods\">Alignment Methods</h2>\n<ul>\n<li>Aligning language models to follow instructions â­ï¸</li>\n<li>Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback â­ï¸</li>\n<li>SELF-INSTRUCT: Aligning Language Models with Self-Generated Instructions â­ï¸â­ï¸</li>\n<li>Pretraining Language Models with Human Preferences â­ï¸</li>\n<li>LIMA: Less Is More for Alignment</li>\n<li><a href=\"https://openai.com/blog/our-approach-to-alignment-research\">https://openai.com/blog/our-approach-to-alignment-research</a> (Aug 2022)</li>\n<li><a href=\"https://openai.com/blog/our-approach-to-alignment-research\">https://openai.com/blog/our-approach-to-alignment-research</a> (Jul 2023) â­ï¸</li>\n</ul>\n<p>â­ï¸: important</p>\n<p>â­ï¸â­ï¸: very important</p>\n<h2 id=\"My-Thoughts\">My Thoughts</h2>\n<p>AI alignment is extremely important, and we know very little about it right now. In my everyday use of ChatGPT, it occasionally refuses to help me. This is presumably because it thinks that assisting me is harmful, while it's actually not. This is a problem of &quot;exaggerated safety&quot;, and it is very similar to the overgeneralization model editing, which is a problem I have worked on previous (see my publication, <a href=\"../../../../2023/09/14/EREN/\">EREN</a>). I think using classifier on top (a simple safe guard), along with prompting methods<sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">[1]</a></sup> and currect alignment methods is a viable solution (seem to work fairly well in ChatGPT), but as we can see from the <a href=\"https://www.anthropic.com/index/claude-2\">technical report of Claude 2</a>, the helpfulness of the model significantly drops after alignment. Therefore, I think minimizing the sacrifice in helpfulness will be an important direction of future research.</p>\n<p>Another concern is that there is no concensus on the goal of alignment. In fact, many people think that the fact that role-playing can be used to jailbreak alignment is not a bad thing per se, especially regarding toxicity, because if the user explicitly tells the AI to role-play a person that slurs a lot, the user expects slurs (one kind of toxicity).</p>\n<p>Moreover, the entire meaning of alignment research might be undermined by the fact that AI system can be unaligned pretty easily. This concern is specially severe for works that focus on reducing the cost of alignment, because the same techniques might be used to effectively unalign AI systems.<sup class=\"footnote-ref\"><a href=\"#fn2\" id=\"fnref2\">[2]</a></sup></p>\n<p>All in all, AI alignment is a sub-field of better controllability of AI system, and I can foresee that it will be a hot research topic for the upcoming five years.</p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn1\" class=\"footnote-item\"><p>Basically prepending an prefix that tells it what is unethical and unsafe. <a href=\"#fnref1\" class=\"footnote-backref\">â†©ï¸</a></p>\n</li>\n<li id=\"fn2\" class=\"footnote-item\"><p>Does there exist a way to make AI impossible to unalign? This reminds me of the &quot;mind stamping&quot; (Chinese: æ€æƒ³é’¢å°) from the Three Body Problem, a novel by Liu Cixin. <a href=\"#fnref2\" class=\"footnote-backref\">â†©ï¸</a></p>\n</li>\n</ol>\n</section>"},{"title":"Some Binary Search","date":"2023-09-14T11:31:09.000Z","_content":"\nA binary search with C++:\n\n```c++\ntemplate<class T>\nint bin_search(vector<T>& arr, T target) {\n    int left = 0, right = arr.size() - 1;\n    while (left <= right) {\n        int mid = (left + right) / 2;\n        if (arr[mid] == target) {\n            break;\n        } else if (arr[mid] < target) {\n            left = mid + 1;\n        } else {\n            right = mid - 1;\n        }\n    }\n    return left;\n}\n```\n\nThe same thing with Rust:\n\n```rust\nfn bin_search<T: Ord>(arr: &Vec<T>, target: &T) -> usize {\n    let mut left = 0;\n    let mut right = arr.len() - 1;\n    while left <= right {\n        let mid = (left + right) / 2;\n        if arr[mid] == *target {\n            break;\n        } else if arr[mid] < *target {\n            left = mid + 1;\n        } else {\n            right = mid - 1;\n        }\n    }\n    left\n}\n```\n\nAnd with Python:\n\n```python\ndef bin_search(arr: list, target):\n    left = 0\n    right = len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] == target:\n            break\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return left\n```","source":"_posts/some_binary_search.md","raw":"---\ntitle: Some Binary Search\ndate: 2023-09-14 19:31:09\ncategories: Test\ntags:\n- algorithm\n- binary-search\n- rust\n- python\n- c++\n- test\n- english\n- code\n---\n\nA binary search with C++:\n\n```c++\ntemplate<class T>\nint bin_search(vector<T>& arr, T target) {\n    int left = 0, right = arr.size() - 1;\n    while (left <= right) {\n        int mid = (left + right) / 2;\n        if (arr[mid] == target) {\n            break;\n        } else if (arr[mid] < target) {\n            left = mid + 1;\n        } else {\n            right = mid - 1;\n        }\n    }\n    return left;\n}\n```\n\nThe same thing with Rust:\n\n```rust\nfn bin_search<T: Ord>(arr: &Vec<T>, target: &T) -> usize {\n    let mut left = 0;\n    let mut right = arr.len() - 1;\n    while left <= right {\n        let mid = (left + right) / 2;\n        if arr[mid] == *target {\n            break;\n        } else if arr[mid] < *target {\n            left = mid + 1;\n        } else {\n            right = mid - 1;\n        }\n    }\n    left\n}\n```\n\nAnd with Python:\n\n```python\ndef bin_search(arr: list, target):\n    left = 0\n    right = len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] == target:\n            break\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return left\n```","slug":"some_binary_search","published":1,"updated":"2024-01-11T04:59:28.107Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cltl4oxei000hxh7k0xuo6ggs","content":"<p>A binary search with C++:</p>\n<figure class=\"highlight c++\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">class</span> T&gt;</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">bin_search</span><span class=\"params\">(vector&lt;T&gt;&amp; arr, T target)</span> </span>{</span><br><span class=\"line\">    <span class=\"type\">int</span> left = <span class=\"number\">0</span>, right = arr.<span class=\"built_in\">size</span>() - <span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (left &lt;= right) {</span><br><span class=\"line\">        <span class=\"type\">int</span> mid = (left + right) / <span class=\"number\">2</span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (arr[mid] == target) {</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        } <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (arr[mid] &lt; target) {</span><br><span class=\"line\">            left = mid + <span class=\"number\">1</span>;</span><br><span class=\"line\">        } <span class=\"keyword\">else</span> {</span><br><span class=\"line\">            right = mid - <span class=\"number\">1</span>;</span><br><span class=\"line\">        }</span><br><span class=\"line\">    }</span><br><span class=\"line\">    <span class=\"keyword\">return</span> left;</span><br><span class=\"line\">}</span><br></pre></td></tr></tbody></table></figure>\n<p>The same thing with Rust:</p>\n<figure class=\"highlight rust\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">fn</span> <span class=\"title function_\">bin_search</span>&lt;T: <span class=\"built_in\">Ord</span>&gt;(arr: &amp;<span class=\"type\">Vec</span>&lt;T&gt;, target: &amp;T) <span class=\"punctuation\">-&gt;</span> <span class=\"type\">usize</span> {</span><br><span class=\"line\">    <span class=\"keyword\">let</span> <span class=\"keyword\">mut </span><span class=\"variable\">left</span> = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">let</span> <span class=\"keyword\">mut </span><span class=\"variable\">right</span> = arr.<span class=\"title function_ invoke__\">len</span>() - <span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> left &lt;= right {</span><br><span class=\"line\">        <span class=\"keyword\">let</span> <span class=\"variable\">mid</span> = (left + right) / <span class=\"number\">2</span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> arr[mid] == *target {</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        } <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> arr[mid] &lt; *target {</span><br><span class=\"line\">            left = mid + <span class=\"number\">1</span>;</span><br><span class=\"line\">        } <span class=\"keyword\">else</span> {</span><br><span class=\"line\">            right = mid - <span class=\"number\">1</span>;</span><br><span class=\"line\">        }</span><br><span class=\"line\">    }</span><br><span class=\"line\">    left</span><br><span class=\"line\">}</span><br></pre></td></tr></tbody></table></figure>\n<p>And with Python:</p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">bin_search</span>(<span class=\"params\">arr: <span class=\"built_in\">list</span>, target</span>):</span><br><span class=\"line\">    left = <span class=\"number\">0</span></span><br><span class=\"line\">    right = <span class=\"built_in\">len</span>(arr) - <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span> left &lt;= right:</span><br><span class=\"line\">        mid = (left + right) // <span class=\"number\">2</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> arr[mid] == target:</span><br><span class=\"line\">            <span class=\"keyword\">break</span></span><br><span class=\"line\">        <span class=\"keyword\">elif</span> arr[mid] &lt; target:</span><br><span class=\"line\">            left = mid + <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            right = mid - <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> left</span><br></pre></td></tr></tbody></table></figure>","site":{"data":{}},"excerpt":"","more":"<p>A binary search with C++:</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">class</span> T&gt;</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">bin_search</span><span class=\"params\">(vector&lt;T&gt;&amp; arr, T target)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> left = <span class=\"number\">0</span>, right = arr.<span class=\"built_in\">size</span>() - <span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (left &lt;= right) &#123;</span><br><span class=\"line\">        <span class=\"type\">int</span> mid = (left + right) / <span class=\"number\">2</span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (arr[mid] == target) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (arr[mid] &lt; target) &#123;</span><br><span class=\"line\">            left = mid + <span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">            right = mid - <span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> left;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>The same thing with Rust:</p>\n<figure class=\"highlight rust\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">fn</span> <span class=\"title function_\">bin_search</span>&lt;T: <span class=\"built_in\">Ord</span>&gt;(arr: &amp;<span class=\"type\">Vec</span>&lt;T&gt;, target: &amp;T) <span class=\"punctuation\">-&gt;</span> <span class=\"type\">usize</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">let</span> <span class=\"keyword\">mut </span><span class=\"variable\">left</span> = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">let</span> <span class=\"keyword\">mut </span><span class=\"variable\">right</span> = arr.<span class=\"title function_ invoke__\">len</span>() - <span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> left &lt;= right &#123;</span><br><span class=\"line\">        <span class=\"keyword\">let</span> <span class=\"variable\">mid</span> = (left + right) / <span class=\"number\">2</span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> arr[mid] == *target &#123;</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> arr[mid] &lt; *target &#123;</span><br><span class=\"line\">            left = mid + <span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">            right = mid - <span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    left</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>And with Python:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">bin_search</span>(<span class=\"params\">arr: <span class=\"built_in\">list</span>, target</span>):</span><br><span class=\"line\">    left = <span class=\"number\">0</span></span><br><span class=\"line\">    right = <span class=\"built_in\">len</span>(arr) - <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span> left &lt;= right:</span><br><span class=\"line\">        mid = (left + right) // <span class=\"number\">2</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> arr[mid] == target:</span><br><span class=\"line\">            <span class=\"keyword\">break</span></span><br><span class=\"line\">        <span class=\"keyword\">elif</span> arr[mid] &lt; target:</span><br><span class=\"line\">            left = mid + <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            right = mid - <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> left</span><br></pre></td></tr></table></figure>"},{"author":"Chen Yingfa","title":"ä¸´è¿‘2023æš‘å‡ï¼Œ00å¸ˆå§ç­”è¾©ï¼Œæ™šä¸Šæ‰“çƒ","date":"2023-05-18T11:12:19.000Z","_content":"\nä»Šå¤©ç¡åˆ°ä¹ç‚¹æ‰é†’æ¥ï¼Œè¿˜æ˜¯è¢«00æ‰“ç”µè¯å«é†’çš„ã€‚å»è¿‡äº†ä¸ªæ—©ï¼Œç„¶åå»ä¸Šã€Šæ·±åº¦å­¦ä¹ ã€‹ã€‚\n\nä»Šå¤©00çš„å¸ˆå§ç­”è¾©ï¼Œä¸‹åˆä¸‰ç‚¹å»äº†ï¼Œå½“æ—¶æˆ‘åœ¨ç¡åˆè§‰ã€‚æ„Ÿè§‰ä¹‹åå¥¹æœ‰ç‚¹emoï¼Œä½†æ˜¯å¥¹ä¸æ‰¿è®¤ï¼Œä¸çŸ¥é“ä¸ºä»€ä¹ˆã€‚ç„¶åèŠäº†å¾ˆå¤šå…³äºæœªæ¥ï¼Œç»“å©šã€ç”Ÿå­©å­ã€æ‰¾å·¥ä½œç­‰äº‹æƒ…ã€‚æ„Ÿè§‰ä¹Ÿæ²¡æœ‰å¾ˆå¤§çš„é—®é¢˜ï¼Œä½†æ˜¯00æ€»æ˜¯æŠŠä¸œè¥¿çœ‹å¾—å¾ˆç°æš—ï¼Œå¾ˆç„¦è™‘ã€‚\n\n<!-- more -->\n","source":"_posts/ä¸´è¿‘2023æš‘å‡.md","raw":"---\nauthor: Chen Yingfa\ntitle: ä¸´è¿‘2023æš‘å‡ï¼Œ00å¸ˆå§ç­”è¾©ï¼Œæ™šä¸Šæ‰“çƒ\ndate: 2023-05-18 19:12:19\ncategories: Life\ntags:\n- life\n- '00'\n- school\n- graduation\n- ä¸­æ–‡\n---\n\nä»Šå¤©ç¡åˆ°ä¹ç‚¹æ‰é†’æ¥ï¼Œè¿˜æ˜¯è¢«00æ‰“ç”µè¯å«é†’çš„ã€‚å»è¿‡äº†ä¸ªæ—©ï¼Œç„¶åå»ä¸Šã€Šæ·±åº¦å­¦ä¹ ã€‹ã€‚\n\nä»Šå¤©00çš„å¸ˆå§ç­”è¾©ï¼Œä¸‹åˆä¸‰ç‚¹å»äº†ï¼Œå½“æ—¶æˆ‘åœ¨ç¡åˆè§‰ã€‚æ„Ÿè§‰ä¹‹åå¥¹æœ‰ç‚¹emoï¼Œä½†æ˜¯å¥¹ä¸æ‰¿è®¤ï¼Œä¸çŸ¥é“ä¸ºä»€ä¹ˆã€‚ç„¶åèŠäº†å¾ˆå¤šå…³äºæœªæ¥ï¼Œç»“å©šã€ç”Ÿå­©å­ã€æ‰¾å·¥ä½œç­‰äº‹æƒ…ã€‚æ„Ÿè§‰ä¹Ÿæ²¡æœ‰å¾ˆå¤§çš„é—®é¢˜ï¼Œä½†æ˜¯00æ€»æ˜¯æŠŠä¸œè¥¿çœ‹å¾—å¾ˆç°æš—ï¼Œå¾ˆç„¦è™‘ã€‚\n\n<!-- more -->\n","slug":"ä¸´è¿‘2023æš‘å‡","published":1,"updated":"2024-01-22T05:38:24.811Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cltl4oxej000mxh7k0gmq2mji","content":"<p>ä»Šå¤©ç¡åˆ°ä¹ç‚¹æ‰é†’æ¥ï¼Œè¿˜æ˜¯è¢«00æ‰“ç”µè¯å«é†’çš„ã€‚å»è¿‡äº†ä¸ªæ—©ï¼Œç„¶åå»ä¸Šã€Šæ·±åº¦å­¦ä¹ ã€‹ã€‚</p>\n<p>ä»Šå¤©00çš„å¸ˆå§ç­”è¾©ï¼Œä¸‹åˆä¸‰ç‚¹å»äº†ï¼Œå½“æ—¶æˆ‘åœ¨ç¡åˆè§‰ã€‚æ„Ÿè§‰ä¹‹åå¥¹æœ‰ç‚¹emoï¼Œä½†æ˜¯å¥¹ä¸æ‰¿è®¤ï¼Œä¸çŸ¥é“ä¸ºä»€ä¹ˆã€‚ç„¶åèŠäº†å¾ˆå¤šå…³äºæœªæ¥ï¼Œç»“å©šã€ç”Ÿå­©å­ã€æ‰¾å·¥ä½œç­‰äº‹æƒ…ã€‚æ„Ÿè§‰ä¹Ÿæ²¡æœ‰å¾ˆå¤§çš„é—®é¢˜ï¼Œä½†æ˜¯00æ€»æ˜¯æŠŠä¸œè¥¿çœ‹å¾—å¾ˆç°æš—ï¼Œå¾ˆç„¦è™‘ã€‚</p>\n<span id=\"more\"></span>\n","site":{"data":{}},"excerpt":"<p>ä»Šå¤©ç¡åˆ°ä¹ç‚¹æ‰é†’æ¥ï¼Œè¿˜æ˜¯è¢«00æ‰“ç”µè¯å«é†’çš„ã€‚å»è¿‡äº†ä¸ªæ—©ï¼Œç„¶åå»ä¸Šã€Šæ·±åº¦å­¦ä¹ ã€‹ã€‚</p>\n<p>ä»Šå¤©00çš„å¸ˆå§ç­”è¾©ï¼Œä¸‹åˆä¸‰ç‚¹å»äº†ï¼Œå½“æ—¶æˆ‘åœ¨ç¡åˆè§‰ã€‚æ„Ÿè§‰ä¹‹åå¥¹æœ‰ç‚¹emoï¼Œä½†æ˜¯å¥¹ä¸æ‰¿è®¤ï¼Œä¸çŸ¥é“ä¸ºä»€ä¹ˆã€‚ç„¶åèŠäº†å¾ˆå¤šå…³äºæœªæ¥ï¼Œç»“å©šã€ç”Ÿå­©å­ã€æ‰¾å·¥ä½œç­‰äº‹æƒ…ã€‚æ„Ÿè§‰ä¹Ÿæ²¡æœ‰å¾ˆå¤§çš„é—®é¢˜ï¼Œä½†æ˜¯00æ€»æ˜¯æŠŠä¸œè¥¿çœ‹å¾—å¾ˆç°æš—ï¼Œå¾ˆç„¦è™‘ã€‚</p>","more":""},{"author":"é™ˆè‹±å‘ Yingfa Chen","title":"æ›´æ–°ä¸ªäººä¸»é¡µ","date":"2023-09-16T15:27:20.000Z","thumbnail":"ç”³è¯·ç­¾è¯.jpg","thubmnail-alt":"00åœ¨ç­¾è¯ä¸­å¿ƒå‰é¢","thumbnail-title":"00åœ¨ç­¾è¯ä¸­å¿ƒå‰é¢","_content":"\nä¹‹å‰æœ‰è¿‡ä¸ªäººä¸»é¡µï¼Œä½†æ˜¯ä¸€ç›´æ²¡æœ‰å¼„å¥½ï¼Œæ›´æ²¡æœ‰æ›´æ–°ã€‚æœ€è¿‘æˆ‘å°†è‡ªå·±çš„ GitHub çš„ç”¨æˆ·åæ”¹äº†ï¼Œå¯¼è‡´ä¹‹å‰çš„ GitHub Pages å¤±æ•ˆäº†ï¼Œå°±è¶æœºé‡æ–°æ­å»ºä¸ªäººä¸»é¡µã€‚\n\nå…œå…œè½¬è½¬ï¼Œè¿˜æ˜¯å†³å®šä½¿ç”¨ Hexoã€‚ä»¥å‰ç”¨è¿‡ Jekyllï¼Œè§‰å¾—è¿˜è¡Œï¼Œä½†æ˜¯çœŸçš„ä¸æƒ³ç”¨ Rubyï¼ŒHugo åˆå¤ªéº»çƒ¦ã€‚\n\n\n<!-- more -->\n\n\né€‰äº†å¥½ä¹…ä¸»é¢˜ï¼ŒHexo å®£ä¼ è¯´æœ‰å¾ˆå¤šä¸»é¢˜ï¼Œä½†æ˜¯å®˜ç½‘ä¸Šä¸åˆ° 400 ä¸ªä¸»é¢˜ï¼Œè€Œä¸”å¤§éƒ¨åˆ†éƒ½ä¸ç¬¦åˆæˆ‘çš„å®¡ç¾æˆ–è€…è¦æ±‚ã€‚æˆ‘æƒ³è¦çš„é£æ ¼æ˜¯ç®€çº¦ï¼Œç°ä»£ï¼Œéœ€è¦åŒæ—¶æ”¯æŒé»‘æš—å’Œç™½äº®æ¨¡å¼ï¼Œéœ€è¦æœ‰ä»£ç é«˜äº®ä¸”æ˜¯ä»£ç æ˜¯ç­‰æ¬¾å­—ä½“ã€‚æœ€æ¥è¿‘æˆ‘çš„è¦æ±‚å°±æ˜¯[Maple](https://www.github.com/xbmlz/hexo-theme-maple)ä¸»é¢˜ã€‚å¯æ˜¯ä»ç„¶æ— æ³•æ»¡è¶³æˆ‘çš„è¦æ±‚ï¼Œæ‰€ä»¥æˆ‘ä¿®æ”¹äº†ä¸€äº›æ ¼å¼ï¼ˆåŸç‰ˆç”šè‡³æœ‰ä¸€äº›é¢œè‰² bugï¼‰ï¼Œæ·»åŠ äº†è‡ªå·±çš„ä¸€äº›å†…å®¹ï¼Œç»“æœæ˜¯ä¸€ä¸ªå«åš[æ«å¶](https://www.github.com/chen-yingfa/hexo-theme-fengye)çš„ä¸»é¢˜ã€‚\n\n## æ—¥è®°\n\nä»Šå¤©æ—©ä¸Šä¸ƒç‚¹åŠèµ·æ¥ ğŸ›ï¼Œæ‰“ç”µè¯ ğŸ“± å«é†’00ï¼ˆç»ˆäºæœ‰ä¸€æ¬¡æ˜¯æˆ‘æ‰“ç”µè¯äº†å“ˆå“ˆå“ˆå“ˆï¼‰ï¼Œç„¶åå»æ ¸ç ”é™¢ä¿±ä¹éƒ¨åœ¨ç»¼ä½“æ‰“ç¾½æ¯›çƒ ğŸ¸ï¼Œåæ¥å‘ç°ä»–ä»¬å…¶å®çº¦äº†è¥¿ä½“ï¼Œä½†æ˜¯æˆ‘è·Ÿ00è‡ªå·±åœ¨è¹­ä¸€ä¸ªç©ºåœºå°±ä¸ç®¡äº†ï¼Œå…«ç‚¹åŠå·¦å³æœ‰äººæ¥äº†æˆ‘ä»¬å°±å»è¿‡æ—©ï¼Œç„¶åå»æˆ‘å®¿èˆ ğŸ¡ã€‚\n\nä¹‹åç‚¹äº†åº“è¿ªï¼Œç„¶åå»äº†å­¦æ ¡å—è¾¹çš„ä¸€ä¸ªè¶…å¸‚ï¼Œä¹°äº†ä¸€å¤§åŒ…è–¯ç‰‡å’Œä¸€ä¸ªæ¦´è²ï¼ç„¶åå°±åœ¨å®¿èˆæ²¡æœ‰åƒåˆé¥­ï¼Œç›´æ¥å¾…åˆ°æ™šé¥­ã€‚ä¸­åˆçš„æ—¶å€™è¿˜æ‹äº†è§†é¢‘ ğŸ“·ï¼Œä¸­é—´è¿˜å·®ç‚¹è¯´åˆ°00emoäº†ï¼Œå“ˆå“ˆå“ˆå“ˆã€‚\n\nä»Šå¤© 00 ä¸‹åˆå››ç‚¹å’Œæ™šä¸Šä¸ƒç‚¹éƒ½æœ‰ç›´æ’­è¯¾ ğŸ‘©ğŸ»â€ğŸ«ï¼Œéƒ½æ˜¯çœŸæ­£å¼€è¯¾ï¼Œä¸‹åˆçš„åœ¨æˆ‘å®¿èˆå¼€çš„ï¼Œå¥½åƒå¾ˆæˆåŠŸï¼Œè™½ç„¶æ‹–å ‚äº†ä¸€ç‚¹ç‚¹ã€‚æ™šä¸Šçš„åœ¨å¥¹è‡ªå·±å®¿èˆï¼Œè²Œä¼¼ä¹Ÿæ‹–å ‚äº†ï¼Œ00 è¯´æœ‰å¥½å¤šäººã€‚\n\næ™šä¸Šä¹ç‚¹å»æ‰“ç¾½æ¯›çƒäº† ğŸ¸ï¼Œå¸¦ä¸Šç›¸æœºå½•äº†æ‰“çƒçš„è§†é¢‘ï¼Œç„¶åå›å»æ´—æ¾¡ï¼Œæ™šä¸Šå»æ—å¤§åŒ—è·¯çš„å®¶ ğŸ¡ã€‚\n\n## æœ€è¿‘\n\næœ€è¿‘å¥½å¿™ï¼Œæ–°å­¦æœŸé©¬ä¸Šå°±è¦å¼€å§‹äº†ï¼Œè¿™é‡Œæ€»ç»“ä¸€ä¸‹æš‘å‡å¼€å§‹åˆ°æ­¤æ¯”è¾ƒé‡è¦çš„äº‹æƒ…å§ã€‚\n\nè¿™ä¸ªæš‘å‡æ¬å‡ºæ ¡åˆæ¬å›æ¥äº†ï¼ŒæŠ˜è…¾äº†åˆè´¹é’± ğŸ’°ï¼Œå­¦æ ¡çœŸçš„å¥½æ¶å¿ƒï¼Œä¹‹å‰è¯´äº†å¤§æ¦‚ç‡æ˜¯ä¸ä¼šæœ‰å®¿èˆï¼Œç°åœ¨å°±æœ‰å¾ˆå¤šç©ºçš„æˆ¿é—´ã€‚\n\næœŸæœ«å‰è·Ÿå¯¼å¸ˆç¡®å®šäº†è¦è¯»åšäº†ï¼Œæˆ‘è·Ÿä»–æˆ‘æƒ³è¦ä¸‰å¹´æ¯•ä¸šï¼Œä»–è¯´æ²¡æœ‰é—®é¢˜ï¼Œå¸Œæœ›çœŸçš„æ˜¯å¯ä»¥å§ï¼Œæˆ‘ä»¬å®éªŒå®¤å¥½åƒåŸºæœ¬éƒ½æ˜¯ç›´åšç”Ÿï¼Œæ™®åšçš„åº”è¯¥éƒ½æ˜¯å››å¹´å§ã€‚00 ä¹Ÿç¡®å®šäº†ä¸ä¼šè¯»åšäº†ï¼Œæœ€è¿‘åœ¨æŠ•ç®€å†ï¼ŒOppo å¥½åƒå·²ç»æ‹¿åˆ°äº† offerï¼Œä½†æ˜¯ä»–ä»¬åŒ—äº¬æ²¡æœ‰éƒ¨é—¨ï¼Œæ‰€ä»¥ 00 ä¸æƒ³å»ï¼Œæˆ‘ä¹Ÿä¸æƒ³å¥¹å»ã€‚å¥½åƒäº’è”ç½‘ä»¥å¤–å¾ˆå¤šå…¬å¸éƒ½ä¸åœ¨åŒ—äº¬â€¦â€¦\n\næˆ‘çš„è®ºæ–‡ ğŸ“ƒ ERENï¼ˆä»¥å‰å«åš EmoRenï¼‰æŠ•å‡ºå»äº†ï¼Œä¸Šå‘¨ rebuttal ç»“æœå‡ºæ¥äº†ï¼Œä¸æ˜¯å¾ˆç†æƒ³ï¼Œæœ¬æ¥ soundess æ˜¯ 433ï¼ŒExcitement 323ï¼Œrebuttal ç»“æŸåç¬¬ä¸€ä¸ªå®¡ç¨¿äººå°† soundness è°ƒä½äº†ã€‚å­¦é•¿è¯´ä¸»ä¼šè®®ä¼°è®¡æ²¡æœ‰æœºä¼šäº†ï¼ŒFindings è¿˜æœ‰å¸Œæœ›ï¼Œæˆ‘å…¶å®æ— æ‰€è°“æ˜¯ä¸æ˜¯ Findingsï¼Œæ„Ÿè§‰å­¦é•¿åè€Œæœ‰ç‚¹ä»‹æ„ã€‚\n\nè¢«å®éªŒå®¤çš„å­¦é•¿å­¦å§æ‹‰å»é¢å£æ™ºèƒ½[^2]å»å¹²æ´»ï¼Œè·Ÿå…¬å¸çš„ä¸šåŠ¡æ²¡å•¥å…³ç³»ï¼Œå°±æ˜¯æŠŠæˆ‘çš„å·¥ä½æ¬äº†ï¼Œå¯èƒ½ä¸æƒ³å ç”¨éš”å£å®éªŒå®¤çš„ä½ç½®å§ ğŸ˜‚ ä½†æ˜¯æˆ‘çœŸçš„ä¸æƒ³å» ğŸ˜­ ä¸èƒ½è·Ÿ 00 å¾…åœ¨ä¸€èµ·äº†ã€‚æˆ‘ç°åœ¨å°±æ˜¯ä¸€å‘¨å¯èƒ½å»ä¸¤ä¸‰å¤© ğŸ˜‚\n\n[^2]:æˆ‘å¯¼å¸ˆå’ŒçŸ¥ä¹å­µåŒ–çš„çš„å…¬å¸\n\næœ€è¿‘è¿˜ç”³è¯·äº†ç­¾è¯ï¼Œå†³å®šäº†å¯’å‡ 00 è·Ÿæˆ‘ä¸€èµ·å›å®¶ï¼åœ¨å®¶å¾…ä¸€æ•´ä¸ªæœˆï¼Œå¥½ç¥å¥‡ï¼Œè§‰å¾—æˆ‘ä»¬çš„å…³ç³»å‘å±•å¾—å¥½é¡ºåˆ©ã€‚é©¬ä¸Šçš„å›½åº† ğŸ‡¨ğŸ‡³ æˆ‘ä¼šè·Ÿ 00 å›å»æ­¦æ±‰å’Œåº”åŸå‚åŠ å¥¹é«˜ä¸­åŒå­¦å’Œè¡¨å§çš„å©šç¤¼ ğŸ’‘ï¼Œé¡ºä¾¿è¿˜ä¼šçœ‹å¥¹çš„å¤–å…¬å¤–å©†ã€‚\n\n![ä¸­å›½å…¬æ°‘çš„æŠ¤ç…§](%E6%9B%B4%E6%96%B0%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5/%E4%B8%AD%E5%9B%BD%E6%8A%A4%E7%85%A7.jpg)\n\n> 00 çš„æŠ¤ç…§ï¼Œæ˜¯æˆ‘å‘å¾€çš„èº«ä»½ï¼\n\n![00åœ¨ç­¾è¯ä¸­å¿ƒå‰é¢](%E6%9B%B4%E6%96%B0%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5/%E7%94%B3%E8%AF%B7%E7%AD%BE%E8%AF%81.jpg)\n\n> ç­¾è¯ä¸­å¿ƒé—¨å£ï¼Œä¸æ˜¯å¤§ä½¿é¦†ï¼Œå¾ˆå¤šä¸ªå›½å®¶ç»Ÿä¸€åŠç†ç­¾è¯çš„åœ°æ–¹ã€‚\n\n![æˆ‘è·Ÿ00åœ¨ç­¾è¯ä¸­å¿ƒå‘¨è¾¹åƒæ¾³é—¨èœ](%E6%9B%B4%E6%96%B0%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5/%E5%90%83%E6%BE%B3%E9%97%A8%E8%8F%9C.jpg)\n\n> åœ¨ç”³è¯·ç­¾è¯çš„åœ°æ–¹æ—è¾¹çš„ä¸€ä¸ªå¾ˆé«˜çº§çš„å•†åœºé‡Œé¢åƒæ¾³é—¨é¤ã€‚\n\n***\n\n## ä¸€äº› Markdown æ¸²æŸ“å™¨æµ‹è¯•\n\næµ‹è¯•ä¸€ä¸‹å…¬å¼çš„æ¸²æŸ“ã€‚^[æˆ‘ç°åœ¨ç”¨äº† [hexo-rendered-markdown-it](https://github.com/hexojs/hexo-renderer-markdown-it) æ¥ä»£æ›¿åŸæ¥ Hexo çš„æ¸²æŸ“å™¨ã€‚åŸæ¥çš„æ¸²æŸ“å™¨æ˜¯ Marked (hexo-renderer-marked)]\n\n$$\n\\theta_i \\leftarrow  \\frac{\\partial}{\\partial  \\theta_i} \\mathcal L( y, f(x; \\theta))\n$$\n\nå…¶ä¸­ $\\mathcal L$ æ˜¯æŸå¤±å‡½æ•°ï¼Œ$f(\\cdot; \\theta)$ ç”± $\\theta$ å‚æ•°åŒ–çš„æ¨¡å‹ã€‚\n\né‚£ `ä»£ç ` å‘¢ï¼Ÿ`codell1i0oO` å¯ä»¥å—ï¼Ÿ\n\nä¸€ä¸ªè¡¨æ ¼ï¼š\n\n| Method | Accuracy |\n| ------ | -------- |\n| A      | 0.1      |\n| B      | 0.2      |\n| C      | 0.3      |\n\nä¸€ä¸ªåµŒå¥—åˆ—è¡¨ï¼š\n\n- a\n\n  - x\n  - asdf\n\n    - æ‡‚å¾—éƒ½æ‡‚\n\n  - å¥½å¥½\n\n    1. item 1\n    2. item 2\n\n       1. sdf\n       2. sdf\n\n    3. item 3\n\n\n- asdf\n\nä¸€ä¸ªä»£åŠåˆ—è¡¨ï¼š\n\n- [ ] ä¹°èœ\n- [ ] åšé¥­\n- [x] è·‘æ­¥\n- [x] æ‰“ç¾½æ¯›çƒ\n- [x] å†™è®ºæ–‡\n- [ ] æç§‘ç ”\n\nauto-links: [www.hexo.io](http://www.hexo.io)","source":"_posts/æ›´æ–°ä¸ªäººä¸»é¡µ.md","raw":"---\nauthor: é™ˆè‹±å‘ Yingfa Chen\ntitle: æ›´æ–°ä¸ªäººä¸»é¡µ\ndate: 2023-09-16 23:27:20\nthumbnail: \"ç”³è¯·ç­¾è¯.jpg\"\nthubmnail-alt: 00åœ¨ç­¾è¯ä¸­å¿ƒå‰é¢\nthumbnail-title: 00åœ¨ç­¾è¯ä¸­å¿ƒå‰é¢\ntags:\n- life\n- blog\n- ä¸­æ–‡\n- '00'\n- ç­¾è¯\n- hexo\n- hugo\n- jekyll\n- static-site-generator\n- research\n- ç¾½æ¯›çƒ\n- work\n- é¢å£æ™ºèƒ½\n- æ«å¶\n- ä¸­å›½\n- æŒªå¨\n- markdown\n- å®¿èˆ\ncategories: Life\n---\n\nä¹‹å‰æœ‰è¿‡ä¸ªäººä¸»é¡µï¼Œä½†æ˜¯ä¸€ç›´æ²¡æœ‰å¼„å¥½ï¼Œæ›´æ²¡æœ‰æ›´æ–°ã€‚æœ€è¿‘æˆ‘å°†è‡ªå·±çš„ GitHub çš„ç”¨æˆ·åæ”¹äº†ï¼Œå¯¼è‡´ä¹‹å‰çš„ GitHub Pages å¤±æ•ˆäº†ï¼Œå°±è¶æœºé‡æ–°æ­å»ºä¸ªäººä¸»é¡µã€‚\n\nå…œå…œè½¬è½¬ï¼Œè¿˜æ˜¯å†³å®šä½¿ç”¨ Hexoã€‚ä»¥å‰ç”¨è¿‡ Jekyllï¼Œè§‰å¾—è¿˜è¡Œï¼Œä½†æ˜¯çœŸçš„ä¸æƒ³ç”¨ Rubyï¼ŒHugo åˆå¤ªéº»çƒ¦ã€‚\n\n\n<!-- more -->\n\n\né€‰äº†å¥½ä¹…ä¸»é¢˜ï¼ŒHexo å®£ä¼ è¯´æœ‰å¾ˆå¤šä¸»é¢˜ï¼Œä½†æ˜¯å®˜ç½‘ä¸Šä¸åˆ° 400 ä¸ªä¸»é¢˜ï¼Œè€Œä¸”å¤§éƒ¨åˆ†éƒ½ä¸ç¬¦åˆæˆ‘çš„å®¡ç¾æˆ–è€…è¦æ±‚ã€‚æˆ‘æƒ³è¦çš„é£æ ¼æ˜¯ç®€çº¦ï¼Œç°ä»£ï¼Œéœ€è¦åŒæ—¶æ”¯æŒé»‘æš—å’Œç™½äº®æ¨¡å¼ï¼Œéœ€è¦æœ‰ä»£ç é«˜äº®ä¸”æ˜¯ä»£ç æ˜¯ç­‰æ¬¾å­—ä½“ã€‚æœ€æ¥è¿‘æˆ‘çš„è¦æ±‚å°±æ˜¯[Maple](https://www.github.com/xbmlz/hexo-theme-maple)ä¸»é¢˜ã€‚å¯æ˜¯ä»ç„¶æ— æ³•æ»¡è¶³æˆ‘çš„è¦æ±‚ï¼Œæ‰€ä»¥æˆ‘ä¿®æ”¹äº†ä¸€äº›æ ¼å¼ï¼ˆåŸç‰ˆç”šè‡³æœ‰ä¸€äº›é¢œè‰² bugï¼‰ï¼Œæ·»åŠ äº†è‡ªå·±çš„ä¸€äº›å†…å®¹ï¼Œç»“æœæ˜¯ä¸€ä¸ªå«åš[æ«å¶](https://www.github.com/chen-yingfa/hexo-theme-fengye)çš„ä¸»é¢˜ã€‚\n\n## æ—¥è®°\n\nä»Šå¤©æ—©ä¸Šä¸ƒç‚¹åŠèµ·æ¥ ğŸ›ï¼Œæ‰“ç”µè¯ ğŸ“± å«é†’00ï¼ˆç»ˆäºæœ‰ä¸€æ¬¡æ˜¯æˆ‘æ‰“ç”µè¯äº†å“ˆå“ˆå“ˆå“ˆï¼‰ï¼Œç„¶åå»æ ¸ç ”é™¢ä¿±ä¹éƒ¨åœ¨ç»¼ä½“æ‰“ç¾½æ¯›çƒ ğŸ¸ï¼Œåæ¥å‘ç°ä»–ä»¬å…¶å®çº¦äº†è¥¿ä½“ï¼Œä½†æ˜¯æˆ‘è·Ÿ00è‡ªå·±åœ¨è¹­ä¸€ä¸ªç©ºåœºå°±ä¸ç®¡äº†ï¼Œå…«ç‚¹åŠå·¦å³æœ‰äººæ¥äº†æˆ‘ä»¬å°±å»è¿‡æ—©ï¼Œç„¶åå»æˆ‘å®¿èˆ ğŸ¡ã€‚\n\nä¹‹åç‚¹äº†åº“è¿ªï¼Œç„¶åå»äº†å­¦æ ¡å—è¾¹çš„ä¸€ä¸ªè¶…å¸‚ï¼Œä¹°äº†ä¸€å¤§åŒ…è–¯ç‰‡å’Œä¸€ä¸ªæ¦´è²ï¼ç„¶åå°±åœ¨å®¿èˆæ²¡æœ‰åƒåˆé¥­ï¼Œç›´æ¥å¾…åˆ°æ™šé¥­ã€‚ä¸­åˆçš„æ—¶å€™è¿˜æ‹äº†è§†é¢‘ ğŸ“·ï¼Œä¸­é—´è¿˜å·®ç‚¹è¯´åˆ°00emoäº†ï¼Œå“ˆå“ˆå“ˆå“ˆã€‚\n\nä»Šå¤© 00 ä¸‹åˆå››ç‚¹å’Œæ™šä¸Šä¸ƒç‚¹éƒ½æœ‰ç›´æ’­è¯¾ ğŸ‘©ğŸ»â€ğŸ«ï¼Œéƒ½æ˜¯çœŸæ­£å¼€è¯¾ï¼Œä¸‹åˆçš„åœ¨æˆ‘å®¿èˆå¼€çš„ï¼Œå¥½åƒå¾ˆæˆåŠŸï¼Œè™½ç„¶æ‹–å ‚äº†ä¸€ç‚¹ç‚¹ã€‚æ™šä¸Šçš„åœ¨å¥¹è‡ªå·±å®¿èˆï¼Œè²Œä¼¼ä¹Ÿæ‹–å ‚äº†ï¼Œ00 è¯´æœ‰å¥½å¤šäººã€‚\n\næ™šä¸Šä¹ç‚¹å»æ‰“ç¾½æ¯›çƒäº† ğŸ¸ï¼Œå¸¦ä¸Šç›¸æœºå½•äº†æ‰“çƒçš„è§†é¢‘ï¼Œç„¶åå›å»æ´—æ¾¡ï¼Œæ™šä¸Šå»æ—å¤§åŒ—è·¯çš„å®¶ ğŸ¡ã€‚\n\n## æœ€è¿‘\n\næœ€è¿‘å¥½å¿™ï¼Œæ–°å­¦æœŸé©¬ä¸Šå°±è¦å¼€å§‹äº†ï¼Œè¿™é‡Œæ€»ç»“ä¸€ä¸‹æš‘å‡å¼€å§‹åˆ°æ­¤æ¯”è¾ƒé‡è¦çš„äº‹æƒ…å§ã€‚\n\nè¿™ä¸ªæš‘å‡æ¬å‡ºæ ¡åˆæ¬å›æ¥äº†ï¼ŒæŠ˜è…¾äº†åˆè´¹é’± ğŸ’°ï¼Œå­¦æ ¡çœŸçš„å¥½æ¶å¿ƒï¼Œä¹‹å‰è¯´äº†å¤§æ¦‚ç‡æ˜¯ä¸ä¼šæœ‰å®¿èˆï¼Œç°åœ¨å°±æœ‰å¾ˆå¤šç©ºçš„æˆ¿é—´ã€‚\n\næœŸæœ«å‰è·Ÿå¯¼å¸ˆç¡®å®šäº†è¦è¯»åšäº†ï¼Œæˆ‘è·Ÿä»–æˆ‘æƒ³è¦ä¸‰å¹´æ¯•ä¸šï¼Œä»–è¯´æ²¡æœ‰é—®é¢˜ï¼Œå¸Œæœ›çœŸçš„æ˜¯å¯ä»¥å§ï¼Œæˆ‘ä»¬å®éªŒå®¤å¥½åƒåŸºæœ¬éƒ½æ˜¯ç›´åšç”Ÿï¼Œæ™®åšçš„åº”è¯¥éƒ½æ˜¯å››å¹´å§ã€‚00 ä¹Ÿç¡®å®šäº†ä¸ä¼šè¯»åšäº†ï¼Œæœ€è¿‘åœ¨æŠ•ç®€å†ï¼ŒOppo å¥½åƒå·²ç»æ‹¿åˆ°äº† offerï¼Œä½†æ˜¯ä»–ä»¬åŒ—äº¬æ²¡æœ‰éƒ¨é—¨ï¼Œæ‰€ä»¥ 00 ä¸æƒ³å»ï¼Œæˆ‘ä¹Ÿä¸æƒ³å¥¹å»ã€‚å¥½åƒäº’è”ç½‘ä»¥å¤–å¾ˆå¤šå…¬å¸éƒ½ä¸åœ¨åŒ—äº¬â€¦â€¦\n\næˆ‘çš„è®ºæ–‡ ğŸ“ƒ ERENï¼ˆä»¥å‰å«åš EmoRenï¼‰æŠ•å‡ºå»äº†ï¼Œä¸Šå‘¨ rebuttal ç»“æœå‡ºæ¥äº†ï¼Œä¸æ˜¯å¾ˆç†æƒ³ï¼Œæœ¬æ¥ soundess æ˜¯ 433ï¼ŒExcitement 323ï¼Œrebuttal ç»“æŸåç¬¬ä¸€ä¸ªå®¡ç¨¿äººå°† soundness è°ƒä½äº†ã€‚å­¦é•¿è¯´ä¸»ä¼šè®®ä¼°è®¡æ²¡æœ‰æœºä¼šäº†ï¼ŒFindings è¿˜æœ‰å¸Œæœ›ï¼Œæˆ‘å…¶å®æ— æ‰€è°“æ˜¯ä¸æ˜¯ Findingsï¼Œæ„Ÿè§‰å­¦é•¿åè€Œæœ‰ç‚¹ä»‹æ„ã€‚\n\nè¢«å®éªŒå®¤çš„å­¦é•¿å­¦å§æ‹‰å»é¢å£æ™ºèƒ½[^2]å»å¹²æ´»ï¼Œè·Ÿå…¬å¸çš„ä¸šåŠ¡æ²¡å•¥å…³ç³»ï¼Œå°±æ˜¯æŠŠæˆ‘çš„å·¥ä½æ¬äº†ï¼Œå¯èƒ½ä¸æƒ³å ç”¨éš”å£å®éªŒå®¤çš„ä½ç½®å§ ğŸ˜‚ ä½†æ˜¯æˆ‘çœŸçš„ä¸æƒ³å» ğŸ˜­ ä¸èƒ½è·Ÿ 00 å¾…åœ¨ä¸€èµ·äº†ã€‚æˆ‘ç°åœ¨å°±æ˜¯ä¸€å‘¨å¯èƒ½å»ä¸¤ä¸‰å¤© ğŸ˜‚\n\n[^2]:æˆ‘å¯¼å¸ˆå’ŒçŸ¥ä¹å­µåŒ–çš„çš„å…¬å¸\n\næœ€è¿‘è¿˜ç”³è¯·äº†ç­¾è¯ï¼Œå†³å®šäº†å¯’å‡ 00 è·Ÿæˆ‘ä¸€èµ·å›å®¶ï¼åœ¨å®¶å¾…ä¸€æ•´ä¸ªæœˆï¼Œå¥½ç¥å¥‡ï¼Œè§‰å¾—æˆ‘ä»¬çš„å…³ç³»å‘å±•å¾—å¥½é¡ºåˆ©ã€‚é©¬ä¸Šçš„å›½åº† ğŸ‡¨ğŸ‡³ æˆ‘ä¼šè·Ÿ 00 å›å»æ­¦æ±‰å’Œåº”åŸå‚åŠ å¥¹é«˜ä¸­åŒå­¦å’Œè¡¨å§çš„å©šç¤¼ ğŸ’‘ï¼Œé¡ºä¾¿è¿˜ä¼šçœ‹å¥¹çš„å¤–å…¬å¤–å©†ã€‚\n\n![ä¸­å›½å…¬æ°‘çš„æŠ¤ç…§](%E6%9B%B4%E6%96%B0%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5/%E4%B8%AD%E5%9B%BD%E6%8A%A4%E7%85%A7.jpg)\n\n> 00 çš„æŠ¤ç…§ï¼Œæ˜¯æˆ‘å‘å¾€çš„èº«ä»½ï¼\n\n![00åœ¨ç­¾è¯ä¸­å¿ƒå‰é¢](%E6%9B%B4%E6%96%B0%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5/%E7%94%B3%E8%AF%B7%E7%AD%BE%E8%AF%81.jpg)\n\n> ç­¾è¯ä¸­å¿ƒé—¨å£ï¼Œä¸æ˜¯å¤§ä½¿é¦†ï¼Œå¾ˆå¤šä¸ªå›½å®¶ç»Ÿä¸€åŠç†ç­¾è¯çš„åœ°æ–¹ã€‚\n\n![æˆ‘è·Ÿ00åœ¨ç­¾è¯ä¸­å¿ƒå‘¨è¾¹åƒæ¾³é—¨èœ](%E6%9B%B4%E6%96%B0%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5/%E5%90%83%E6%BE%B3%E9%97%A8%E8%8F%9C.jpg)\n\n> åœ¨ç”³è¯·ç­¾è¯çš„åœ°æ–¹æ—è¾¹çš„ä¸€ä¸ªå¾ˆé«˜çº§çš„å•†åœºé‡Œé¢åƒæ¾³é—¨é¤ã€‚\n\n***\n\n## ä¸€äº› Markdown æ¸²æŸ“å™¨æµ‹è¯•\n\næµ‹è¯•ä¸€ä¸‹å…¬å¼çš„æ¸²æŸ“ã€‚^[æˆ‘ç°åœ¨ç”¨äº† [hexo-rendered-markdown-it](https://github.com/hexojs/hexo-renderer-markdown-it) æ¥ä»£æ›¿åŸæ¥ Hexo çš„æ¸²æŸ“å™¨ã€‚åŸæ¥çš„æ¸²æŸ“å™¨æ˜¯ Marked (hexo-renderer-marked)]\n\n$$\n\\theta_i \\leftarrow  \\frac{\\partial}{\\partial  \\theta_i} \\mathcal L( y, f(x; \\theta))\n$$\n\nå…¶ä¸­ $\\mathcal L$ æ˜¯æŸå¤±å‡½æ•°ï¼Œ$f(\\cdot; \\theta)$ ç”± $\\theta$ å‚æ•°åŒ–çš„æ¨¡å‹ã€‚\n\né‚£ `ä»£ç ` å‘¢ï¼Ÿ`codell1i0oO` å¯ä»¥å—ï¼Ÿ\n\nä¸€ä¸ªè¡¨æ ¼ï¼š\n\n| Method | Accuracy |\n| ------ | -------- |\n| A      | 0.1      |\n| B      | 0.2      |\n| C      | 0.3      |\n\nä¸€ä¸ªåµŒå¥—åˆ—è¡¨ï¼š\n\n- a\n\n  - x\n  - asdf\n\n    - æ‡‚å¾—éƒ½æ‡‚\n\n  - å¥½å¥½\n\n    1. item 1\n    2. item 2\n\n       1. sdf\n       2. sdf\n\n    3. item 3\n\n\n- asdf\n\nä¸€ä¸ªä»£åŠåˆ—è¡¨ï¼š\n\n- [ ] ä¹°èœ\n- [ ] åšé¥­\n- [x] è·‘æ­¥\n- [x] æ‰“ç¾½æ¯›çƒ\n- [x] å†™è®ºæ–‡\n- [ ] æç§‘ç ”\n\nauto-links: [www.hexo.io](http://www.hexo.io)","slug":"æ›´æ–°ä¸ªäººä¸»é¡µ","published":1,"updated":"2024-01-23T11:32:52.626Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cltl4oxej000nxh7kc2lgbk3d","content":"<p>ä¹‹å‰æœ‰è¿‡ä¸ªäººä¸»é¡µï¼Œä½†æ˜¯ä¸€ç›´æ²¡æœ‰å¼„å¥½ï¼Œæ›´æ²¡æœ‰æ›´æ–°ã€‚æœ€è¿‘æˆ‘å°†è‡ªå·±çš„ GitHub çš„ç”¨æˆ·åæ”¹äº†ï¼Œå¯¼è‡´ä¹‹å‰çš„ GitHub Pages å¤±æ•ˆäº†ï¼Œå°±è¶æœºé‡æ–°æ­å»ºä¸ªäººä¸»é¡µã€‚</p>\n<p>å…œå…œè½¬è½¬ï¼Œè¿˜æ˜¯å†³å®šä½¿ç”¨ Hexoã€‚ä»¥å‰ç”¨è¿‡ Jekyllï¼Œè§‰å¾—è¿˜è¡Œï¼Œä½†æ˜¯çœŸçš„ä¸æƒ³ç”¨ Rubyï¼ŒHugo åˆå¤ªéº»çƒ¦ã€‚</p>\n<span id=\"more\"></span>\n<p>é€‰äº†å¥½ä¹…ä¸»é¢˜ï¼ŒHexo å®£ä¼ è¯´æœ‰å¾ˆå¤šä¸»é¢˜ï¼Œä½†æ˜¯å®˜ç½‘ä¸Šä¸åˆ° 400 ä¸ªä¸»é¢˜ï¼Œè€Œä¸”å¤§éƒ¨åˆ†éƒ½ä¸ç¬¦åˆæˆ‘çš„å®¡ç¾æˆ–è€…è¦æ±‚ã€‚æˆ‘æƒ³è¦çš„é£æ ¼æ˜¯ç®€çº¦ï¼Œç°ä»£ï¼Œéœ€è¦åŒæ—¶æ”¯æŒé»‘æš—å’Œç™½äº®æ¨¡å¼ï¼Œéœ€è¦æœ‰ä»£ç é«˜äº®ä¸”æ˜¯ä»£ç æ˜¯ç­‰æ¬¾å­—ä½“ã€‚æœ€æ¥è¿‘æˆ‘çš„è¦æ±‚å°±æ˜¯<a href=\"https://www.github.com/xbmlz/hexo-theme-maple\">Maple</a>ä¸»é¢˜ã€‚å¯æ˜¯ä»ç„¶æ— æ³•æ»¡è¶³æˆ‘çš„è¦æ±‚ï¼Œæ‰€ä»¥æˆ‘ä¿®æ”¹äº†ä¸€äº›æ ¼å¼ï¼ˆåŸç‰ˆç”šè‡³æœ‰ä¸€äº›é¢œè‰² bugï¼‰ï¼Œæ·»åŠ äº†è‡ªå·±çš„ä¸€äº›å†…å®¹ï¼Œç»“æœæ˜¯ä¸€ä¸ªå«åš<a href=\"https://www.github.com/chen-yingfa/hexo-theme-fengye\">æ«å¶</a>çš„ä¸»é¢˜ã€‚</p>\n<h2 id=\"æ—¥è®°\">æ—¥è®°</h2>\n<p>ä»Šå¤©æ—©ä¸Šä¸ƒç‚¹åŠèµ·æ¥ ğŸ›ï¼Œæ‰“ç”µè¯ ğŸ“± å«é†’00ï¼ˆç»ˆäºæœ‰ä¸€æ¬¡æ˜¯æˆ‘æ‰“ç”µè¯äº†å“ˆå“ˆå“ˆå“ˆï¼‰ï¼Œç„¶åå»æ ¸ç ”é™¢ä¿±ä¹éƒ¨åœ¨ç»¼ä½“æ‰“ç¾½æ¯›çƒ ğŸ¸ï¼Œåæ¥å‘ç°ä»–ä»¬å…¶å®çº¦äº†è¥¿ä½“ï¼Œä½†æ˜¯æˆ‘è·Ÿ00è‡ªå·±åœ¨è¹­ä¸€ä¸ªç©ºåœºå°±ä¸ç®¡äº†ï¼Œå…«ç‚¹åŠå·¦å³æœ‰äººæ¥äº†æˆ‘ä»¬å°±å»è¿‡æ—©ï¼Œç„¶åå»æˆ‘å®¿èˆ ğŸ¡ã€‚</p>\n<p>ä¹‹åç‚¹äº†åº“è¿ªï¼Œç„¶åå»äº†å­¦æ ¡å—è¾¹çš„ä¸€ä¸ªè¶…å¸‚ï¼Œä¹°äº†ä¸€å¤§åŒ…è–¯ç‰‡å’Œä¸€ä¸ªæ¦´è²ï¼ç„¶åå°±åœ¨å®¿èˆæ²¡æœ‰åƒåˆé¥­ï¼Œç›´æ¥å¾…åˆ°æ™šé¥­ã€‚ä¸­åˆçš„æ—¶å€™è¿˜æ‹äº†è§†é¢‘ ğŸ“·ï¼Œä¸­é—´è¿˜å·®ç‚¹è¯´åˆ°00emoäº†ï¼Œå“ˆå“ˆå“ˆå“ˆã€‚</p>\n<p>ä»Šå¤© 00 ä¸‹åˆå››ç‚¹å’Œæ™šä¸Šä¸ƒç‚¹éƒ½æœ‰ç›´æ’­è¯¾ ğŸ‘©ğŸ»â€ğŸ«ï¼Œéƒ½æ˜¯çœŸæ­£å¼€è¯¾ï¼Œä¸‹åˆçš„åœ¨æˆ‘å®¿èˆå¼€çš„ï¼Œå¥½åƒå¾ˆæˆåŠŸï¼Œè™½ç„¶æ‹–å ‚äº†ä¸€ç‚¹ç‚¹ã€‚æ™šä¸Šçš„åœ¨å¥¹è‡ªå·±å®¿èˆï¼Œè²Œä¼¼ä¹Ÿæ‹–å ‚äº†ï¼Œ00 è¯´æœ‰å¥½å¤šäººã€‚</p>\n<p>æ™šä¸Šä¹ç‚¹å»æ‰“ç¾½æ¯›çƒäº† ğŸ¸ï¼Œå¸¦ä¸Šç›¸æœºå½•äº†æ‰“çƒçš„è§†é¢‘ï¼Œç„¶åå›å»æ´—æ¾¡ï¼Œæ™šä¸Šå»æ—å¤§åŒ—è·¯çš„å®¶ ğŸ¡ã€‚</p>\n<h2 id=\"æœ€è¿‘\">æœ€è¿‘</h2>\n<p>æœ€è¿‘å¥½å¿™ï¼Œæ–°å­¦æœŸé©¬ä¸Šå°±è¦å¼€å§‹äº†ï¼Œè¿™é‡Œæ€»ç»“ä¸€ä¸‹æš‘å‡å¼€å§‹åˆ°æ­¤æ¯”è¾ƒé‡è¦çš„äº‹æƒ…å§ã€‚</p>\n<p>è¿™ä¸ªæš‘å‡æ¬å‡ºæ ¡åˆæ¬å›æ¥äº†ï¼ŒæŠ˜è…¾äº†åˆè´¹é’± ğŸ’°ï¼Œå­¦æ ¡çœŸçš„å¥½æ¶å¿ƒï¼Œä¹‹å‰è¯´äº†å¤§æ¦‚ç‡æ˜¯ä¸ä¼šæœ‰å®¿èˆï¼Œç°åœ¨å°±æœ‰å¾ˆå¤šç©ºçš„æˆ¿é—´ã€‚</p>\n<p>æœŸæœ«å‰è·Ÿå¯¼å¸ˆç¡®å®šäº†è¦è¯»åšäº†ï¼Œæˆ‘è·Ÿä»–æˆ‘æƒ³è¦ä¸‰å¹´æ¯•ä¸šï¼Œä»–è¯´æ²¡æœ‰é—®é¢˜ï¼Œå¸Œæœ›çœŸçš„æ˜¯å¯ä»¥å§ï¼Œæˆ‘ä»¬å®éªŒå®¤å¥½åƒåŸºæœ¬éƒ½æ˜¯ç›´åšç”Ÿï¼Œæ™®åšçš„åº”è¯¥éƒ½æ˜¯å››å¹´å§ã€‚00 ä¹Ÿç¡®å®šäº†ä¸ä¼šè¯»åšäº†ï¼Œæœ€è¿‘åœ¨æŠ•ç®€å†ï¼ŒOppo å¥½åƒå·²ç»æ‹¿åˆ°äº† offerï¼Œä½†æ˜¯ä»–ä»¬åŒ—äº¬æ²¡æœ‰éƒ¨é—¨ï¼Œæ‰€ä»¥ 00 ä¸æƒ³å»ï¼Œæˆ‘ä¹Ÿä¸æƒ³å¥¹å»ã€‚å¥½åƒäº’è”ç½‘ä»¥å¤–å¾ˆå¤šå…¬å¸éƒ½ä¸åœ¨åŒ—äº¬â€¦â€¦</p>\n<p>æˆ‘çš„è®ºæ–‡ ğŸ“ƒ ERENï¼ˆä»¥å‰å«åš EmoRenï¼‰æŠ•å‡ºå»äº†ï¼Œä¸Šå‘¨ rebuttal ç»“æœå‡ºæ¥äº†ï¼Œä¸æ˜¯å¾ˆç†æƒ³ï¼Œæœ¬æ¥ soundess æ˜¯ 433ï¼ŒExcitement 323ï¼Œrebuttal ç»“æŸåç¬¬ä¸€ä¸ªå®¡ç¨¿äººå°† soundness è°ƒä½äº†ã€‚å­¦é•¿è¯´ä¸»ä¼šè®®ä¼°è®¡æ²¡æœ‰æœºä¼šäº†ï¼ŒFindings è¿˜æœ‰å¸Œæœ›ï¼Œæˆ‘å…¶å®æ— æ‰€è°“æ˜¯ä¸æ˜¯ Findingsï¼Œæ„Ÿè§‰å­¦é•¿åè€Œæœ‰ç‚¹ä»‹æ„ã€‚</p>\n<p>è¢«å®éªŒå®¤çš„å­¦é•¿å­¦å§æ‹‰å»é¢å£æ™ºèƒ½<sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">[1]</a></sup>å»å¹²æ´»ï¼Œè·Ÿå…¬å¸çš„ä¸šåŠ¡æ²¡å•¥å…³ç³»ï¼Œå°±æ˜¯æŠŠæˆ‘çš„å·¥ä½æ¬äº†ï¼Œå¯èƒ½ä¸æƒ³å ç”¨éš”å£å®éªŒå®¤çš„ä½ç½®å§ ğŸ˜‚ ä½†æ˜¯æˆ‘çœŸçš„ä¸æƒ³å» ğŸ˜­ ä¸èƒ½è·Ÿ 00 å¾…åœ¨ä¸€èµ·äº†ã€‚æˆ‘ç°åœ¨å°±æ˜¯ä¸€å‘¨å¯èƒ½å»ä¸¤ä¸‰å¤© ğŸ˜‚</p>\n<p>æœ€è¿‘è¿˜ç”³è¯·äº†ç­¾è¯ï¼Œå†³å®šäº†å¯’å‡ 00 è·Ÿæˆ‘ä¸€èµ·å›å®¶ï¼åœ¨å®¶å¾…ä¸€æ•´ä¸ªæœˆï¼Œå¥½ç¥å¥‡ï¼Œè§‰å¾—æˆ‘ä»¬çš„å…³ç³»å‘å±•å¾—å¥½é¡ºåˆ©ã€‚é©¬ä¸Šçš„å›½åº† ğŸ‡¨ğŸ‡³ æˆ‘ä¼šè·Ÿ 00 å›å»æ­¦æ±‰å’Œåº”åŸå‚åŠ å¥¹é«˜ä¸­åŒå­¦å’Œè¡¨å§çš„å©šç¤¼ ğŸ’‘ï¼Œé¡ºä¾¿è¿˜ä¼šçœ‹å¥¹çš„å¤–å…¬å¤–å©†ã€‚</p>\n<p><img src=\"%E6%9B%B4%E6%96%B0%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5/%E4%B8%AD%E5%9B%BD%E6%8A%A4%E7%85%A7.jpg\" alt=\"ä¸­å›½å…¬æ°‘çš„æŠ¤ç…§\"></p>\n<blockquote>\n<p>00 çš„æŠ¤ç…§ï¼Œæ˜¯æˆ‘å‘å¾€çš„èº«ä»½ï¼</p>\n</blockquote>\n<p><img src=\"%E6%9B%B4%E6%96%B0%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5/%E7%94%B3%E8%AF%B7%E7%AD%BE%E8%AF%81.jpg\" alt=\"00åœ¨ç­¾è¯ä¸­å¿ƒå‰é¢\"></p>\n<blockquote>\n<p>ç­¾è¯ä¸­å¿ƒé—¨å£ï¼Œä¸æ˜¯å¤§ä½¿é¦†ï¼Œå¾ˆå¤šä¸ªå›½å®¶ç»Ÿä¸€åŠç†ç­¾è¯çš„åœ°æ–¹ã€‚</p>\n</blockquote>\n<p><img src=\"%E6%9B%B4%E6%96%B0%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5/%E5%90%83%E6%BE%B3%E9%97%A8%E8%8F%9C.jpg\" alt=\"æˆ‘è·Ÿ00åœ¨ç­¾è¯ä¸­å¿ƒå‘¨è¾¹åƒæ¾³é—¨èœ\"></p>\n<blockquote>\n<p>åœ¨ç”³è¯·ç­¾è¯çš„åœ°æ–¹æ—è¾¹çš„ä¸€ä¸ªå¾ˆé«˜çº§çš„å•†åœºé‡Œé¢åƒæ¾³é—¨é¤ã€‚</p>\n</blockquote>\n<hr>\n<h2 id=\"ä¸€äº›-Markdown-æ¸²æŸ“å™¨æµ‹è¯•\">ä¸€äº› Markdown æ¸²æŸ“å™¨æµ‹è¯•</h2>\n<p>æµ‹è¯•ä¸€ä¸‹å…¬å¼çš„æ¸²æŸ“ã€‚<sup class=\"footnote-ref\"><a href=\"#fn2\" id=\"fnref2\">[2]</a></sup></p>\n<p>$$\n\\theta_i \\leftarrow  \\frac{\\partial}{\\partial  \\theta_i} \\mathcal L( y, f(x; \\theta))\n$$</p>\n<p>å…¶ä¸­ $\\mathcal L$ æ˜¯æŸå¤±å‡½æ•°ï¼Œ$f(\\cdot; \\theta)$ ç”± $\\theta$ å‚æ•°åŒ–çš„æ¨¡å‹ã€‚</p>\n<p>é‚£ <code>ä»£ç </code> å‘¢ï¼Ÿ<code>codell1i0oO</code> å¯ä»¥å—ï¼Ÿ</p>\n<p>ä¸€ä¸ªè¡¨æ ¼ï¼š</p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Accuracy</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>A</td>\n<td>0.1</td>\n</tr>\n<tr>\n<td>B</td>\n<td>0.2</td>\n</tr>\n<tr>\n<td>C</td>\n<td>0.3</td>\n</tr>\n</tbody>\n</table>\n<p>ä¸€ä¸ªåµŒå¥—åˆ—è¡¨ï¼š</p>\n<ul>\n<li>\n<p>a</p>\n<ul>\n<li>\n<p>x</p>\n</li>\n<li>\n<p>asdf</p>\n<ul>\n<li>æ‡‚å¾—éƒ½æ‡‚</li>\n</ul>\n</li>\n<li>\n<p>å¥½å¥½</p>\n<ol>\n<li>\n<p>item 1</p>\n</li>\n<li>\n<p>item 2</p>\n<ol>\n<li>sdf</li>\n<li>sdf</li>\n</ol>\n</li>\n<li>\n<p>item 3</p>\n</li>\n</ol>\n</li>\n</ul>\n</li>\n<li>\n<p>asdf</p>\n</li>\n</ul>\n<p>ä¸€ä¸ªä»£åŠåˆ—è¡¨ï¼š</p>\n<ul class=\"contains-task-list\">\n<li class=\"task-list-item\"><input class=\"task-list-item-checkbox\" disabled=\"\" type=\"checkbox\"> ä¹°èœ</li>\n<li class=\"task-list-item\"><input class=\"task-list-item-checkbox\" disabled=\"\" type=\"checkbox\"> åšé¥­</li>\n<li class=\"task-list-item\"><input class=\"task-list-item-checkbox\" checked=\"\" disabled=\"\" type=\"checkbox\"> è·‘æ­¥</li>\n<li class=\"task-list-item\"><input class=\"task-list-item-checkbox\" checked=\"\" disabled=\"\" type=\"checkbox\"> æ‰“ç¾½æ¯›çƒ</li>\n<li class=\"task-list-item\"><input class=\"task-list-item-checkbox\" checked=\"\" disabled=\"\" type=\"checkbox\"> å†™è®ºæ–‡</li>\n<li class=\"task-list-item\"><input class=\"task-list-item-checkbox\" disabled=\"\" type=\"checkbox\"> æç§‘ç ”</li>\n</ul>\n<p>auto-links: <a href=\"http://www.hexo.io\">www.hexo.io</a></p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn1\" class=\"footnote-item\"><p>æˆ‘å¯¼å¸ˆå’ŒçŸ¥ä¹å­µåŒ–çš„çš„å…¬å¸ <a href=\"#fnref1\" class=\"footnote-backref\">â†©ï¸</a></p>\n</li>\n<li id=\"fn2\" class=\"footnote-item\"><p>æˆ‘ç°åœ¨ç”¨äº† <a href=\"https://github.com/hexojs/hexo-renderer-markdown-it\">hexo-rendered-markdown-it</a> æ¥ä»£æ›¿åŸæ¥ Hexo çš„æ¸²æŸ“å™¨ã€‚åŸæ¥çš„æ¸²æŸ“å™¨æ˜¯ Marked (hexo-renderer-marked) <a href=\"#fnref2\" class=\"footnote-backref\">â†©ï¸</a></p>\n</li>\n</ol>\n</section>\n","site":{"data":{}},"excerpt":"<p>ä¹‹å‰æœ‰è¿‡ä¸ªäººä¸»é¡µï¼Œä½†æ˜¯ä¸€ç›´æ²¡æœ‰å¼„å¥½ï¼Œæ›´æ²¡æœ‰æ›´æ–°ã€‚æœ€è¿‘æˆ‘å°†è‡ªå·±çš„ GitHub çš„ç”¨æˆ·åæ”¹äº†ï¼Œå¯¼è‡´ä¹‹å‰çš„ GitHub Pages å¤±æ•ˆäº†ï¼Œå°±è¶æœºé‡æ–°æ­å»ºä¸ªäººä¸»é¡µã€‚</p>\n<p>å…œå…œè½¬è½¬ï¼Œè¿˜æ˜¯å†³å®šä½¿ç”¨ Hexoã€‚ä»¥å‰ç”¨è¿‡ Jekyllï¼Œè§‰å¾—è¿˜è¡Œï¼Œä½†æ˜¯çœŸçš„ä¸æƒ³ç”¨ Rubyï¼ŒHugo åˆå¤ªéº»çƒ¦ã€‚</p>","more":"<p>é€‰äº†å¥½ä¹…ä¸»é¢˜ï¼ŒHexo å®£ä¼ è¯´æœ‰å¾ˆå¤šä¸»é¢˜ï¼Œä½†æ˜¯å®˜ç½‘ä¸Šä¸åˆ° 400 ä¸ªä¸»é¢˜ï¼Œè€Œä¸”å¤§éƒ¨åˆ†éƒ½ä¸ç¬¦åˆæˆ‘çš„å®¡ç¾æˆ–è€…è¦æ±‚ã€‚æˆ‘æƒ³è¦çš„é£æ ¼æ˜¯ç®€çº¦ï¼Œç°ä»£ï¼Œéœ€è¦åŒæ—¶æ”¯æŒé»‘æš—å’Œç™½äº®æ¨¡å¼ï¼Œéœ€è¦æœ‰ä»£ç é«˜äº®ä¸”æ˜¯ä»£ç æ˜¯ç­‰æ¬¾å­—ä½“ã€‚æœ€æ¥è¿‘æˆ‘çš„è¦æ±‚å°±æ˜¯<a href=\"https://www.github.com/xbmlz/hexo-theme-maple\">Maple</a>ä¸»é¢˜ã€‚å¯æ˜¯ä»ç„¶æ— æ³•æ»¡è¶³æˆ‘çš„è¦æ±‚ï¼Œæ‰€ä»¥æˆ‘ä¿®æ”¹äº†ä¸€äº›æ ¼å¼ï¼ˆåŸç‰ˆç”šè‡³æœ‰ä¸€äº›é¢œè‰² bugï¼‰ï¼Œæ·»åŠ äº†è‡ªå·±çš„ä¸€äº›å†…å®¹ï¼Œç»“æœæ˜¯ä¸€ä¸ªå«åš<a href=\"https://www.github.com/chen-yingfa/hexo-theme-fengye\">æ«å¶</a>çš„ä¸»é¢˜ã€‚</p>\n<h2 id=\"æ—¥è®°\">æ—¥è®°</h2>\n<p>ä»Šå¤©æ—©ä¸Šä¸ƒç‚¹åŠèµ·æ¥ ğŸ›ï¼Œæ‰“ç”µè¯ ğŸ“± å«é†’00ï¼ˆç»ˆäºæœ‰ä¸€æ¬¡æ˜¯æˆ‘æ‰“ç”µè¯äº†å“ˆå“ˆå“ˆå“ˆï¼‰ï¼Œç„¶åå»æ ¸ç ”é™¢ä¿±ä¹éƒ¨åœ¨ç»¼ä½“æ‰“ç¾½æ¯›çƒ ğŸ¸ï¼Œåæ¥å‘ç°ä»–ä»¬å…¶å®çº¦äº†è¥¿ä½“ï¼Œä½†æ˜¯æˆ‘è·Ÿ00è‡ªå·±åœ¨è¹­ä¸€ä¸ªç©ºåœºå°±ä¸ç®¡äº†ï¼Œå…«ç‚¹åŠå·¦å³æœ‰äººæ¥äº†æˆ‘ä»¬å°±å»è¿‡æ—©ï¼Œç„¶åå»æˆ‘å®¿èˆ ğŸ¡ã€‚</p>\n<p>ä¹‹åç‚¹äº†åº“è¿ªï¼Œç„¶åå»äº†å­¦æ ¡å—è¾¹çš„ä¸€ä¸ªè¶…å¸‚ï¼Œä¹°äº†ä¸€å¤§åŒ…è–¯ç‰‡å’Œä¸€ä¸ªæ¦´è²ï¼ç„¶åå°±åœ¨å®¿èˆæ²¡æœ‰åƒåˆé¥­ï¼Œç›´æ¥å¾…åˆ°æ™šé¥­ã€‚ä¸­åˆçš„æ—¶å€™è¿˜æ‹äº†è§†é¢‘ ğŸ“·ï¼Œä¸­é—´è¿˜å·®ç‚¹è¯´åˆ°00emoäº†ï¼Œå“ˆå“ˆå“ˆå“ˆã€‚</p>\n<p>ä»Šå¤© 00 ä¸‹åˆå››ç‚¹å’Œæ™šä¸Šä¸ƒç‚¹éƒ½æœ‰ç›´æ’­è¯¾ ğŸ‘©ğŸ»â€ğŸ«ï¼Œéƒ½æ˜¯çœŸæ­£å¼€è¯¾ï¼Œä¸‹åˆçš„åœ¨æˆ‘å®¿èˆå¼€çš„ï¼Œå¥½åƒå¾ˆæˆåŠŸï¼Œè™½ç„¶æ‹–å ‚äº†ä¸€ç‚¹ç‚¹ã€‚æ™šä¸Šçš„åœ¨å¥¹è‡ªå·±å®¿èˆï¼Œè²Œä¼¼ä¹Ÿæ‹–å ‚äº†ï¼Œ00 è¯´æœ‰å¥½å¤šäººã€‚</p>\n<p>æ™šä¸Šä¹ç‚¹å»æ‰“ç¾½æ¯›çƒäº† ğŸ¸ï¼Œå¸¦ä¸Šç›¸æœºå½•äº†æ‰“çƒçš„è§†é¢‘ï¼Œç„¶åå›å»æ´—æ¾¡ï¼Œæ™šä¸Šå»æ—å¤§åŒ—è·¯çš„å®¶ ğŸ¡ã€‚</p>\n<h2 id=\"æœ€è¿‘\">æœ€è¿‘</h2>\n<p>æœ€è¿‘å¥½å¿™ï¼Œæ–°å­¦æœŸé©¬ä¸Šå°±è¦å¼€å§‹äº†ï¼Œè¿™é‡Œæ€»ç»“ä¸€ä¸‹æš‘å‡å¼€å§‹åˆ°æ­¤æ¯”è¾ƒé‡è¦çš„äº‹æƒ…å§ã€‚</p>\n<p>è¿™ä¸ªæš‘å‡æ¬å‡ºæ ¡åˆæ¬å›æ¥äº†ï¼ŒæŠ˜è…¾äº†åˆè´¹é’± ğŸ’°ï¼Œå­¦æ ¡çœŸçš„å¥½æ¶å¿ƒï¼Œä¹‹å‰è¯´äº†å¤§æ¦‚ç‡æ˜¯ä¸ä¼šæœ‰å®¿èˆï¼Œç°åœ¨å°±æœ‰å¾ˆå¤šç©ºçš„æˆ¿é—´ã€‚</p>\n<p>æœŸæœ«å‰è·Ÿå¯¼å¸ˆç¡®å®šäº†è¦è¯»åšäº†ï¼Œæˆ‘è·Ÿä»–æˆ‘æƒ³è¦ä¸‰å¹´æ¯•ä¸šï¼Œä»–è¯´æ²¡æœ‰é—®é¢˜ï¼Œå¸Œæœ›çœŸçš„æ˜¯å¯ä»¥å§ï¼Œæˆ‘ä»¬å®éªŒå®¤å¥½åƒåŸºæœ¬éƒ½æ˜¯ç›´åšç”Ÿï¼Œæ™®åšçš„åº”è¯¥éƒ½æ˜¯å››å¹´å§ã€‚00 ä¹Ÿç¡®å®šäº†ä¸ä¼šè¯»åšäº†ï¼Œæœ€è¿‘åœ¨æŠ•ç®€å†ï¼ŒOppo å¥½åƒå·²ç»æ‹¿åˆ°äº† offerï¼Œä½†æ˜¯ä»–ä»¬åŒ—äº¬æ²¡æœ‰éƒ¨é—¨ï¼Œæ‰€ä»¥ 00 ä¸æƒ³å»ï¼Œæˆ‘ä¹Ÿä¸æƒ³å¥¹å»ã€‚å¥½åƒäº’è”ç½‘ä»¥å¤–å¾ˆå¤šå…¬å¸éƒ½ä¸åœ¨åŒ—äº¬â€¦â€¦</p>\n<p>æˆ‘çš„è®ºæ–‡ ğŸ“ƒ ERENï¼ˆä»¥å‰å«åš EmoRenï¼‰æŠ•å‡ºå»äº†ï¼Œä¸Šå‘¨ rebuttal ç»“æœå‡ºæ¥äº†ï¼Œä¸æ˜¯å¾ˆç†æƒ³ï¼Œæœ¬æ¥ soundess æ˜¯ 433ï¼ŒExcitement 323ï¼Œrebuttal ç»“æŸåç¬¬ä¸€ä¸ªå®¡ç¨¿äººå°† soundness è°ƒä½äº†ã€‚å­¦é•¿è¯´ä¸»ä¼šè®®ä¼°è®¡æ²¡æœ‰æœºä¼šäº†ï¼ŒFindings è¿˜æœ‰å¸Œæœ›ï¼Œæˆ‘å…¶å®æ— æ‰€è°“æ˜¯ä¸æ˜¯ Findingsï¼Œæ„Ÿè§‰å­¦é•¿åè€Œæœ‰ç‚¹ä»‹æ„ã€‚</p>\n<p>è¢«å®éªŒå®¤çš„å­¦é•¿å­¦å§æ‹‰å»é¢å£æ™ºèƒ½<sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">[1]</a></sup>å»å¹²æ´»ï¼Œè·Ÿå…¬å¸çš„ä¸šåŠ¡æ²¡å•¥å…³ç³»ï¼Œå°±æ˜¯æŠŠæˆ‘çš„å·¥ä½æ¬äº†ï¼Œå¯èƒ½ä¸æƒ³å ç”¨éš”å£å®éªŒå®¤çš„ä½ç½®å§ ğŸ˜‚ ä½†æ˜¯æˆ‘çœŸçš„ä¸æƒ³å» ğŸ˜­ ä¸èƒ½è·Ÿ 00 å¾…åœ¨ä¸€èµ·äº†ã€‚æˆ‘ç°åœ¨å°±æ˜¯ä¸€å‘¨å¯èƒ½å»ä¸¤ä¸‰å¤© ğŸ˜‚</p>\n<p>æœ€è¿‘è¿˜ç”³è¯·äº†ç­¾è¯ï¼Œå†³å®šäº†å¯’å‡ 00 è·Ÿæˆ‘ä¸€èµ·å›å®¶ï¼åœ¨å®¶å¾…ä¸€æ•´ä¸ªæœˆï¼Œå¥½ç¥å¥‡ï¼Œè§‰å¾—æˆ‘ä»¬çš„å…³ç³»å‘å±•å¾—å¥½é¡ºåˆ©ã€‚é©¬ä¸Šçš„å›½åº† ğŸ‡¨ğŸ‡³ æˆ‘ä¼šè·Ÿ 00 å›å»æ­¦æ±‰å’Œåº”åŸå‚åŠ å¥¹é«˜ä¸­åŒå­¦å’Œè¡¨å§çš„å©šç¤¼ ğŸ’‘ï¼Œé¡ºä¾¿è¿˜ä¼šçœ‹å¥¹çš„å¤–å…¬å¤–å©†ã€‚</p>\n<p><img src=\"%E6%9B%B4%E6%96%B0%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5/%E4%B8%AD%E5%9B%BD%E6%8A%A4%E7%85%A7.jpg\" alt=\"ä¸­å›½å…¬æ°‘çš„æŠ¤ç…§\"></p>\n<blockquote>\n<p>00 çš„æŠ¤ç…§ï¼Œæ˜¯æˆ‘å‘å¾€çš„èº«ä»½ï¼</p>\n</blockquote>\n<p><img src=\"%E6%9B%B4%E6%96%B0%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5/%E7%94%B3%E8%AF%B7%E7%AD%BE%E8%AF%81.jpg\" alt=\"00åœ¨ç­¾è¯ä¸­å¿ƒå‰é¢\"></p>\n<blockquote>\n<p>ç­¾è¯ä¸­å¿ƒé—¨å£ï¼Œä¸æ˜¯å¤§ä½¿é¦†ï¼Œå¾ˆå¤šä¸ªå›½å®¶ç»Ÿä¸€åŠç†ç­¾è¯çš„åœ°æ–¹ã€‚</p>\n</blockquote>\n<p><img src=\"%E6%9B%B4%E6%96%B0%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5/%E5%90%83%E6%BE%B3%E9%97%A8%E8%8F%9C.jpg\" alt=\"æˆ‘è·Ÿ00åœ¨ç­¾è¯ä¸­å¿ƒå‘¨è¾¹åƒæ¾³é—¨èœ\"></p>\n<blockquote>\n<p>åœ¨ç”³è¯·ç­¾è¯çš„åœ°æ–¹æ—è¾¹çš„ä¸€ä¸ªå¾ˆé«˜çº§çš„å•†åœºé‡Œé¢åƒæ¾³é—¨é¤ã€‚</p>\n</blockquote>\n<hr>\n<h2 id=\"ä¸€äº›-Markdown-æ¸²æŸ“å™¨æµ‹è¯•\">ä¸€äº› Markdown æ¸²æŸ“å™¨æµ‹è¯•</h2>\n<p>æµ‹è¯•ä¸€ä¸‹å…¬å¼çš„æ¸²æŸ“ã€‚<sup class=\"footnote-ref\"><a href=\"#fn2\" id=\"fnref2\">[2]</a></sup></p>\n<p>$$\n\\theta_i \\leftarrow  \\frac{\\partial}{\\partial  \\theta_i} \\mathcal L( y, f(x; \\theta))\n$$</p>\n<p>å…¶ä¸­ $\\mathcal L$ æ˜¯æŸå¤±å‡½æ•°ï¼Œ$f(\\cdot; \\theta)$ ç”± $\\theta$ å‚æ•°åŒ–çš„æ¨¡å‹ã€‚</p>\n<p>é‚£ <code>ä»£ç </code> å‘¢ï¼Ÿ<code>codell1i0oO</code> å¯ä»¥å—ï¼Ÿ</p>\n<p>ä¸€ä¸ªè¡¨æ ¼ï¼š</p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Accuracy</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>A</td>\n<td>0.1</td>\n</tr>\n<tr>\n<td>B</td>\n<td>0.2</td>\n</tr>\n<tr>\n<td>C</td>\n<td>0.3</td>\n</tr>\n</tbody>\n</table>\n<p>ä¸€ä¸ªåµŒå¥—åˆ—è¡¨ï¼š</p>\n<ul>\n<li>\n<p>a</p>\n<ul>\n<li>\n<p>x</p>\n</li>\n<li>\n<p>asdf</p>\n<ul>\n<li>æ‡‚å¾—éƒ½æ‡‚</li>\n</ul>\n</li>\n<li>\n<p>å¥½å¥½</p>\n<ol>\n<li>\n<p>item 1</p>\n</li>\n<li>\n<p>item 2</p>\n<ol>\n<li>sdf</li>\n<li>sdf</li>\n</ol>\n</li>\n<li>\n<p>item 3</p>\n</li>\n</ol>\n</li>\n</ul>\n</li>\n<li>\n<p>asdf</p>\n</li>\n</ul>\n<p>ä¸€ä¸ªä»£åŠåˆ—è¡¨ï¼š</p>\n<ul class=\"contains-task-list\">\n<li class=\"task-list-item\"><input class=\"task-list-item-checkbox\" disabled=\"\" type=\"checkbox\"> ä¹°èœ</li>\n<li class=\"task-list-item\"><input class=\"task-list-item-checkbox\" disabled=\"\" type=\"checkbox\"> åšé¥­</li>\n<li class=\"task-list-item\"><input class=\"task-list-item-checkbox\" checked=\"\" disabled=\"\" type=\"checkbox\"> è·‘æ­¥</li>\n<li class=\"task-list-item\"><input class=\"task-list-item-checkbox\" checked=\"\" disabled=\"\" type=\"checkbox\"> æ‰“ç¾½æ¯›çƒ</li>\n<li class=\"task-list-item\"><input class=\"task-list-item-checkbox\" checked=\"\" disabled=\"\" type=\"checkbox\"> å†™è®ºæ–‡</li>\n<li class=\"task-list-item\"><input class=\"task-list-item-checkbox\" disabled=\"\" type=\"checkbox\"> æç§‘ç ”</li>\n</ul>\n<p>auto-links: <a href=\"http://www.hexo.io\">www.hexo.io</a></p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn1\" class=\"footnote-item\"><p>æˆ‘å¯¼å¸ˆå’ŒçŸ¥ä¹å­µåŒ–çš„çš„å…¬å¸ <a href=\"#fnref1\" class=\"footnote-backref\">â†©ï¸</a></p>\n</li>\n<li id=\"fn2\" class=\"footnote-item\"><p>æˆ‘ç°åœ¨ç”¨äº† <a href=\"https://github.com/hexojs/hexo-renderer-markdown-it\">hexo-rendered-markdown-it</a> æ¥ä»£æ›¿åŸæ¥ Hexo çš„æ¸²æŸ“å™¨ã€‚åŸæ¥çš„æ¸²æŸ“å™¨æ˜¯ Marked (hexo-renderer-marked) <a href=\"#fnref2\" class=\"footnote-backref\">â†©ï¸</a></p>\n</li>\n</ol>\n</section>"},{"author":"é™ˆè‹±å‘ Yingfa Chen","title":"ç¬¬ä¸€ä¸ªå¸–å­ï¼Œçå†™ç‚¹ä¸œè¥¿","date":"2023-05-17T15:00:00.000Z","thumbnail":"lillesand0.jpg","_content":"\nç°åœ¨æ˜¯ 2023 å¹´äº”æœˆåä¸ƒï¼Œé©¬ä¸Šç¡•å£«ä¸€å¹´çº§å°±ç»“æŸï¼Œåœ¨æ¸…åå›­å·²ç»å¿«äº”å¹´äº†ï¼Œæ„Ÿè§‰å¯¹æˆ‘äººç”Ÿçš„å½±å“çœŸçš„å·¨å¤§ã€‚è¿™ä¸€å¹´è®¤è¯†äº†å¾ˆå¯çˆ±çš„ 00ï¼Œå¸Œæœ›å¯ä»¥ä¸€ç›´èµ°ä¸‹å»ã€‚\n\næˆ‘å’Œ 00 çš„å­©å­ä»¬ï¼š\n\n\n<!-- more -->\n\n\n- å§é¾™ï¼šè°ƒçš®çš„è‚¥çŒ« ğŸ±\n- å°ç»¿ï¼šå–œæ¬¢å’¬ä¸œè¥¿çš„é³„é±¼ ğŸŠ\n- éª†é›ï¼šè¶…çº§å¤§çš„åœŸé¸¡ï¼ğŸ°\n- å‡¤é›ï¼šä¸è°ƒçš®çš„çŒ«å’ª ğŸ±\n- é»„å¸ï¼šæ›´å¤§çš„å·¨å…” ğŸ°\n- å†…å­˜æ¡ï¼šç™½è‰²çš„ç†Šç†Š ğŸ»\n- é—ªå…‰ç¯ï¼šç°è‰²çš„ç†Šç†Š ğŸ»\n\n## ç°åœ¨è¦åšçš„äº‹æƒ…\n\n- æŠŠ EmoRen æŠ•äº†\n\n  - èƒ½ä¸èƒ½è¡Œå•Š\n\n- è·‘ CFD çš„ä¸¹ç‚‰è°ƒå¥½\n\n  - å¥½éš¾å‘€\n\n- å†™å®Œä½œä¸š\n\n  - NLPå’ŒDLçš„å¤§ä½œä¸šï¼\n\n- æå®šå»ACLçš„æ‰‹ç»­\n\n  - å»åŠ æ‹¿å¤§ï¼Œç„¶åå›æŒªå¨ä¸€ä¸¤å‘¨ï¼Œç„¶åå›æ¥è·Ÿ 00 å»å—äº¬ï¼Œæˆ‘ä¸ç”¨ç­¾è¯ï¼Œä½†æ˜¯è¿˜æ˜¯æœ‰å¾ˆå¤šæ‰‹ç»­ã€‚\n\n- å†™å¥½å¼€é¢˜æŠ¥å‘Š\n\n  - è¿˜ä¸çŸ¥é“åšå•¥å‘¢\n\n\n## æˆ‘çš„å®¶ä¹¡ Lillesand\n\n![Lillesandï¼ŒæŒªå¨å—è¾¹çš„ä¸€ä¸ªæ²¿æµ·å°é•‡ï¼Œäººå£å¤§çº¦ä¸€ä¸‡ã€‚æˆ‘å‡ºç”Ÿé•¿å¤§çš„åœ°æ–¹ï¼Œåˆ°æœ¬ç§‘æ¥æ¸…åæ‰ç¦»å¼€çš„ã€‚](./ç¬¬ä¸€ä¸ªå¸–å­ï¼Œçå†™ç‚¹ä¸œè¥¿/lillesand0.jpg)\n\n![Lillesand çš„æ¸¯å£](./ç¬¬ä¸€ä¸ªå¸–å­ï¼Œçå†™ç‚¹ä¸œè¥¿/lillesand1.jpg)\n\nå¥½ä¹…æ²¡æœ‰å›å»äº†ï¼Œä¸Šä¸€æ¬¡å›æŒªå¨ä¹Ÿæ²¡æœ‰å›å»","source":"_posts/ç¬¬ä¸€ä¸ªå¸–å­ï¼Œçå†™ç‚¹ä¸œè¥¿.md","raw":"---\nauthor: é™ˆè‹±å‘ Yingfa Chen\ntitle: ç¬¬ä¸€ä¸ªå¸–å­ï¼Œçå†™ç‚¹ä¸œè¥¿\ndate: 2023-05-17 23:00:00\ncategories: Life\nthumbnail: lillesand0.jpg\ntags:\n- life\n- school\n- research\n- \"00\"\n- å­©å­ä»¬\n- å§é¾™\n- å‡¤é›\n- éª†é›\n- é»„å¸\n- å°ç»¿\n- çŒ«å’ª\n- åœŸé¸¡\n- ä¸­æ–‡\n- ç†Š\n- ğŸ»\n- ğŸ±\n- ğŸ‡\n- ğŸ°\n- ğŸŠ\n- emoren\n- cfd\n- acl\n- lillesand\n- åŠ æ‹¿å¤§\n---\n\nç°åœ¨æ˜¯ 2023 å¹´äº”æœˆåä¸ƒï¼Œé©¬ä¸Šç¡•å£«ä¸€å¹´çº§å°±ç»“æŸï¼Œåœ¨æ¸…åå›­å·²ç»å¿«äº”å¹´äº†ï¼Œæ„Ÿè§‰å¯¹æˆ‘äººç”Ÿçš„å½±å“çœŸçš„å·¨å¤§ã€‚è¿™ä¸€å¹´è®¤è¯†äº†å¾ˆå¯çˆ±çš„ 00ï¼Œå¸Œæœ›å¯ä»¥ä¸€ç›´èµ°ä¸‹å»ã€‚\n\næˆ‘å’Œ 00 çš„å­©å­ä»¬ï¼š\n\n\n<!-- more -->\n\n\n- å§é¾™ï¼šè°ƒçš®çš„è‚¥çŒ« ğŸ±\n- å°ç»¿ï¼šå–œæ¬¢å’¬ä¸œè¥¿çš„é³„é±¼ ğŸŠ\n- éª†é›ï¼šè¶…çº§å¤§çš„åœŸé¸¡ï¼ğŸ°\n- å‡¤é›ï¼šä¸è°ƒçš®çš„çŒ«å’ª ğŸ±\n- é»„å¸ï¼šæ›´å¤§çš„å·¨å…” ğŸ°\n- å†…å­˜æ¡ï¼šç™½è‰²çš„ç†Šç†Š ğŸ»\n- é—ªå…‰ç¯ï¼šç°è‰²çš„ç†Šç†Š ğŸ»\n\n## ç°åœ¨è¦åšçš„äº‹æƒ…\n\n- æŠŠ EmoRen æŠ•äº†\n\n  - èƒ½ä¸èƒ½è¡Œå•Š\n\n- è·‘ CFD çš„ä¸¹ç‚‰è°ƒå¥½\n\n  - å¥½éš¾å‘€\n\n- å†™å®Œä½œä¸š\n\n  - NLPå’ŒDLçš„å¤§ä½œä¸šï¼\n\n- æå®šå»ACLçš„æ‰‹ç»­\n\n  - å»åŠ æ‹¿å¤§ï¼Œç„¶åå›æŒªå¨ä¸€ä¸¤å‘¨ï¼Œç„¶åå›æ¥è·Ÿ 00 å»å—äº¬ï¼Œæˆ‘ä¸ç”¨ç­¾è¯ï¼Œä½†æ˜¯è¿˜æ˜¯æœ‰å¾ˆå¤šæ‰‹ç»­ã€‚\n\n- å†™å¥½å¼€é¢˜æŠ¥å‘Š\n\n  - è¿˜ä¸çŸ¥é“åšå•¥å‘¢\n\n\n## æˆ‘çš„å®¶ä¹¡ Lillesand\n\n![Lillesandï¼ŒæŒªå¨å—è¾¹çš„ä¸€ä¸ªæ²¿æµ·å°é•‡ï¼Œäººå£å¤§çº¦ä¸€ä¸‡ã€‚æˆ‘å‡ºç”Ÿé•¿å¤§çš„åœ°æ–¹ï¼Œåˆ°æœ¬ç§‘æ¥æ¸…åæ‰ç¦»å¼€çš„ã€‚](./ç¬¬ä¸€ä¸ªå¸–å­ï¼Œçå†™ç‚¹ä¸œè¥¿/lillesand0.jpg)\n\n![Lillesand çš„æ¸¯å£](./ç¬¬ä¸€ä¸ªå¸–å­ï¼Œçå†™ç‚¹ä¸œè¥¿/lillesand1.jpg)\n\nå¥½ä¹…æ²¡æœ‰å›å»äº†ï¼Œä¸Šä¸€æ¬¡å›æŒªå¨ä¹Ÿæ²¡æœ‰å›å»","slug":"ç¬¬ä¸€ä¸ªå¸–å­ï¼Œçå†™ç‚¹ä¸œè¥¿","published":1,"updated":"2024-02-26T02:56:21.456Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cltl4oxel000rxh7kh777euay","content":"<p>ç°åœ¨æ˜¯ 2023 å¹´äº”æœˆåä¸ƒï¼Œé©¬ä¸Šç¡•å£«ä¸€å¹´çº§å°±ç»“æŸï¼Œåœ¨æ¸…åå›­å·²ç»å¿«äº”å¹´äº†ï¼Œæ„Ÿè§‰å¯¹æˆ‘äººç”Ÿçš„å½±å“çœŸçš„å·¨å¤§ã€‚è¿™ä¸€å¹´è®¤è¯†äº†å¾ˆå¯çˆ±çš„ 00ï¼Œå¸Œæœ›å¯ä»¥ä¸€ç›´èµ°ä¸‹å»ã€‚</p>\n<p>æˆ‘å’Œ 00 çš„å­©å­ä»¬ï¼š</p>\n<span id=\"more\"></span>\n<ul>\n<li>å§é¾™ï¼šè°ƒçš®çš„è‚¥çŒ« ğŸ±</li>\n<li>å°ç»¿ï¼šå–œæ¬¢å’¬ä¸œè¥¿çš„é³„é±¼ ğŸŠ</li>\n<li>éª†é›ï¼šè¶…çº§å¤§çš„åœŸé¸¡ï¼ğŸ°</li>\n<li>å‡¤é›ï¼šä¸è°ƒçš®çš„çŒ«å’ª ğŸ±</li>\n<li>é»„å¸ï¼šæ›´å¤§çš„å·¨å…” ğŸ°</li>\n<li>å†…å­˜æ¡ï¼šç™½è‰²çš„ç†Šç†Š ğŸ»</li>\n<li>é—ªå…‰ç¯ï¼šç°è‰²çš„ç†Šç†Š ğŸ»</li>\n</ul>\n<h2 id=\"ç°åœ¨è¦åšçš„äº‹æƒ…\">ç°åœ¨è¦åšçš„äº‹æƒ…</h2>\n<ul>\n<li>\n<p>æŠŠ EmoRen æŠ•äº†</p>\n<ul>\n<li>èƒ½ä¸èƒ½è¡Œå•Š</li>\n</ul>\n</li>\n<li>\n<p>è·‘ CFD çš„ä¸¹ç‚‰è°ƒå¥½</p>\n<ul>\n<li>å¥½éš¾å‘€</li>\n</ul>\n</li>\n<li>\n<p>å†™å®Œä½œä¸š</p>\n<ul>\n<li>NLPå’ŒDLçš„å¤§ä½œä¸šï¼</li>\n</ul>\n</li>\n<li>\n<p>æå®šå»ACLçš„æ‰‹ç»­</p>\n<ul>\n<li>å»åŠ æ‹¿å¤§ï¼Œç„¶åå›æŒªå¨ä¸€ä¸¤å‘¨ï¼Œç„¶åå›æ¥è·Ÿ 00 å»å—äº¬ï¼Œæˆ‘ä¸ç”¨ç­¾è¯ï¼Œä½†æ˜¯è¿˜æ˜¯æœ‰å¾ˆå¤šæ‰‹ç»­ã€‚</li>\n</ul>\n</li>\n<li>\n<p>å†™å¥½å¼€é¢˜æŠ¥å‘Š</p>\n<ul>\n<li>è¿˜ä¸çŸ¥é“åšå•¥å‘¢</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"æˆ‘çš„å®¶ä¹¡-Lillesand\">æˆ‘çš„å®¶ä¹¡ Lillesand</h2>\n<p><img src=\"/2023/05/17/%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%B8%96%E5%AD%90%EF%BC%8C%E7%9E%8E%E5%86%99%E7%82%B9%E4%B8%9C%E8%A5%BF/lillesand0.jpg\" alt=\"Lillesandï¼ŒæŒªå¨å—è¾¹çš„ä¸€ä¸ªæ²¿æµ·å°é•‡ï¼Œäººå£å¤§çº¦ä¸€ä¸‡ã€‚æˆ‘å‡ºç”Ÿé•¿å¤§çš„åœ°æ–¹ï¼Œåˆ°æœ¬ç§‘æ¥æ¸…åæ‰ç¦»å¼€çš„ã€‚\"></p>\n<p><img src=\"/2023/05/17/%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%B8%96%E5%AD%90%EF%BC%8C%E7%9E%8E%E5%86%99%E7%82%B9%E4%B8%9C%E8%A5%BF/lillesand1.jpg\" alt=\"Lillesand çš„æ¸¯å£\"></p>\n<p>å¥½ä¹…æ²¡æœ‰å›å»äº†ï¼Œä¸Šä¸€æ¬¡å›æŒªå¨ä¹Ÿæ²¡æœ‰å›å»</p>\n","site":{"data":{}},"excerpt":"<p>ç°åœ¨æ˜¯ 2023 å¹´äº”æœˆåä¸ƒï¼Œé©¬ä¸Šç¡•å£«ä¸€å¹´çº§å°±ç»“æŸï¼Œåœ¨æ¸…åå›­å·²ç»å¿«äº”å¹´äº†ï¼Œæ„Ÿè§‰å¯¹æˆ‘äººç”Ÿçš„å½±å“çœŸçš„å·¨å¤§ã€‚è¿™ä¸€å¹´è®¤è¯†äº†å¾ˆå¯çˆ±çš„ 00ï¼Œå¸Œæœ›å¯ä»¥ä¸€ç›´èµ°ä¸‹å»ã€‚</p>\n<p>æˆ‘å’Œ 00 çš„å­©å­ä»¬ï¼š</p>","more":"<ul>\n<li>å§é¾™ï¼šè°ƒçš®çš„è‚¥çŒ« ğŸ±</li>\n<li>å°ç»¿ï¼šå–œæ¬¢å’¬ä¸œè¥¿çš„é³„é±¼ ğŸŠ</li>\n<li>éª†é›ï¼šè¶…çº§å¤§çš„åœŸé¸¡ï¼ğŸ°</li>\n<li>å‡¤é›ï¼šä¸è°ƒçš®çš„çŒ«å’ª ğŸ±</li>\n<li>é»„å¸ï¼šæ›´å¤§çš„å·¨å…” ğŸ°</li>\n<li>å†…å­˜æ¡ï¼šç™½è‰²çš„ç†Šç†Š ğŸ»</li>\n<li>é—ªå…‰ç¯ï¼šç°è‰²çš„ç†Šç†Š ğŸ»</li>\n</ul>\n<h2 id=\"ç°åœ¨è¦åšçš„äº‹æƒ…\">ç°åœ¨è¦åšçš„äº‹æƒ…</h2>\n<ul>\n<li>\n<p>æŠŠ EmoRen æŠ•äº†</p>\n<ul>\n<li>èƒ½ä¸èƒ½è¡Œå•Š</li>\n</ul>\n</li>\n<li>\n<p>è·‘ CFD çš„ä¸¹ç‚‰è°ƒå¥½</p>\n<ul>\n<li>å¥½éš¾å‘€</li>\n</ul>\n</li>\n<li>\n<p>å†™å®Œä½œä¸š</p>\n<ul>\n<li>NLPå’ŒDLçš„å¤§ä½œä¸šï¼</li>\n</ul>\n</li>\n<li>\n<p>æå®šå»ACLçš„æ‰‹ç»­</p>\n<ul>\n<li>å»åŠ æ‹¿å¤§ï¼Œç„¶åå›æŒªå¨ä¸€ä¸¤å‘¨ï¼Œç„¶åå›æ¥è·Ÿ 00 å»å—äº¬ï¼Œæˆ‘ä¸ç”¨ç­¾è¯ï¼Œä½†æ˜¯è¿˜æ˜¯æœ‰å¾ˆå¤šæ‰‹ç»­ã€‚</li>\n</ul>\n</li>\n<li>\n<p>å†™å¥½å¼€é¢˜æŠ¥å‘Š</p>\n<ul>\n<li>è¿˜ä¸çŸ¥é“åšå•¥å‘¢</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"æˆ‘çš„å®¶ä¹¡-Lillesand\">æˆ‘çš„å®¶ä¹¡ Lillesand</h2>\n<p><img src=\"/2023/05/17/%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%B8%96%E5%AD%90%EF%BC%8C%E7%9E%8E%E5%86%99%E7%82%B9%E4%B8%9C%E8%A5%BF/lillesand0.jpg\" alt=\"Lillesandï¼ŒæŒªå¨å—è¾¹çš„ä¸€ä¸ªæ²¿æµ·å°é•‡ï¼Œäººå£å¤§çº¦ä¸€ä¸‡ã€‚æˆ‘å‡ºç”Ÿé•¿å¤§çš„åœ°æ–¹ï¼Œåˆ°æœ¬ç§‘æ¥æ¸…åæ‰ç¦»å¼€çš„ã€‚\"></p>\n<p><img src=\"/2023/05/17/%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%B8%96%E5%AD%90%EF%BC%8C%E7%9E%8E%E5%86%99%E7%82%B9%E4%B8%9C%E8%A5%BF/lillesand1.jpg\" alt=\"Lillesand çš„æ¸¯å£\"></p>\n<p>å¥½ä¹…æ²¡æœ‰å›å»äº†ï¼Œä¸Šä¸€æ¬¡å›æŒªå¨ä¹Ÿæ²¡æœ‰å›å»</p>"},{"layout":"post","author":"é™ˆè‹±å‘ Yingfa Chen","title":"ç¬¬ä¸€ç¯‡","date":"2022-10-27T06:04:16.000Z","_content":"\nä¹‹å‰å°è¯•ç”¨ Hugo æ¥éƒ¨ç½²ï¼Œå‘ç° Hugo ä¸ä»…æŒºå¤æ‚ï¼Œè€Œä¸”è¿˜æœ‰å¾ˆå¤šå°é—®é¢˜ï¼Œå¯èƒ½è¿™å°±æ˜¯é€Ÿåº¦å¸¦æ¥çš„ä»£ä»·å§ã€‚ä½†æ˜¯å…¶å®æˆ‘ä¹Ÿä¸æ˜¯å†™å¾ˆå¤šå†…å®¹ï¼Œæ‰€ä»¥ Jekyll çš„é€Ÿåº¦åº”è¯¥æ˜¯å¤Ÿç”¨çš„ã€‚\n\nJekyll æ”¯æŒåœ¨ markdown å†…å®¹é‡Œé¢ç”¨ Liquid template tags æ¥ç”ŸæˆåŠ¨æ€å†…å®¹ï¼Œæ¯”å¦‚æ ¹æ® front matter ä¸­çš„ tagsï¼Œç»™æ¯ä¸ª tag ç”Ÿæˆ html divã€‚å¦‚ä¸‹ liquid è¯­æ³•ï¼š\n\n{% raw %}\n```html\n<div>\n{% for tag in site.tags %}\n    <a style=\"background-color: blue;\">#{{ tag[0] }}</a>\n{% endfor %}\n</div>\n```\n{% endraw %}\n\n<!-- more -->\n\nä¼šæ ¹æ® post markdown æ–‡ä»¶ä¸­çš„ front matter ä¸­å®šä¹‰çš„ tagsï¼š\n\n```markdown\n---\ntags: some tags here\n---\n```\n\nç”Ÿæˆç›¸åº”çš„ html divï¼š\n\n```html\n<div class=\"post-tags\">\n    <a href=\"/tags/life\" class=\"tag-card\">life</a>\n    <a href=\"/tags/update\" class=\"tag-card\">update</a>\n    <a href=\"/tags/learn\" class=\"tag-card\">learn</a>\n    <a href=\"/tags/important\" class=\"tag-card\">important</a>\n    <a href=\"/tags/jekyll\" class=\"tag-card\">jekyll</a>\n    <a href=\"/tags/hugo\" class=\"tag-card\">hugo</a>\n    <a href=\"/tags/static-site-generator\" class=\"tag-card\">static-site-generator</a>\n</div>\n```\n\n---\n\nä¸çŸ¥é“å†™å•¥ï¼Œå°±å†™ä¸€ä¸ª Python çš„äºŒåˆ†æœç´¢å§ï¼š\n\n```python\ndef bin_search(arr: list, target: Any) -> int:\n    '''\n    Return the smallest index such that when target is inserted at that index,\n    the array will remain sorted.\n    '''\n    lo, hi = 0, len(arr)\n    while lo < hi:\n        m = (lo + hi) // 2\n        if arr[m] < target:\n            lo = m + 1\n        else:\n            hi = m\n    return lo\n```","source":"_posts/ç¬¬ä¸€ç¯‡.md","raw":"---\nlayout: post\nauthor: é™ˆè‹±å‘ Yingfa Chen\ntitle: ç¬¬ä¸€ç¯‡\ndate: 2022-10-27 14:04:16 +0800\ncategories: Life\ntags: \n- life\n- jekyll\n- test\n- hugo\n- static-site-generator\n- ä¸­æ–‡\n- html\n- liquid\n---\n\nä¹‹å‰å°è¯•ç”¨ Hugo æ¥éƒ¨ç½²ï¼Œå‘ç° Hugo ä¸ä»…æŒºå¤æ‚ï¼Œè€Œä¸”è¿˜æœ‰å¾ˆå¤šå°é—®é¢˜ï¼Œå¯èƒ½è¿™å°±æ˜¯é€Ÿåº¦å¸¦æ¥çš„ä»£ä»·å§ã€‚ä½†æ˜¯å…¶å®æˆ‘ä¹Ÿä¸æ˜¯å†™å¾ˆå¤šå†…å®¹ï¼Œæ‰€ä»¥ Jekyll çš„é€Ÿåº¦åº”è¯¥æ˜¯å¤Ÿç”¨çš„ã€‚\n\nJekyll æ”¯æŒåœ¨ markdown å†…å®¹é‡Œé¢ç”¨ Liquid template tags æ¥ç”ŸæˆåŠ¨æ€å†…å®¹ï¼Œæ¯”å¦‚æ ¹æ® front matter ä¸­çš„ tagsï¼Œç»™æ¯ä¸ª tag ç”Ÿæˆ html divã€‚å¦‚ä¸‹ liquid è¯­æ³•ï¼š\n\n{% raw %}\n```html\n<div>\n{% for tag in site.tags %}\n    <a style=\"background-color: blue;\">#{{ tag[0] }}</a>\n{% endfor %}\n</div>\n```\n{% endraw %}\n\n<!-- more -->\n\nä¼šæ ¹æ® post markdown æ–‡ä»¶ä¸­çš„ front matter ä¸­å®šä¹‰çš„ tagsï¼š\n\n```markdown\n---\ntags: some tags here\n---\n```\n\nç”Ÿæˆç›¸åº”çš„ html divï¼š\n\n```html\n<div class=\"post-tags\">\n    <a href=\"/tags/life\" class=\"tag-card\">life</a>\n    <a href=\"/tags/update\" class=\"tag-card\">update</a>\n    <a href=\"/tags/learn\" class=\"tag-card\">learn</a>\n    <a href=\"/tags/important\" class=\"tag-card\">important</a>\n    <a href=\"/tags/jekyll\" class=\"tag-card\">jekyll</a>\n    <a href=\"/tags/hugo\" class=\"tag-card\">hugo</a>\n    <a href=\"/tags/static-site-generator\" class=\"tag-card\">static-site-generator</a>\n</div>\n```\n\n---\n\nä¸çŸ¥é“å†™å•¥ï¼Œå°±å†™ä¸€ä¸ª Python çš„äºŒåˆ†æœç´¢å§ï¼š\n\n```python\ndef bin_search(arr: list, target: Any) -> int:\n    '''\n    Return the smallest index such that when target is inserted at that index,\n    the array will remain sorted.\n    '''\n    lo, hi = 0, len(arr)\n    while lo < hi:\n        m = (lo + hi) // 2\n        if arr[m] < target:\n            lo = m + 1\n        else:\n            hi = m\n    return lo\n```","slug":"ç¬¬ä¸€ç¯‡","published":1,"updated":"2024-01-10T07:24:56.516Z","comments":1,"photos":[],"link":"","_id":"cltl4oxel000sxh7k05fk4sya","content":"<p>ä¹‹å‰å°è¯•ç”¨ Hugo æ¥éƒ¨ç½²ï¼Œå‘ç° Hugo ä¸ä»…æŒºå¤æ‚ï¼Œè€Œä¸”è¿˜æœ‰å¾ˆå¤šå°é—®é¢˜ï¼Œå¯èƒ½è¿™å°±æ˜¯é€Ÿåº¦å¸¦æ¥çš„ä»£ä»·å§ã€‚ä½†æ˜¯å…¶å®æˆ‘ä¹Ÿä¸æ˜¯å†™å¾ˆå¤šå†…å®¹ï¼Œæ‰€ä»¥ Jekyll çš„é€Ÿåº¦åº”è¯¥æ˜¯å¤Ÿç”¨çš„ã€‚</p>\n<p>Jekyll æ”¯æŒåœ¨ markdown å†…å®¹é‡Œé¢ç”¨ Liquid template tags æ¥ç”ŸæˆåŠ¨æ€å†…å®¹ï¼Œæ¯”å¦‚æ ¹æ® front matter ä¸­çš„ tagsï¼Œç»™æ¯ä¸ª tag ç”Ÿæˆ html divã€‚å¦‚ä¸‹ liquid è¯­æ³•ï¼š</p>\n\n<figure class=\"highlight html\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\">{% for tag in site.tags %}</span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">a</span> <span class=\"attr\">style</span>=<span class=\"string\">\"background-color: blue;\"</span>&gt;</span>#{{ tag[0] }}<span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span></span><br><span class=\"line\">{% endfor %}</span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br></pre></td></tr></tbody></table></figure>\n\n<span id=\"more\"></span>\n<p>ä¼šæ ¹æ® post markdown æ–‡ä»¶ä¸­çš„ front matter ä¸­å®šä¹‰çš„ tagsï¼š</p>\n<figure class=\"highlight markdown\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">---</span><br><span class=\"line\"><span class=\"section\">tags: some tags here</span></span><br><span class=\"line\"><span class=\"section\">---</span></span><br></pre></td></tr></tbody></table></figure>\n<p>ç”Ÿæˆç›¸åº”çš„ html divï¼š</p>\n<figure class=\"highlight html\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">\"post-tags\"</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">a</span> <span class=\"attr\">href</span>=<span class=\"string\">\"/tags/life\"</span> <span class=\"attr\">class</span>=<span class=\"string\">\"tag-card\"</span>&gt;</span>life<span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">a</span> <span class=\"attr\">href</span>=<span class=\"string\">\"/tags/update\"</span> <span class=\"attr\">class</span>=<span class=\"string\">\"tag-card\"</span>&gt;</span>update<span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">a</span> <span class=\"attr\">href</span>=<span class=\"string\">\"/tags/learn\"</span> <span class=\"attr\">class</span>=<span class=\"string\">\"tag-card\"</span>&gt;</span>learn<span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">a</span> <span class=\"attr\">href</span>=<span class=\"string\">\"/tags/important\"</span> <span class=\"attr\">class</span>=<span class=\"string\">\"tag-card\"</span>&gt;</span>important<span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">a</span> <span class=\"attr\">href</span>=<span class=\"string\">\"/tags/jekyll\"</span> <span class=\"attr\">class</span>=<span class=\"string\">\"tag-card\"</span>&gt;</span>jekyll<span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">a</span> <span class=\"attr\">href</span>=<span class=\"string\">\"/tags/hugo\"</span> <span class=\"attr\">class</span>=<span class=\"string\">\"tag-card\"</span>&gt;</span>hugo<span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">a</span> <span class=\"attr\">href</span>=<span class=\"string\">\"/tags/static-site-generator\"</span> <span class=\"attr\">class</span>=<span class=\"string\">\"tag-card\"</span>&gt;</span>static-site-generator<span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br></pre></td></tr></tbody></table></figure>\n<hr>\n<p>ä¸çŸ¥é“å†™å•¥ï¼Œå°±å†™ä¸€ä¸ª Python çš„äºŒåˆ†æœç´¢å§ï¼š</p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">bin_search</span>(<span class=\"params\">arr: <span class=\"built_in\">list</span>, target: <span class=\"type\">Any</span></span>) -&gt; <span class=\"built_in\">int</span>:</span><br><span class=\"line\">    <span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">    Return the smallest index such that when target is inserted at that index,</span></span><br><span class=\"line\"><span class=\"string\">    the array will remain sorted.</span></span><br><span class=\"line\"><span class=\"string\">    '''</span></span><br><span class=\"line\">    lo, hi = <span class=\"number\">0</span>, <span class=\"built_in\">len</span>(arr)</span><br><span class=\"line\">    <span class=\"keyword\">while</span> lo &lt; hi:</span><br><span class=\"line\">        m = (lo + hi) // <span class=\"number\">2</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> arr[m] &lt; target:</span><br><span class=\"line\">            lo = m + <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            hi = m</span><br><span class=\"line\">    <span class=\"keyword\">return</span> lo</span><br></pre></td></tr></tbody></table></figure>","site":{"data":{}},"excerpt":"<p>ä¹‹å‰å°è¯•ç”¨ Hugo æ¥éƒ¨ç½²ï¼Œå‘ç° Hugo ä¸ä»…æŒºå¤æ‚ï¼Œè€Œä¸”è¿˜æœ‰å¾ˆå¤šå°é—®é¢˜ï¼Œå¯èƒ½è¿™å°±æ˜¯é€Ÿåº¦å¸¦æ¥çš„ä»£ä»·å§ã€‚ä½†æ˜¯å…¶å®æˆ‘ä¹Ÿä¸æ˜¯å†™å¾ˆå¤šå†…å®¹ï¼Œæ‰€ä»¥ Jekyll çš„é€Ÿåº¦åº”è¯¥æ˜¯å¤Ÿç”¨çš„ã€‚</p>\n<p>Jekyll æ”¯æŒåœ¨ markdown å†…å®¹é‡Œé¢ç”¨ Liquid template tags æ¥ç”ŸæˆåŠ¨æ€å†…å®¹ï¼Œæ¯”å¦‚æ ¹æ® front matter ä¸­çš„ tagsï¼Œç»™æ¯ä¸ª tag ç”Ÿæˆ html divã€‚å¦‚ä¸‹ liquid è¯­æ³•ï¼š</p>\n\n<figure class=\"highlight html\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\">{% for tag in site.tags %}</span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">a</span> <span class=\"attr\">style</span>=<span class=\"string\">\"background-color: blue;\"</span>&gt;</span>#{{ tag[0] }}<span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span></span><br><span class=\"line\">{% endfor %}</span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br></pre></td></tr></tbody></table></figure>","more":"<p>ä¼šæ ¹æ® post markdown æ–‡ä»¶ä¸­çš„ front matter ä¸­å®šä¹‰çš„ tagsï¼š</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">---</span><br><span class=\"line\"><span class=\"section\">tags: some tags here</span></span><br><span class=\"line\"><span class=\"section\">---</span></span><br></pre></td></tr></table></figure>\n<p>ç”Ÿæˆç›¸åº”çš„ html divï¼š</p>\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;post-tags&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">a</span> <span class=\"attr\">href</span>=<span class=\"string\">&quot;/tags/life&quot;</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;tag-card&quot;</span>&gt;</span>life<span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">a</span> <span class=\"attr\">href</span>=<span class=\"string\">&quot;/tags/update&quot;</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;tag-card&quot;</span>&gt;</span>update<span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">a</span> <span class=\"attr\">href</span>=<span class=\"string\">&quot;/tags/learn&quot;</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;tag-card&quot;</span>&gt;</span>learn<span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">a</span> <span class=\"attr\">href</span>=<span class=\"string\">&quot;/tags/important&quot;</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;tag-card&quot;</span>&gt;</span>important<span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">a</span> <span class=\"attr\">href</span>=<span class=\"string\">&quot;/tags/jekyll&quot;</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;tag-card&quot;</span>&gt;</span>jekyll<span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">a</span> <span class=\"attr\">href</span>=<span class=\"string\">&quot;/tags/hugo&quot;</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;tag-card&quot;</span>&gt;</span>hugo<span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">a</span> <span class=\"attr\">href</span>=<span class=\"string\">&quot;/tags/static-site-generator&quot;</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;tag-card&quot;</span>&gt;</span>static-site-generator<span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<hr>\n<p>ä¸çŸ¥é“å†™å•¥ï¼Œå°±å†™ä¸€ä¸ª Python çš„äºŒåˆ†æœç´¢å§ï¼š</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">bin_search</span>(<span class=\"params\">arr: <span class=\"built_in\">list</span>, target: <span class=\"type\">Any</span></span>) -&gt; <span class=\"built_in\">int</span>:</span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    Return the smallest index such that when target is inserted at that index,</span></span><br><span class=\"line\"><span class=\"string\">    the array will remain sorted.</span></span><br><span class=\"line\"><span class=\"string\">    &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    lo, hi = <span class=\"number\">0</span>, <span class=\"built_in\">len</span>(arr)</span><br><span class=\"line\">    <span class=\"keyword\">while</span> lo &lt; hi:</span><br><span class=\"line\">        m = (lo + hi) // <span class=\"number\">2</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> arr[m] &lt; target:</span><br><span class=\"line\">            lo = m + <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            hi = m</span><br><span class=\"line\">    <span class=\"keyword\">return</span> lo</span><br></pre></td></tr></table></figure>"},{"author":"é™ˆè‹±å‘ Yingfa Chen","title":"(EREN) Robust and Scalable Model Editing for Large Language Models","date":"2023-09-14T11:39:34.000Z","featured":true,"_content":"\nThis is my first ever stand-alone research work, I hope it can be accepted at EMNLP 2023, but judging from its review scores, it can only get EMNLP Findings. But that's acceptable.\n\n\n<!-- more -->\n\n\n> The first reviewer actually lowered the \"soundness\" score after rebuttal! WHY!\n\n[OpenReview link](https://openreview.net/forum?id=vDUsGqCIyb&noteId=K2D6oCGNlE)\n\n> TLDR: A reader is augmented with a growing notebook that caches all edits in natural texts, and the reader retrieves relevant edits and make inference based on them. This achieves SOTA in model editing in QA and fact-checking.\n\n***\n\n<strong>Abstract</strong>\n\n\n<div style=\"text-align: justify;\">\nThe memorized knowledge of large language models (LLMs) may not be consistent with that of the real world, and the model may produce undesired behaviors or incorrect predictions. Therefore, there is a need for model editing, i.e., modifying the behavior of an LLM on specific examples while preserving its performance on unrelated instances. Existing LLM editing methods modify the behavior by adding a prompt to the input, which is not scalable due to the limited length of the prompt and may negatively affect the performance on unrelated instances. In this paper, we propose EREN (Edit models by REading Notes) to improve the scalability and robustness of LLM editing. Specifically, EREN complements the LLM with a notebook that contains all edits in natural text and retrieves the relevant edits for a given input. To avoid the negative effect of irrelevant edits, we propose a two-step reading comprehension procedure to determine whether there is a relevant edit for the input. If not found, the LLM will make predictions directly. Empirical results on question-answering and fact-checking show that our method outperforms current state-of-the-art methods by a large margin. Unlike existing techniques, it can integrate knowledge from multiple edits, and correctly respond to syntactically similar but semantically unrelated inputs (and vice versa).\n</div>\n\n\n## Introduction\n\n...\n\n","source":"_drafts/EREN.md","raw":"---\nauthor: é™ˆè‹±å‘ Yingfa Chen\ntitle: \"(EREN) Robust and Scalable Model Editing for Large Language Models\"\ndate: 2023-09-14 19:39:34\ncategories: Paper\ntags:\n- paper\n- arxiv\n- llm\n- knowledge\n- research\n- emnlp\n- english\n- model editing\n- in-context-learning\n- ai\n- serac\n- rome\n- eren\n- mend\nfeatured: true\n---\n\nThis is my first ever stand-alone research work, I hope it can be accepted at EMNLP 2023, but judging from its review scores, it can only get EMNLP Findings. But that's acceptable.\n\n\n<!-- more -->\n\n\n> The first reviewer actually lowered the \"soundness\" score after rebuttal! WHY!\n\n[OpenReview link](https://openreview.net/forum?id=vDUsGqCIyb&noteId=K2D6oCGNlE)\n\n> TLDR: A reader is augmented with a growing notebook that caches all edits in natural texts, and the reader retrieves relevant edits and make inference based on them. This achieves SOTA in model editing in QA and fact-checking.\n\n***\n\n<strong>Abstract</strong>\n\n\n<div style=\"text-align: justify;\">\nThe memorized knowledge of large language models (LLMs) may not be consistent with that of the real world, and the model may produce undesired behaviors or incorrect predictions. Therefore, there is a need for model editing, i.e., modifying the behavior of an LLM on specific examples while preserving its performance on unrelated instances. Existing LLM editing methods modify the behavior by adding a prompt to the input, which is not scalable due to the limited length of the prompt and may negatively affect the performance on unrelated instances. In this paper, we propose EREN (Edit models by REading Notes) to improve the scalability and robustness of LLM editing. Specifically, EREN complements the LLM with a notebook that contains all edits in natural text and retrieves the relevant edits for a given input. To avoid the negative effect of irrelevant edits, we propose a two-step reading comprehension procedure to determine whether there is a relevant edit for the input. If not found, the LLM will make predictions directly. Empirical results on question-answering and fact-checking show that our method outperforms current state-of-the-art methods by a large margin. Unlike existing techniques, it can integrate knowledge from multiple edits, and correctly respond to syntactically similar but semantically unrelated inputs (and vice versa).\n</div>\n\n\n## Introduction\n\n...\n\n","slug":"EREN","published":0,"updated":"2024-01-23T11:09:17.490Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cltl4oxem000wxh7kge2td07k","content":"<p>This is my first ever stand-alone research work, I hope it can be accepted at EMNLP 2023, but judging from its review scores, it can only get EMNLP Findings. But that's acceptable.</p>\n<span id=\"more\"></span>\n<blockquote>\n<p>The first reviewer actually lowered the \"soundness\" score after rebuttal! WHY!</p>\n</blockquote>\n<p><a href=\"https://openreview.net/forum?id=vDUsGqCIyb&amp;noteId=K2D6oCGNlE\">OpenReview link</a></p>\n<blockquote>\n<p>TLDR: A reader is augmented with a growing notebook that caches all edits in natural texts, and the reader retrieves relevant edits and make inference based on them. This achieves SOTA in model editing in QA and fact-checking.</p>\n</blockquote>\n<hr>\n<p><strong>Abstract</strong></p>\n<div style=\"text-align: justify;\">\nThe memorized knowledge of large language models (LLMs) may not be consistent with that of the real world, and the model may produce undesired behaviors or incorrect predictions. Therefore, there is a need for model editing, i.e., modifying the behavior of an LLM on specific examples while preserving its performance on unrelated instances. Existing LLM editing methods modify the behavior by adding a prompt to the input, which is not scalable due to the limited length of the prompt and may negatively affect the performance on unrelated instances. In this paper, we propose EREN (Edit models by REading Notes) to improve the scalability and robustness of LLM editing. Specifically, EREN complements the LLM with a notebook that contains all edits in natural text and retrieves the relevant edits for a given input. To avoid the negative effect of irrelevant edits, we propose a two-step reading comprehension procedure to determine whether there is a relevant edit for the input. If not found, the LLM will make predictions directly. Empirical results on question-answering and fact-checking show that our method outperforms current state-of-the-art methods by a large margin. Unlike existing techniques, it can integrate knowledge from multiple edits, and correctly respond to syntactically similar but semantically unrelated inputs (and vice versa).\n</div>\n<h2 id=\"Introduction\">Introduction</h2>\n<p>...</p>\n","site":{"data":{}},"excerpt":"<p>This is my first ever stand-alone research work, I hope it can be accepted at EMNLP 2023, but judging from its review scores, it can only get EMNLP Findings. But that's acceptable.</p>","more":"<blockquote>\n<p>The first reviewer actually lowered the &quot;soundness&quot; score after rebuttal! WHY!</p>\n</blockquote>\n<p><a href=\"https://openreview.net/forum?id=vDUsGqCIyb&amp;noteId=K2D6oCGNlE\">OpenReview link</a></p>\n<blockquote>\n<p>TLDR: A reader is augmented with a growing notebook that caches all edits in natural texts, and the reader retrieves relevant edits and make inference based on them. This achieves SOTA in model editing in QA and fact-checking.</p>\n</blockquote>\n<hr>\n<p><strong>Abstract</strong></p>\n<div style=\"text-align: justify;\">\nThe memorized knowledge of large language models (LLMs) may not be consistent with that of the real world, and the model may produce undesired behaviors or incorrect predictions. Therefore, there is a need for model editing, i.e., modifying the behavior of an LLM on specific examples while preserving its performance on unrelated instances. Existing LLM editing methods modify the behavior by adding a prompt to the input, which is not scalable due to the limited length of the prompt and may negatively affect the performance on unrelated instances. In this paper, we propose EREN (Edit models by REading Notes) to improve the scalability and robustness of LLM editing. Specifically, EREN complements the LLM with a notebook that contains all edits in natural text and retrieves the relevant edits for a given input. To avoid the negative effect of irrelevant edits, we propose a two-step reading comprehension procedure to determine whether there is a relevant edit for the input. If not found, the LLM will make predictions directly. Empirical results on question-answering and fact-checking show that our method outperforms current state-of-the-art methods by a large margin. Unlike existing techniques, it can integrate knowledge from multiple edits, and correctly respond to syntactically similar but semantically unrelated inputs (and vice versa).\n</div>\n<h2 id=\"Introduction\">Introduction</h2>\n<p>...</p>"},{"author":"é™ˆè‹±å‘ Yingfa Chen","title":"è¯­è¨€æ¨¡å‹çš„è¿›åŒ–","date":"2024-02-01T09:51:30.000Z","draft":true,"_content":"<style>\nsection {\n  font-size: 24px;\n}\n</style>\n\nè‡ªä» Transformer é—®ä¸–ä»¥æ¥ï¼ŒåŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„è¯­è¨€æ¨¡å‹å–å¾—äº†æå¤§çš„æˆåŠŸã€‚ç°åœ¨çš„å¤§è¯­è¨€æ¨¡å‹ï¼Œå¦‚ GPTã€LLaMaã€Mistraléƒ½æ˜¯åŸºäº Transformer æ¶æ„çš„ã€‚æœ€è¿‘æœ‰è¯¸å¤šç ”ç©¶å·¥ä½œæå‡ºå¯èƒ½å¯ä»¥æ›¿ä»£ transformer æ¶æ„çš„æ–°æ¶æ„ã€‚æœ¬æ–‡æ•´ç†ä¸€ä¸‹è¿‘äº›å¹´è¯­è¨€æ¨¡å‹çš„å˜åŒ–ã€‚\n\n<!-- more -->\n\n# è¯­è¨€æ¨¡å‹\n\nè¯­è¨€æ¨¡å‹ï¼ˆlanguage modelï¼‰æ˜¯ä¸€ä¸ªå¯¹è¯­è¨€çš„æ¦‚ç‡æ¨¡å‹ï¼Œé¦–æ¬¡åœ¨å…«åå¹´ä»£æå‡ºã€‚è¯­è¨€æ¨¡å‹ä¸“æ³¨äºé¢„æµ‹ä¸€æ®µæ–‡å­—å‡ºç°çš„æ¦‚ç‡ã€‚\n\n$$\nP(s) = P(w_{1}, \\dots, w_{m}) = \\prod_{i}^{m} P(w_i|w_{1},\\dots, w_{i-1})\n$$\n\nå…¶ä¸­ $w_i$ ä¸€èˆ¬æ˜¯ tokenï¼Œè€Œä¸€èˆ¬ä¸€ä¸ª token å°±æ˜¯ä¸€ä¸ªå•è¯ã€‚åœ¨ä»¥ä¸‹çš„è®¨è®ºä¸­ï¼Œå¦‚æœæ²¡æœ‰ç‰¹åˆ«è¯´æ˜ï¼Œæˆ‘é»˜è®¤ä¸€ä¸ª token å°±æ˜¯ä¸€ä¸ªå•è¯ï¼Œè€Œä¸€æ®µæ–‡å­—ï¼ˆæ¯”å¦‚ä¸€ä¸ªå¥å­ï¼‰çš„ä¸å¯åˆ†å‰²å•å…ƒå°±æ˜¯ä¸€ä¸ªå•è¯ã€‚\n\næ¢å¥è¯è¯´ï¼Œè¿™ä¸ªé—®é¢˜å¯ä»¥è½¬åŒ–ä¸ºï¼Œç»™å®šä¸€ä¸ªåºåˆ— $w_{<t}$ï¼Œè®¡ç®—ä¸‹ä¸€ä¸ªå•è¯çš„æ¦‚ç‡åˆ†å¸ƒï¼š\n\n$$\nP(w_{t}|w_{<t}) \\in \\mathbb R^V\n$$\n\nå…¶ä¸­ $V$ æ˜¯è¯è¡¨å¤§å°ï¼Œ$t$ æ˜¯å½“å‰è¦é¢„æµ‹çš„å•è¯çš„ä½ç½®ã€‚è¿™ä¸ªç›®æ ‡å«åš next token predictionã€‚\n\n# è¯­è¨€æ¨¡å‹çš„åº”ç”¨\n\nå­¦ä¹ è¯­è¨€çš„è¡¨ç¤ºï¼Œæ¯”å¦‚è¯å‘é‡ï¼ˆword embeddingsï¼‰ï¼Œè¿™æ˜¯å‡è®¾é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯æ‰€éœ€è¦çš„ä¿¡æ¯å¯ä»¥è¢«åˆ©ç”¨æ¥å®Œæˆå…¶ä»–ä»»åŠ¡ã€‚\n\n# ç»Ÿè®¡è¯­è¨€æ¨¡å‹\n\næ–¹æ³•ï¼šç»Ÿè®¡æ¯ä¸ªåºåˆ—å‡ºç°çš„æ¬¡æ•°ï¼š\n\n$$\nP(w_t |w_{<t}) = \\frac{ å‡ºç°æ¬¡æ•° ( w_{<t}, w_t)}{\\sum_{w} å‡ºç°æ¬¡æ•° (w_{<t}, w)} ï¼Œ \\quad w \\in V\n$$\n\n**é—®é¢˜**ï¼šå•è¯çš„ç»„åˆæ•°é‡æ˜¯ $O(V ^ t)$ å¤æ‚åº¦ã€‚\n\n## Word $n$-gram è¯­è¨€æ¨¡å‹\n\n$n$-gram å‡è®¾æ¨¡å‹å‡è®¾æ¯ä¸ªå•è¯å‡ºç°çš„æ¦‚ç‡ä»…å–å†³äºå‰ $n$ ä¸ªå•è¯ã€‚å¯ä»¥å†™æˆï¼š\n\n$$\nP(w_{i} | w_{1}, \\dots, w_{i-1}) \\approx P(w_{i}|w_{i- 1 (n-1)}, \\dots ,w_{i-1})\n$$\n\n## Bag-of-Words æ¨¡å‹\n\nå‡è®¾æ¦‚ç‡è·Ÿé¡ºåºæ— å…³ï¼š\n\n$$\nP(w_{i} | w_{1}, \\dots, w_{i-1}) \\approx P(w_{i})\n$$\n\n---\n\n# æ·±åº¦å­¦ä¹ è¯­è¨€æ¨¡å‹\n\n# Recurrent Neural Network (RNN)\n\nç”¨ä¸€ä¸ª hidden state æ¥ä¿å­˜ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆ$w_{<t}$ï¼‰ï¼Œç„¶åç”¨è¿™ä¸ª hidden state æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ã€‚\n\n![Recurrent Neural Network](%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%BF%9B%E5%8C%96/rnn.png)\n\nåé¢æå‡ºçš„ Long Short-Term Memory (LSTM) å’Œ Gated Recurrent Unit (GRU) éƒ½æ˜¯ RNN çš„å˜ç§ï¼Œåªä¸è¿‡æ˜¯å¯¹è¾“å…¥å’Œ hidden state çš„å¤„ç†æ–¹å¼ä¸åŒã€‚\n\nä¸¤ä¸ªé—®é¢˜ï¼š\n\n- å¾ˆéš¾æŠŠæ‰€æœ‰ä¸Šä¸‹æ–‡ä¿¡æ¯éƒ½å‹ç¼©åˆ°ä¸€ä¸ª hidden state é‡Œé¢ã€‚\n- éš¾ä»¥å¹¶è¡ŒåŒ–ï¼šæ¯ä¸ª token å¯¹åº”çš„ hidden state éƒ½ä¾èµ–äºå‰ä¸€ä¸ª token çš„ hidden stateï¼Œæ‰€ä»¥æ— æ³•å¹¶è¡Œè®¡ç®—ä¸åŒæ•´ä¸ªåºåˆ—ã€‚\n\n# Transformer: Attention is All You Need\n\nSelf-Attention æœºåˆ¶å¦‚ä¸‹ã€‚\n\n![Self-attention æœºåˆ¶ç¤ºæ„å›¾ã€‚](%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%BF%9B%E5%8C%96/self-attention.png)\n\nå›¾ä¸­å±•ç¤ºè¾“å…¥æ˜¯ \"see that girl run\"ï¼Œç„¶åæˆ‘ä»¬è®¡ç®— \"that\" çš„æ½œåœ¨è¡¨ç¤ºã€‚\n\nå› ä¸ºå³è¾¹çš„å‘é‡ï¼ˆhidden representationï¼‰ä¸ä¾èµ–äºå…¶ä»– token çš„ hidden representationï¼Œæ‰€ä»¥å¯ä»¥å¹¶è¡Œè®¡ç®—ã€‚\n\nTransformer æ¨¡å‹ï¼š\n\n$$\n\\begin{align*}\nX^{(0)} &= \\text{Embedding}(s) \\in \\mathbb R ^{N \\times d} \\\\ \nX^{(l)} &= \\text{Attention}\\left(X^{(l-1)}\\right) \\in \\mathbb R ^{N \\times d} \\\\\nX^{(l)} &= \\text{Norm}\\left(X^{(l)} + X^{(l-1)}\\right) \\in \\mathbb R ^{N \\times d} \\\\\nX^{(l)} &= \\text{FFN}\\left(X^{(l)}\\right) \\in \\mathbb R ^{N \\times d} \\\\\nX^{(l)} &= \\text{Norm}\\left(X^{(l)} + X^{(l-1)}\\right) \\in \\mathbb R ^{N \\times d} \\\\\n\\\\\n\\text{Attention}(X) &= \\text{softmax}\\left(\\frac{XW_{q}(XW_{k})^\\top}{\\sqrt{d}} \\right)XW_{v} \\\\\n\\end{align*}\n$$\n\nå…¶ä¸­ $N$ æ˜¯åºåˆ—é•¿åº¦ï¼Œ$d$ æ˜¯ hidden sizeï¼Œ$s$ æ˜¯è¾“å…¥åºåˆ—ï¼Œ$l$ æ˜¯å±‚æ•°ï¼Œ$W_{q}, W_{k}, W_{v}$ æ˜¯å‚æ•°çŸ©é˜µï¼Œ$\\text{LN}$ æ˜¯ layer normalizationï¼Œ$\\text{FFN}$ æ˜¯ä¸€ä¸ªè·Ÿ $t$ æ— å…³çš„ feed-forward networkã€‚\n\n## Attention çš„é—®é¢˜\n\nAttention æœºåˆ¶çš„è®¡ç®—å¤æ‚åº¦æ˜¯ $O(n^2)$ã€‚å¯¹äºè®­ç»ƒæ¥è¯´ï¼Œè¿™ä¸€èˆ¬ä¸æ˜¯å¾ˆå¤§çš„é—®é¢˜ã€‚å¯æ˜¯åœ¨æ¨ç†çš„è¿‡ç¨‹ä¸­ï¼Œå¯¹äºæ¯ä¸ª token éƒ½è¦è®¡ç®—ä¸€æ¬¡ attentionï¼Œä¹Ÿå°±æ˜¯æ¯ç”Ÿæˆä¸€ä¸ª token çš„å¤æ‚åº¦æ˜¯ $O(n)$ã€‚\n","source":"_drafts/è¯­è¨€æ¨¡å‹çš„è¿›åŒ–.md","raw":"---\nauthor: é™ˆè‹±å‘ Yingfa Chen\ntitle: è¯­è¨€æ¨¡å‹çš„è¿›åŒ–\ndate: 2024-02-01 17:51:30\ncategories:\ntags:\n- language modeling\n- language models\n- llm\n- transformer\n- nlp\n- rnn\n- deep learning\n- machine learning\n- research\n- ai\ndraft: true\n---\n<style>\nsection {\n  font-size: 24px;\n}\n</style>\n\nè‡ªä» Transformer é—®ä¸–ä»¥æ¥ï¼ŒåŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„è¯­è¨€æ¨¡å‹å–å¾—äº†æå¤§çš„æˆåŠŸã€‚ç°åœ¨çš„å¤§è¯­è¨€æ¨¡å‹ï¼Œå¦‚ GPTã€LLaMaã€Mistraléƒ½æ˜¯åŸºäº Transformer æ¶æ„çš„ã€‚æœ€è¿‘æœ‰è¯¸å¤šç ”ç©¶å·¥ä½œæå‡ºå¯èƒ½å¯ä»¥æ›¿ä»£ transformer æ¶æ„çš„æ–°æ¶æ„ã€‚æœ¬æ–‡æ•´ç†ä¸€ä¸‹è¿‘äº›å¹´è¯­è¨€æ¨¡å‹çš„å˜åŒ–ã€‚\n\n<!-- more -->\n\n# è¯­è¨€æ¨¡å‹\n\nè¯­è¨€æ¨¡å‹ï¼ˆlanguage modelï¼‰æ˜¯ä¸€ä¸ªå¯¹è¯­è¨€çš„æ¦‚ç‡æ¨¡å‹ï¼Œé¦–æ¬¡åœ¨å…«åå¹´ä»£æå‡ºã€‚è¯­è¨€æ¨¡å‹ä¸“æ³¨äºé¢„æµ‹ä¸€æ®µæ–‡å­—å‡ºç°çš„æ¦‚ç‡ã€‚\n\n$$\nP(s) = P(w_{1}, \\dots, w_{m}) = \\prod_{i}^{m} P(w_i|w_{1},\\dots, w_{i-1})\n$$\n\nå…¶ä¸­ $w_i$ ä¸€èˆ¬æ˜¯ tokenï¼Œè€Œä¸€èˆ¬ä¸€ä¸ª token å°±æ˜¯ä¸€ä¸ªå•è¯ã€‚åœ¨ä»¥ä¸‹çš„è®¨è®ºä¸­ï¼Œå¦‚æœæ²¡æœ‰ç‰¹åˆ«è¯´æ˜ï¼Œæˆ‘é»˜è®¤ä¸€ä¸ª token å°±æ˜¯ä¸€ä¸ªå•è¯ï¼Œè€Œä¸€æ®µæ–‡å­—ï¼ˆæ¯”å¦‚ä¸€ä¸ªå¥å­ï¼‰çš„ä¸å¯åˆ†å‰²å•å…ƒå°±æ˜¯ä¸€ä¸ªå•è¯ã€‚\n\næ¢å¥è¯è¯´ï¼Œè¿™ä¸ªé—®é¢˜å¯ä»¥è½¬åŒ–ä¸ºï¼Œç»™å®šä¸€ä¸ªåºåˆ— $w_{<t}$ï¼Œè®¡ç®—ä¸‹ä¸€ä¸ªå•è¯çš„æ¦‚ç‡åˆ†å¸ƒï¼š\n\n$$\nP(w_{t}|w_{<t}) \\in \\mathbb R^V\n$$\n\nå…¶ä¸­ $V$ æ˜¯è¯è¡¨å¤§å°ï¼Œ$t$ æ˜¯å½“å‰è¦é¢„æµ‹çš„å•è¯çš„ä½ç½®ã€‚è¿™ä¸ªç›®æ ‡å«åš next token predictionã€‚\n\n# è¯­è¨€æ¨¡å‹çš„åº”ç”¨\n\nå­¦ä¹ è¯­è¨€çš„è¡¨ç¤ºï¼Œæ¯”å¦‚è¯å‘é‡ï¼ˆword embeddingsï¼‰ï¼Œè¿™æ˜¯å‡è®¾é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯æ‰€éœ€è¦çš„ä¿¡æ¯å¯ä»¥è¢«åˆ©ç”¨æ¥å®Œæˆå…¶ä»–ä»»åŠ¡ã€‚\n\n# ç»Ÿè®¡è¯­è¨€æ¨¡å‹\n\næ–¹æ³•ï¼šç»Ÿè®¡æ¯ä¸ªåºåˆ—å‡ºç°çš„æ¬¡æ•°ï¼š\n\n$$\nP(w_t |w_{<t}) = \\frac{ å‡ºç°æ¬¡æ•° ( w_{<t}, w_t)}{\\sum_{w} å‡ºç°æ¬¡æ•° (w_{<t}, w)} ï¼Œ \\quad w \\in V\n$$\n\n**é—®é¢˜**ï¼šå•è¯çš„ç»„åˆæ•°é‡æ˜¯ $O(V ^ t)$ å¤æ‚åº¦ã€‚\n\n## Word $n$-gram è¯­è¨€æ¨¡å‹\n\n$n$-gram å‡è®¾æ¨¡å‹å‡è®¾æ¯ä¸ªå•è¯å‡ºç°çš„æ¦‚ç‡ä»…å–å†³äºå‰ $n$ ä¸ªå•è¯ã€‚å¯ä»¥å†™æˆï¼š\n\n$$\nP(w_{i} | w_{1}, \\dots, w_{i-1}) \\approx P(w_{i}|w_{i- 1 (n-1)}, \\dots ,w_{i-1})\n$$\n\n## Bag-of-Words æ¨¡å‹\n\nå‡è®¾æ¦‚ç‡è·Ÿé¡ºåºæ— å…³ï¼š\n\n$$\nP(w_{i} | w_{1}, \\dots, w_{i-1}) \\approx P(w_{i})\n$$\n\n---\n\n# æ·±åº¦å­¦ä¹ è¯­è¨€æ¨¡å‹\n\n# Recurrent Neural Network (RNN)\n\nç”¨ä¸€ä¸ª hidden state æ¥ä¿å­˜ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆ$w_{<t}$ï¼‰ï¼Œç„¶åç”¨è¿™ä¸ª hidden state æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ã€‚\n\n![Recurrent Neural Network](%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%BF%9B%E5%8C%96/rnn.png)\n\nåé¢æå‡ºçš„ Long Short-Term Memory (LSTM) å’Œ Gated Recurrent Unit (GRU) éƒ½æ˜¯ RNN çš„å˜ç§ï¼Œåªä¸è¿‡æ˜¯å¯¹è¾“å…¥å’Œ hidden state çš„å¤„ç†æ–¹å¼ä¸åŒã€‚\n\nä¸¤ä¸ªé—®é¢˜ï¼š\n\n- å¾ˆéš¾æŠŠæ‰€æœ‰ä¸Šä¸‹æ–‡ä¿¡æ¯éƒ½å‹ç¼©åˆ°ä¸€ä¸ª hidden state é‡Œé¢ã€‚\n- éš¾ä»¥å¹¶è¡ŒåŒ–ï¼šæ¯ä¸ª token å¯¹åº”çš„ hidden state éƒ½ä¾èµ–äºå‰ä¸€ä¸ª token çš„ hidden stateï¼Œæ‰€ä»¥æ— æ³•å¹¶è¡Œè®¡ç®—ä¸åŒæ•´ä¸ªåºåˆ—ã€‚\n\n# Transformer: Attention is All You Need\n\nSelf-Attention æœºåˆ¶å¦‚ä¸‹ã€‚\n\n![Self-attention æœºåˆ¶ç¤ºæ„å›¾ã€‚](%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%BF%9B%E5%8C%96/self-attention.png)\n\nå›¾ä¸­å±•ç¤ºè¾“å…¥æ˜¯ \"see that girl run\"ï¼Œç„¶åæˆ‘ä»¬è®¡ç®— \"that\" çš„æ½œåœ¨è¡¨ç¤ºã€‚\n\nå› ä¸ºå³è¾¹çš„å‘é‡ï¼ˆhidden representationï¼‰ä¸ä¾èµ–äºå…¶ä»– token çš„ hidden representationï¼Œæ‰€ä»¥å¯ä»¥å¹¶è¡Œè®¡ç®—ã€‚\n\nTransformer æ¨¡å‹ï¼š\n\n$$\n\\begin{align*}\nX^{(0)} &= \\text{Embedding}(s) \\in \\mathbb R ^{N \\times d} \\\\ \nX^{(l)} &= \\text{Attention}\\left(X^{(l-1)}\\right) \\in \\mathbb R ^{N \\times d} \\\\\nX^{(l)} &= \\text{Norm}\\left(X^{(l)} + X^{(l-1)}\\right) \\in \\mathbb R ^{N \\times d} \\\\\nX^{(l)} &= \\text{FFN}\\left(X^{(l)}\\right) \\in \\mathbb R ^{N \\times d} \\\\\nX^{(l)} &= \\text{Norm}\\left(X^{(l)} + X^{(l-1)}\\right) \\in \\mathbb R ^{N \\times d} \\\\\n\\\\\n\\text{Attention}(X) &= \\text{softmax}\\left(\\frac{XW_{q}(XW_{k})^\\top}{\\sqrt{d}} \\right)XW_{v} \\\\\n\\end{align*}\n$$\n\nå…¶ä¸­ $N$ æ˜¯åºåˆ—é•¿åº¦ï¼Œ$d$ æ˜¯ hidden sizeï¼Œ$s$ æ˜¯è¾“å…¥åºåˆ—ï¼Œ$l$ æ˜¯å±‚æ•°ï¼Œ$W_{q}, W_{k}, W_{v}$ æ˜¯å‚æ•°çŸ©é˜µï¼Œ$\\text{LN}$ æ˜¯ layer normalizationï¼Œ$\\text{FFN}$ æ˜¯ä¸€ä¸ªè·Ÿ $t$ æ— å…³çš„ feed-forward networkã€‚\n\n## Attention çš„é—®é¢˜\n\nAttention æœºåˆ¶çš„è®¡ç®—å¤æ‚åº¦æ˜¯ $O(n^2)$ã€‚å¯¹äºè®­ç»ƒæ¥è¯´ï¼Œè¿™ä¸€èˆ¬ä¸æ˜¯å¾ˆå¤§çš„é—®é¢˜ã€‚å¯æ˜¯åœ¨æ¨ç†çš„è¿‡ç¨‹ä¸­ï¼Œå¯¹äºæ¯ä¸ª token éƒ½è¦è®¡ç®—ä¸€æ¬¡ attentionï¼Œä¹Ÿå°±æ˜¯æ¯ç”Ÿæˆä¸€ä¸ª token çš„å¤æ‚åº¦æ˜¯ $O(n)$ã€‚\n","slug":"è¯­è¨€æ¨¡å‹çš„è¿›åŒ–","published":0,"updated":"2024-02-26T02:53:18.088Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cltl4oxem000yxh7kee7oacqe","content":"<p>è‡ªä» Transformer é—®ä¸–ä»¥æ¥ï¼ŒåŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„è¯­è¨€æ¨¡å‹å–å¾—äº†æå¤§çš„æˆåŠŸã€‚ç°åœ¨çš„å¤§è¯­è¨€æ¨¡å‹ï¼Œå¦‚ GPTã€LLaMaã€Mistraléƒ½æ˜¯åŸºäº Transformer æ¶æ„çš„ã€‚æœ€è¿‘æœ‰è¯¸å¤šç ”ç©¶å·¥ä½œæå‡ºå¯èƒ½å¯ä»¥æ›¿ä»£ transformer æ¶æ„çš„æ–°æ¶æ„ã€‚æœ¬æ–‡æ•´ç†ä¸€ä¸‹è¿‘äº›å¹´è¯­è¨€æ¨¡å‹çš„å˜åŒ–ã€‚</p>\n<span id=\"more\"></span>\n<h1>è¯­è¨€æ¨¡å‹</h1>\n<p>è¯­è¨€æ¨¡å‹ï¼ˆlanguage modelï¼‰æ˜¯ä¸€ä¸ªå¯¹è¯­è¨€çš„æ¦‚ç‡æ¨¡å‹ï¼Œé¦–æ¬¡åœ¨å…«åå¹´ä»£æå‡ºã€‚è¯­è¨€æ¨¡å‹ä¸“æ³¨äºé¢„æµ‹ä¸€æ®µæ–‡å­—å‡ºç°çš„æ¦‚ç‡ã€‚</p>\n<p>$$\nP(s) = P(w_{1}, \\dots, w_{m}) = \\prod_{i}^{m} P(w_i|w_{1},\\dots, w_{i-1})\n$$</p>\n<p>å…¶ä¸­ $w_i$ ä¸€èˆ¬æ˜¯ tokenï¼Œè€Œä¸€èˆ¬ä¸€ä¸ª token å°±æ˜¯ä¸€ä¸ªå•è¯ã€‚åœ¨ä»¥ä¸‹çš„è®¨è®ºä¸­ï¼Œå¦‚æœæ²¡æœ‰ç‰¹åˆ«è¯´æ˜ï¼Œæˆ‘é»˜è®¤ä¸€ä¸ª token å°±æ˜¯ä¸€ä¸ªå•è¯ï¼Œè€Œä¸€æ®µæ–‡å­—ï¼ˆæ¯”å¦‚ä¸€ä¸ªå¥å­ï¼‰çš„ä¸å¯åˆ†å‰²å•å…ƒå°±æ˜¯ä¸€ä¸ªå•è¯ã€‚</p>\n<p>æ¢å¥è¯è¯´ï¼Œè¿™ä¸ªé—®é¢˜å¯ä»¥è½¬åŒ–ä¸ºï¼Œç»™å®šä¸€ä¸ªåºåˆ— $w_{&lt;t}$ï¼Œè®¡ç®—ä¸‹ä¸€ä¸ªå•è¯çš„æ¦‚ç‡åˆ†å¸ƒï¼š</p>\n<p>$$\nP(w_{t}|w_{&lt;t}) \\in \\mathbb R^V\n$$</p>\n<p>å…¶ä¸­ $V$ æ˜¯è¯è¡¨å¤§å°ï¼Œ$t$ æ˜¯å½“å‰è¦é¢„æµ‹çš„å•è¯çš„ä½ç½®ã€‚è¿™ä¸ªç›®æ ‡å«åš next token predictionã€‚</p>\n<h1>è¯­è¨€æ¨¡å‹çš„åº”ç”¨</h1>\n<p>å­¦ä¹ è¯­è¨€çš„è¡¨ç¤ºï¼Œæ¯”å¦‚è¯å‘é‡ï¼ˆword embeddingsï¼‰ï¼Œè¿™æ˜¯å‡è®¾é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯æ‰€éœ€è¦çš„ä¿¡æ¯å¯ä»¥è¢«åˆ©ç”¨æ¥å®Œæˆå…¶ä»–ä»»åŠ¡ã€‚</p>\n<h1>ç»Ÿè®¡è¯­è¨€æ¨¡å‹</h1>\n<p>æ–¹æ³•ï¼šç»Ÿè®¡æ¯ä¸ªåºåˆ—å‡ºç°çš„æ¬¡æ•°ï¼š</p>\n<p>$$\nP(w_t |w_{&lt;t}) = \\frac{ å‡ºç°æ¬¡æ•° ( w_{&lt;t}, w_t)}{\\sum_{w} å‡ºç°æ¬¡æ•° (w_{&lt;t}, w)} ï¼Œ \\quad w \\in V\n$$</p>\n<p><strong>é—®é¢˜</strong>ï¼šå•è¯çš„ç»„åˆæ•°é‡æ˜¯ $O(V ^ t)$ å¤æ‚åº¦ã€‚</p>\n<h2 id=\"Word-n-gram-è¯­è¨€æ¨¡å‹\">Word $n$-gram è¯­è¨€æ¨¡å‹</h2>\n<p>$n$-gram å‡è®¾æ¨¡å‹å‡è®¾æ¯ä¸ªå•è¯å‡ºç°çš„æ¦‚ç‡ä»…å–å†³äºå‰ $n$ ä¸ªå•è¯ã€‚å¯ä»¥å†™æˆï¼š</p>\n<p>$$\nP(w_{i} | w_{1}, \\dots, w_{i-1}) \\approx P(w_{i}|w_{i- 1 (n-1)}, \\dots ,w_{i-1})\n$$</p>\n<h2 id=\"Bag-of-Words-æ¨¡å‹\">Bag-of-Words æ¨¡å‹</h2>\n<p>å‡è®¾æ¦‚ç‡è·Ÿé¡ºåºæ— å…³ï¼š</p>\n<p>$$\nP(w_{i} | w_{1}, \\dots, w_{i-1}) \\approx P(w_{i})\n$$</p>\n<hr>\n<h1>æ·±åº¦å­¦ä¹ è¯­è¨€æ¨¡å‹</h1>\n<h1>Recurrent Neural Network (RNN)</h1>\n<p>ç”¨ä¸€ä¸ª hidden state æ¥ä¿å­˜ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆ$w_{&lt;t}$ï¼‰ï¼Œç„¶åç”¨è¿™ä¸ª hidden state æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ã€‚</p>\n<p><img src=\"%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%BF%9B%E5%8C%96/rnn.png\" alt=\"Recurrent Neural Network\"></p>\n<p>åé¢æå‡ºçš„ Long Short-Term Memory (LSTM) å’Œ Gated Recurrent Unit (GRU) éƒ½æ˜¯ RNN çš„å˜ç§ï¼Œåªä¸è¿‡æ˜¯å¯¹è¾“å…¥å’Œ hidden state çš„å¤„ç†æ–¹å¼ä¸åŒã€‚</p>\n<p>ä¸¤ä¸ªé—®é¢˜ï¼š</p>\n<ul>\n<li>å¾ˆéš¾æŠŠæ‰€æœ‰ä¸Šä¸‹æ–‡ä¿¡æ¯éƒ½å‹ç¼©åˆ°ä¸€ä¸ª hidden state é‡Œé¢ã€‚</li>\n<li>éš¾ä»¥å¹¶è¡ŒåŒ–ï¼šæ¯ä¸ª token å¯¹åº”çš„ hidden state éƒ½ä¾èµ–äºå‰ä¸€ä¸ª token çš„ hidden stateï¼Œæ‰€ä»¥æ— æ³•å¹¶è¡Œè®¡ç®—ä¸åŒæ•´ä¸ªåºåˆ—ã€‚</li>\n</ul>\n<h1>Transformer: Attention is All You Need</h1>\n<p>Self-Attention æœºåˆ¶å¦‚ä¸‹ã€‚</p>\n<p><img src=\"%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%BF%9B%E5%8C%96/self-attention.png\" alt=\"Self-attention æœºåˆ¶ç¤ºæ„å›¾ã€‚\"></p>\n<p>å›¾ä¸­å±•ç¤ºè¾“å…¥æ˜¯ \"see that girl run\"ï¼Œç„¶åæˆ‘ä»¬è®¡ç®— \"that\" çš„æ½œåœ¨è¡¨ç¤ºã€‚</p>\n<p>å› ä¸ºå³è¾¹çš„å‘é‡ï¼ˆhidden representationï¼‰ä¸ä¾èµ–äºå…¶ä»– token çš„ hidden representationï¼Œæ‰€ä»¥å¯ä»¥å¹¶è¡Œè®¡ç®—ã€‚</p>\n<p>Transformer æ¨¡å‹ï¼š</p>\n<p>$$\n\\begin{align*}\nX^{(0)} &amp;= \\text{Embedding}(s) \\in \\mathbb R ^{N \\times d} \\\nX^{(l)} &amp;= \\text{Attention}\\left(X^{(l-1)}\\right) \\in \\mathbb R ^{N \\times d} \\\nX^{(l)} &amp;= \\text{Norm}\\left(X^{(l)} + X^{(l-1)}\\right) \\in \\mathbb R ^{N \\times d} \\\nX^{(l)} &amp;= \\text{FFN}\\left(X^{(l)}\\right) \\in \\mathbb R ^{N \\times d} \\\nX^{(l)} &amp;= \\text{Norm}\\left(X^{(l)} + X^{(l-1)}\\right) \\in \\mathbb R ^{N \\times d} \\\n\\\n\\text{Attention}(X) &amp;= \\text{softmax}\\left(\\frac{XW_{q}(XW_{k})^\\top}{\\sqrt{d}} \\right)XW_{v} \\\n\\end{align*}\n$$</p>\n<p>å…¶ä¸­ $N$ æ˜¯åºåˆ—é•¿åº¦ï¼Œ$d$ æ˜¯ hidden sizeï¼Œ$s$ æ˜¯è¾“å…¥åºåˆ—ï¼Œ$l$ æ˜¯å±‚æ•°ï¼Œ$W_{q}, W_{k}, W_{v}$ æ˜¯å‚æ•°çŸ©é˜µï¼Œ$\\text{LN}$ æ˜¯ layer normalizationï¼Œ$\\text{FFN}$ æ˜¯ä¸€ä¸ªè·Ÿ $t$ æ— å…³çš„ feed-forward networkã€‚</p>\n<h2 id=\"Attention-çš„é—®é¢˜\">Attention çš„é—®é¢˜</h2>\n<p>Attention æœºåˆ¶çš„è®¡ç®—å¤æ‚åº¦æ˜¯ $O(n^2)$ã€‚å¯¹äºè®­ç»ƒæ¥è¯´ï¼Œè¿™ä¸€èˆ¬ä¸æ˜¯å¾ˆå¤§çš„é—®é¢˜ã€‚å¯æ˜¯åœ¨æ¨ç†çš„è¿‡ç¨‹ä¸­ï¼Œå¯¹äºæ¯ä¸ª token éƒ½è¦è®¡ç®—ä¸€æ¬¡ attentionï¼Œä¹Ÿå°±æ˜¯æ¯ç”Ÿæˆä¸€ä¸ª token çš„å¤æ‚åº¦æ˜¯ $O(n)$ã€‚</p>\n","site":{"data":{}},"excerpt":"<p>è‡ªä» Transformer é—®ä¸–ä»¥æ¥ï¼ŒåŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„è¯­è¨€æ¨¡å‹å–å¾—äº†æå¤§çš„æˆåŠŸã€‚ç°åœ¨çš„å¤§è¯­è¨€æ¨¡å‹ï¼Œå¦‚ GPTã€LLaMaã€Mistraléƒ½æ˜¯åŸºäº Transformer æ¶æ„çš„ã€‚æœ€è¿‘æœ‰è¯¸å¤šç ”ç©¶å·¥ä½œæå‡ºå¯èƒ½å¯ä»¥æ›¿ä»£ transformer æ¶æ„çš„æ–°æ¶æ„ã€‚æœ¬æ–‡æ•´ç†ä¸€ä¸‹è¿‘äº›å¹´è¯­è¨€æ¨¡å‹çš„å˜åŒ–ã€‚</p>","more":"<h1>è¯­è¨€æ¨¡å‹</h1>\n<p>è¯­è¨€æ¨¡å‹ï¼ˆlanguage modelï¼‰æ˜¯ä¸€ä¸ªå¯¹è¯­è¨€çš„æ¦‚ç‡æ¨¡å‹ï¼Œé¦–æ¬¡åœ¨å…«åå¹´ä»£æå‡ºã€‚è¯­è¨€æ¨¡å‹ä¸“æ³¨äºé¢„æµ‹ä¸€æ®µæ–‡å­—å‡ºç°çš„æ¦‚ç‡ã€‚</p>\n<p>$$\nP(s) = P(w_{1}, \\dots, w_{m}) = \\prod_{i}^{m} P(w_i|w_{1},\\dots, w_{i-1})\n$$</p>\n<p>å…¶ä¸­ $w_i$ ä¸€èˆ¬æ˜¯ tokenï¼Œè€Œä¸€èˆ¬ä¸€ä¸ª token å°±æ˜¯ä¸€ä¸ªå•è¯ã€‚åœ¨ä»¥ä¸‹çš„è®¨è®ºä¸­ï¼Œå¦‚æœæ²¡æœ‰ç‰¹åˆ«è¯´æ˜ï¼Œæˆ‘é»˜è®¤ä¸€ä¸ª token å°±æ˜¯ä¸€ä¸ªå•è¯ï¼Œè€Œä¸€æ®µæ–‡å­—ï¼ˆæ¯”å¦‚ä¸€ä¸ªå¥å­ï¼‰çš„ä¸å¯åˆ†å‰²å•å…ƒå°±æ˜¯ä¸€ä¸ªå•è¯ã€‚</p>\n<p>æ¢å¥è¯è¯´ï¼Œè¿™ä¸ªé—®é¢˜å¯ä»¥è½¬åŒ–ä¸ºï¼Œç»™å®šä¸€ä¸ªåºåˆ— $w_{&lt;t}$ï¼Œè®¡ç®—ä¸‹ä¸€ä¸ªå•è¯çš„æ¦‚ç‡åˆ†å¸ƒï¼š</p>\n<p>$$\nP(w_{t}|w_{&lt;t}) \\in \\mathbb R^V\n$$</p>\n<p>å…¶ä¸­ $V$ æ˜¯è¯è¡¨å¤§å°ï¼Œ$t$ æ˜¯å½“å‰è¦é¢„æµ‹çš„å•è¯çš„ä½ç½®ã€‚è¿™ä¸ªç›®æ ‡å«åš next token predictionã€‚</p>\n<h1>è¯­è¨€æ¨¡å‹çš„åº”ç”¨</h1>\n<p>å­¦ä¹ è¯­è¨€çš„è¡¨ç¤ºï¼Œæ¯”å¦‚è¯å‘é‡ï¼ˆword embeddingsï¼‰ï¼Œè¿™æ˜¯å‡è®¾é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯æ‰€éœ€è¦çš„ä¿¡æ¯å¯ä»¥è¢«åˆ©ç”¨æ¥å®Œæˆå…¶ä»–ä»»åŠ¡ã€‚</p>\n<h1>ç»Ÿè®¡è¯­è¨€æ¨¡å‹</h1>\n<p>æ–¹æ³•ï¼šç»Ÿè®¡æ¯ä¸ªåºåˆ—å‡ºç°çš„æ¬¡æ•°ï¼š</p>\n<p>$$\nP(w_t |w_{&lt;t}) = \\frac{ å‡ºç°æ¬¡æ•° ( w_{&lt;t}, w_t)}{\\sum_{w} å‡ºç°æ¬¡æ•° (w_{&lt;t}, w)} ï¼Œ \\quad w \\in V\n$$</p>\n<p><strong>é—®é¢˜</strong>ï¼šå•è¯çš„ç»„åˆæ•°é‡æ˜¯ $O(V ^ t)$ å¤æ‚åº¦ã€‚</p>\n<h2 id=\"Word-n-gram-è¯­è¨€æ¨¡å‹\">Word $n$-gram è¯­è¨€æ¨¡å‹</h2>\n<p>$n$-gram å‡è®¾æ¨¡å‹å‡è®¾æ¯ä¸ªå•è¯å‡ºç°çš„æ¦‚ç‡ä»…å–å†³äºå‰ $n$ ä¸ªå•è¯ã€‚å¯ä»¥å†™æˆï¼š</p>\n<p>$$\nP(w_{i} | w_{1}, \\dots, w_{i-1}) \\approx P(w_{i}|w_{i- 1 (n-1)}, \\dots ,w_{i-1})\n$$</p>\n<h2 id=\"Bag-of-Words-æ¨¡å‹\">Bag-of-Words æ¨¡å‹</h2>\n<p>å‡è®¾æ¦‚ç‡è·Ÿé¡ºåºæ— å…³ï¼š</p>\n<p>$$\nP(w_{i} | w_{1}, \\dots, w_{i-1}) \\approx P(w_{i})\n$$</p>\n<hr>\n<h1>æ·±åº¦å­¦ä¹ è¯­è¨€æ¨¡å‹</h1>\n<h1>Recurrent Neural Network (RNN)</h1>\n<p>ç”¨ä¸€ä¸ª hidden state æ¥ä¿å­˜ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆ$w_{&lt;t}$ï¼‰ï¼Œç„¶åç”¨è¿™ä¸ª hidden state æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ã€‚</p>\n<p><img src=\"%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%BF%9B%E5%8C%96/rnn.png\" alt=\"Recurrent Neural Network\"></p>\n<p>åé¢æå‡ºçš„ Long Short-Term Memory (LSTM) å’Œ Gated Recurrent Unit (GRU) éƒ½æ˜¯ RNN çš„å˜ç§ï¼Œåªä¸è¿‡æ˜¯å¯¹è¾“å…¥å’Œ hidden state çš„å¤„ç†æ–¹å¼ä¸åŒã€‚</p>\n<p>ä¸¤ä¸ªé—®é¢˜ï¼š</p>\n<ul>\n<li>å¾ˆéš¾æŠŠæ‰€æœ‰ä¸Šä¸‹æ–‡ä¿¡æ¯éƒ½å‹ç¼©åˆ°ä¸€ä¸ª hidden state é‡Œé¢ã€‚</li>\n<li>éš¾ä»¥å¹¶è¡ŒåŒ–ï¼šæ¯ä¸ª token å¯¹åº”çš„ hidden state éƒ½ä¾èµ–äºå‰ä¸€ä¸ª token çš„ hidden stateï¼Œæ‰€ä»¥æ— æ³•å¹¶è¡Œè®¡ç®—ä¸åŒæ•´ä¸ªåºåˆ—ã€‚</li>\n</ul>\n<h1>Transformer: Attention is All You Need</h1>\n<p>Self-Attention æœºåˆ¶å¦‚ä¸‹ã€‚</p>\n<p><img src=\"%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%BF%9B%E5%8C%96/self-attention.png\" alt=\"Self-attention æœºåˆ¶ç¤ºæ„å›¾ã€‚\"></p>\n<p>å›¾ä¸­å±•ç¤ºè¾“å…¥æ˜¯ &quot;see that girl run&quot;ï¼Œç„¶åæˆ‘ä»¬è®¡ç®— &quot;that&quot; çš„æ½œåœ¨è¡¨ç¤ºã€‚</p>\n<p>å› ä¸ºå³è¾¹çš„å‘é‡ï¼ˆhidden representationï¼‰ä¸ä¾èµ–äºå…¶ä»– token çš„ hidden representationï¼Œæ‰€ä»¥å¯ä»¥å¹¶è¡Œè®¡ç®—ã€‚</p>\n<p>Transformer æ¨¡å‹ï¼š</p>\n<p>$$\n\\begin{align*}\nX^{(0)} &amp;= \\text{Embedding}(s) \\in \\mathbb R ^{N \\times d} \\\nX^{(l)} &amp;= \\text{Attention}\\left(X^{(l-1)}\\right) \\in \\mathbb R ^{N \\times d} \\\nX^{(l)} &amp;= \\text{Norm}\\left(X^{(l)} + X^{(l-1)}\\right) \\in \\mathbb R ^{N \\times d} \\\nX^{(l)} &amp;= \\text{FFN}\\left(X^{(l)}\\right) \\in \\mathbb R ^{N \\times d} \\\nX^{(l)} &amp;= \\text{Norm}\\left(X^{(l)} + X^{(l-1)}\\right) \\in \\mathbb R ^{N \\times d} \\\n\\\n\\text{Attention}(X) &amp;= \\text{softmax}\\left(\\frac{XW_{q}(XW_{k})^\\top}{\\sqrt{d}} \\right)XW_{v} \\\n\\end{align*}\n$$</p>\n<p>å…¶ä¸­ $N$ æ˜¯åºåˆ—é•¿åº¦ï¼Œ$d$ æ˜¯ hidden sizeï¼Œ$s$ æ˜¯è¾“å…¥åºåˆ—ï¼Œ$l$ æ˜¯å±‚æ•°ï¼Œ$W_{q}, W_{k}, W_{v}$ æ˜¯å‚æ•°çŸ©é˜µï¼Œ$\\text{LN}$ æ˜¯ layer normalizationï¼Œ$\\text{FFN}$ æ˜¯ä¸€ä¸ªè·Ÿ $t$ æ— å…³çš„ feed-forward networkã€‚</p>\n<h2 id=\"Attention-çš„é—®é¢˜\">Attention çš„é—®é¢˜</h2>\n<p>Attention æœºåˆ¶çš„è®¡ç®—å¤æ‚åº¦æ˜¯ $O(n^2)$ã€‚å¯¹äºè®­ç»ƒæ¥è¯´ï¼Œè¿™ä¸€èˆ¬ä¸æ˜¯å¾ˆå¤§çš„é—®é¢˜ã€‚å¯æ˜¯åœ¨æ¨ç†çš„è¿‡ç¨‹ä¸­ï¼Œå¯¹äºæ¯ä¸ª token éƒ½è¦è®¡ç®—ä¸€æ¬¡ attentionï¼Œä¹Ÿå°±æ˜¯æ¯ç”Ÿæˆä¸€ä¸ª token çš„å¤æ‚åº¦æ˜¯ $O(n)$ã€‚</p>"}],"PostAsset":[{"_id":"source/_posts/2023ä¸­ç§‹/æ–°å¤©åœ°-éœ¸ç‹èŒ¶å§¬.png","slug":"æ–°å¤©åœ°-éœ¸ç‹èŒ¶å§¬.png","post":"cltl4oxed0001xh7k0ksv1gpa","modified":0,"renderable":0},{"_id":"source/_posts/2023ä¸­ç§‹/æ­¦å•†æ¢¦æ—¶ä»£.png","slug":"æ­¦å•†æ¢¦æ—¶ä»£.png","post":"cltl4oxed0001xh7k0ksv1gpa","modified":0,"renderable":0},{"_id":"source/_posts/2023ä¸­ç§‹/æ­¦æ±‰æ¬¢ä¹è°·.png","slug":"æ­¦æ±‰æ¬¢ä¹è°·.png","post":"cltl4oxed0001xh7k0ksv1gpa","modified":0,"renderable":0},{"_id":"source/_posts/2023ä¸­ç§‹/è§£æ”¾å…¬å›­ä¸­é—´.png","slug":"è§£æ”¾å…¬å›­ä¸­é—´.png","post":"cltl4oxed0001xh7k0ksv1gpa","modified":0,"renderable":0},{"_id":"source/_posts/infinitebench/data-stat-pie.png","slug":"data-stat-pie.png","post":"cltl4oxeh0009xh7k6tym7gjd","modified":0,"renderable":0},{"_id":"source/_posts/infinitebench/results.png","slug":"results.png","post":"cltl4oxeh0009xh7k6tym7gjd","modified":0,"renderable":0},{"_id":"source/_posts/actadd/alg.png","slug":"alg.png","post":"cltl4oxeg0007xh7k5ati3i3b","modified":0,"renderable":0},{"_id":"source/_posts/actadd/method.png","slug":"method.png","post":"cltl4oxeg0007xh7k5ati3i3b","modified":0,"renderable":0},{"_id":"source/_posts/actadd/result.png","slug":"result.png","post":"cltl4oxeg0007xh7k5ati3i3b","modified":0,"renderable":0},{"_id":"source/_posts/æ›´æ–°ä¸ªäººä¸»é¡µ/ä¸­å›½æŠ¤ç…§.jpg","slug":"ä¸­å›½æŠ¤ç…§.jpg","post":"cltl4oxej000nxh7kc2lgbk3d","modified":0,"renderable":0},{"_id":"source/_posts/æ›´æ–°ä¸ªäººä¸»é¡µ/åƒæ¾³é—¨èœ.jpg","slug":"åƒæ¾³é—¨èœ.jpg","post":"cltl4oxej000nxh7kc2lgbk3d","modified":0,"renderable":0},{"_id":"source/_posts/æ›´æ–°ä¸ªäººä¸»é¡µ/ç”³è¯·ç­¾è¯.jpg","slug":"ç”³è¯·ç­¾è¯.jpg","post":"cltl4oxej000nxh7kc2lgbk3d","modified":0,"renderable":0},{"_id":"source/_posts/ç¬¬ä¸€ä¸ªå¸–å­ï¼Œçå†™ç‚¹ä¸œè¥¿/lillesand0.jpg","slug":"lillesand0.jpg","post":"cltl4oxel000rxh7kh777euay","modified":0,"renderable":0},{"_id":"source/_posts/ç¬¬ä¸€ä¸ªå¸–å­ï¼Œçå†™ç‚¹ä¸œè¥¿/lillesand1.jpg","slug":"lillesand1.jpg","post":"cltl4oxel000rxh7kh777euay","modified":0,"renderable":0}],"PostCategory":[{"post_id":"cltl4oxed0001xh7k0ksv1gpa","category_id":"cltl4oxef0004xh7kdcffd02p","_id":"cltl4oxej000ixh7kaeivd2qx"},{"post_id":"cltl4oxee0003xh7k1kul88dt","category_id":"cltl4oxei000cxh7k8w255zde","_id":"cltl4oxek000oxh7k61v9ej27"},{"post_id":"cltl4oxej000mxh7k0gmq2mji","category_id":"cltl4oxef0004xh7kdcffd02p","_id":"cltl4oxem000txh7kcui3cwi0"},{"post_id":"cltl4oxeg0007xh7k5ati3i3b","category_id":"cltl4oxej000jxh7k86kp7os4","_id":"cltl4oxem000xxh7k3cds4zay"},{"post_id":"cltl4oxej000nxh7kc2lgbk3d","category_id":"cltl4oxef0004xh7kdcffd02p","_id":"cltl4oxen000zxh7k7w5t0q0g"},{"post_id":"cltl4oxel000rxh7kh777euay","category_id":"cltl4oxef0004xh7kdcffd02p","_id":"cltl4oxen0011xh7k08ru30mj"},{"post_id":"cltl4oxeh0009xh7k6tym7gjd","category_id":"cltl4oxei000cxh7k8w255zde","_id":"cltl4oxen0013xh7k51be6kda"},{"post_id":"cltl4oxel000sxh7k05fk4sya","category_id":"cltl4oxef0004xh7kdcffd02p","_id":"cltl4oxen0015xh7kccif5br3"},{"post_id":"cltl4oxeh000bxh7k5kpl1zp0","category_id":"cltl4oxem000uxh7k7oa1duf3","_id":"cltl4oxen0017xh7k2h991idi"},{"post_id":"cltl4oxei000fxh7k1qa8gbtd","category_id":"cltl4oxem000uxh7k7oa1duf3","_id":"cltl4oxen0018xh7k5z61cqcn"},{"post_id":"cltl4oxei000hxh7k0xuo6ggs","category_id":"cltl4oxen0014xh7k50v9dsv5","_id":"cltl4oxen001bxh7kgk3j0rxx"},{"post_id":"cltl4oxem000wxh7kge2td07k","category_id":"cltl4oxen0019xh7k621mhbtp","_id":"cltl4oxen001dxh7khyvgcs4q"}],"PostTag":[{"post_id":"cltl4oxed0001xh7k0ksv1gpa","tag_id":"cltl4oxeg0005xh7khank0y94","_id":"cltl4oxeo001fxh7k9y3cg7nw"},{"post_id":"cltl4oxed0001xh7k0ksv1gpa","tag_id":"cltl4oxei000dxh7k3v9vg1ls","_id":"cltl4oxeo001gxh7kh6e8czm1"},{"post_id":"cltl4oxed0001xh7k0ksv1gpa","tag_id":"cltl4oxej000kxh7k4mh56brx","_id":"cltl4oxeo001ixh7kfz9e5lx9"},{"post_id":"cltl4oxed0001xh7k0ksv1gpa","tag_id":"cltl4oxek000qxh7k14wog3li","_id":"cltl4oxeo001jxh7k8cf30g7m"},{"post_id":"cltl4oxed0001xh7k0ksv1gpa","tag_id":"cltl4oxem000vxh7k5w1p8d7e","_id":"cltl4oxeo001lxh7k1gf7f1ip"},{"post_id":"cltl4oxed0001xh7k0ksv1gpa","tag_id":"cltl4oxen0012xh7kcnebc3oc","_id":"cltl4oxeo001mxh7k1fxagksi"},{"post_id":"cltl4oxed0001xh7k0ksv1gpa","tag_id":"cltl4oxen0016xh7k7vsndyq6","_id":"cltl4oxeo001oxh7k9o5b03av"},{"post_id":"cltl4oxed0001xh7k0ksv1gpa","tag_id":"cltl4oxen001axh7k627279ga","_id":"cltl4oxeo001pxh7k6h3ldqgw"},{"post_id":"cltl4oxed0001xh7k0ksv1gpa","tag_id":"cltl4oxen001cxh7k9je4bmj2","_id":"cltl4oxeo001qxh7k6t6b8e9y"},{"post_id":"cltl4oxee0003xh7k1kul88dt","tag_id":"cltl4oxeo001exh7kgdn55uua","_id":"cltl4oxep0021xh7k7zgbcb4o"},{"post_id":"cltl4oxee0003xh7k1kul88dt","tag_id":"cltl4oxeo001hxh7kc5qi9ef7","_id":"cltl4oxep0022xh7kbdvaf5yd"},{"post_id":"cltl4oxee0003xh7k1kul88dt","tag_id":"cltl4oxeo001kxh7kazm95q38","_id":"cltl4oxep0024xh7kdtr07tep"},{"post_id":"cltl4oxee0003xh7k1kul88dt","tag_id":"cltl4oxeo001nxh7k2yws3zeq","_id":"cltl4oxep0025xh7k6fn615d3"},{"post_id":"cltl4oxee0003xh7k1kul88dt","tag_id":"cltl4oxek000qxh7k14wog3li","_id":"cltl4oxep0027xh7k8u279yuh"},{"post_id":"cltl4oxee0003xh7k1kul88dt","tag_id":"cltl4oxeo001sxh7kegqn7ff5","_id":"cltl4oxep0028xh7k399h52dh"},{"post_id":"cltl4oxee0003xh7k1kul88dt","tag_id":"cltl4oxeo001txh7k4fpn407l","_id":"cltl4oxep002axh7k4wymb8ex"},{"post_id":"cltl4oxee0003xh7k1kul88dt","tag_id":"cltl4oxeo001uxh7kfhrac527","_id":"cltl4oxep002bxh7kda663bl1"},{"post_id":"cltl4oxee0003xh7k1kul88dt","tag_id":"cltl4oxeo001vxh7kc6cufb5s","_id":"cltl4oxep002dxh7kgdol6ac2"},{"post_id":"cltl4oxee0003xh7k1kul88dt","tag_id":"cltl4oxeo001wxh7kc7dl7ghh","_id":"cltl4oxep002exh7k6qyo4ulx"},{"post_id":"cltl4oxee0003xh7k1kul88dt","tag_id":"cltl4oxeo001xxh7k52kjcz1e","_id":"cltl4oxep002fxh7kc0i94ke0"},{"post_id":"cltl4oxee0003xh7k1kul88dt","tag_id":"cltl4oxeo001yxh7k786udfz2","_id":"cltl4oxep002hxh7kczgp8lxm"},{"post_id":"cltl4oxee0003xh7k1kul88dt","tag_id":"cltl4oxeo001zxh7keq2406bh","_id":"cltl4oxep002ixh7k38itdead"},{"post_id":"cltl4oxeg0007xh7k5ati3i3b","tag_id":"cltl4oxeo001sxh7kegqn7ff5","_id":"cltl4oxep002oxh7k1t8504yt"},{"post_id":"cltl4oxeg0007xh7k5ati3i3b","tag_id":"cltl4oxep0023xh7k91jpa4xc","_id":"cltl4oxep002pxh7k6pqf8b48"},{"post_id":"cltl4oxeg0007xh7k5ati3i3b","tag_id":"cltl4oxep0026xh7k2xhl61nn","_id":"cltl4oxeq002rxh7kgufzaapd"},{"post_id":"cltl4oxeg0007xh7k5ati3i3b","tag_id":"cltl4oxep0029xh7karbxddgz","_id":"cltl4oxeq002sxh7kgd6h45ra"},{"post_id":"cltl4oxeg0007xh7k5ati3i3b","tag_id":"cltl4oxep002cxh7kch8h39zw","_id":"cltl4oxeq002uxh7k01mj2f4c"},{"post_id":"cltl4oxeg0007xh7k5ati3i3b","tag_id":"cltl4oxep002gxh7kdv61gdf4","_id":"cltl4oxeq002vxh7k69uq5mzr"},{"post_id":"cltl4oxeg0007xh7k5ati3i3b","tag_id":"cltl4oxep002jxh7k4g111uwu","_id":"cltl4oxeq002xxh7kgof5hifo"},{"post_id":"cltl4oxeg0007xh7k5ati3i3b","tag_id":"cltl4oxep002kxh7k99023bmu","_id":"cltl4oxeq002yxh7k2vr4cgiu"},{"post_id":"cltl4oxeg0007xh7k5ati3i3b","tag_id":"cltl4oxep002lxh7kbk6594qr","_id":"cltl4oxeq0030xh7key6u8qj2"},{"post_id":"cltl4oxeg0007xh7k5ati3i3b","tag_id":"cltl4oxep002mxh7k0fuy6d90","_id":"cltl4oxeq0031xh7k4upr5nmi"},{"post_id":"cltl4oxeh0009xh7k6tym7gjd","tag_id":"cltl4oxeo001hxh7kc5qi9ef7","_id":"cltl4oxeq0036xh7k3vf62rg3"},{"post_id":"cltl4oxeh0009xh7k6tym7gjd","tag_id":"cltl4oxep0026xh7k2xhl61nn","_id":"cltl4oxeq0037xh7kb3d44aca"},{"post_id":"cltl4oxeh0009xh7k6tym7gjd","tag_id":"cltl4oxeq002txh7ka5atb1wn","_id":"cltl4oxeq0039xh7kfd62gm5r"},{"post_id":"cltl4oxeh0009xh7k6tym7gjd","tag_id":"cltl4oxeq002wxh7k11lraz4y","_id":"cltl4oxeq003axh7kfrmjfmyk"},{"post_id":"cltl4oxeh0009xh7k6tym7gjd","tag_id":"cltl4oxeq002zxh7kgms546b8","_id":"cltl4oxeq003cxh7k1bk55rt7"},{"post_id":"cltl4oxeh0009xh7k6tym7gjd","tag_id":"cltl4oxeq0032xh7kbub7f1t4","_id":"cltl4oxeq003dxh7kbqclgw0o"},{"post_id":"cltl4oxeh0009xh7k6tym7gjd","tag_id":"cltl4oxeq0033xh7kh8189guc","_id":"cltl4oxeq003fxh7k00z42dhu"},{"post_id":"cltl4oxeh0009xh7k6tym7gjd","tag_id":"cltl4oxeq0034xh7k002s4pqb","_id":"cltl4oxer003gxh7k1kpc748u"},{"post_id":"cltl4oxeh000bxh7k5kpl1zp0","tag_id":"cltl4oxeo001sxh7kegqn7ff5","_id":"cltl4oxer003mxh7kfau08dkf"},{"post_id":"cltl4oxeh000bxh7k5kpl1zp0","tag_id":"cltl4oxeq0038xh7kgyvv6tq1","_id":"cltl4oxer003nxh7kg1qc87c3"},{"post_id":"cltl4oxeh000bxh7k5kpl1zp0","tag_id":"cltl4oxep002kxh7k99023bmu","_id":"cltl4oxer003pxh7kd5z6h7sx"},{"post_id":"cltl4oxeh000bxh7k5kpl1zp0","tag_id":"cltl4oxeq003exh7kahxrblg0","_id":"cltl4oxer003qxh7k0g977sr6"},{"post_id":"cltl4oxeh000bxh7k5kpl1zp0","tag_id":"cltl4oxer003hxh7kh1m1hyfk","_id":"cltl4oxer003sxh7k55pl142i"},{"post_id":"cltl4oxeh000bxh7k5kpl1zp0","tag_id":"cltl4oxer003ixh7kcfzxdza6","_id":"cltl4oxer003txh7k6qppdl8h"},{"post_id":"cltl4oxeh000bxh7k5kpl1zp0","tag_id":"cltl4oxep0026xh7k2xhl61nn","_id":"cltl4oxer003vxh7k86uz475g"},{"post_id":"cltl4oxeh000bxh7k5kpl1zp0","tag_id":"cltl4oxer003kxh7k55md4emg","_id":"cltl4oxer003wxh7kanx2fka5"},{"post_id":"cltl4oxei000fxh7k1qa8gbtd","tag_id":"cltl4oxeo001sxh7kegqn7ff5","_id":"cltl4oxes0048xh7karkx73nv"},{"post_id":"cltl4oxei000fxh7k1qa8gbtd","tag_id":"cltl4oxeo001hxh7kc5qi9ef7","_id":"cltl4oxes0049xh7k8a0hc8ot"},{"post_id":"cltl4oxei000fxh7k1qa8gbtd","tag_id":"cltl4oxep0026xh7k2xhl61nn","_id":"cltl4oxes004bxh7kcjwj81iy"},{"post_id":"cltl4oxei000fxh7k1qa8gbtd","tag_id":"cltl4oxeo001wxh7kc7dl7ghh","_id":"cltl4oxes004cxh7k82pfe6b6"},{"post_id":"cltl4oxei000fxh7k1qa8gbtd","tag_id":"cltl4oxer003xxh7kgkyp9vgf","_id":"cltl4oxes004exh7kb0ab5rsz"},{"post_id":"cltl4oxei000fxh7k1qa8gbtd","tag_id":"cltl4oxer003yxh7k8m2l3fvy","_id":"cltl4oxes004fxh7kehnq2cy5"},{"post_id":"cltl4oxei000fxh7k1qa8gbtd","tag_id":"cltl4oxep0023xh7k91jpa4xc","_id":"cltl4oxes004hxh7k6xzmhcl2"},{"post_id":"cltl4oxei000fxh7k1qa8gbtd","tag_id":"cltl4oxes0040xh7khish02ig","_id":"cltl4oxes004ixh7k45svh3yf"},{"post_id":"cltl4oxei000fxh7k1qa8gbtd","tag_id":"cltl4oxes0041xh7k1urp35be","_id":"cltl4oxes004jxh7kbg8jcamd"},{"post_id":"cltl4oxei000fxh7k1qa8gbtd","tag_id":"cltl4oxeg0005xh7khank0y94","_id":"cltl4oxes004lxh7kb5r48gpy"},{"post_id":"cltl4oxei000fxh7k1qa8gbtd","tag_id":"cltl4oxes0042xh7kb7ifej2q","_id":"cltl4oxes004mxh7kcka4es56"},{"post_id":"cltl4oxei000fxh7k1qa8gbtd","tag_id":"cltl4oxes0043xh7k2kxh10wn","_id":"cltl4oxes004oxh7kcd8f2z0x"},{"post_id":"cltl4oxei000fxh7k1qa8gbtd","tag_id":"cltl4oxes0044xh7k9spvftri","_id":"cltl4oxes004pxh7k1v6ddnmb"},{"post_id":"cltl4oxei000fxh7k1qa8gbtd","tag_id":"cltl4oxes0045xh7k06c5dz5i","_id":"cltl4oxet004rxh7kg4ezby12"},{"post_id":"cltl4oxei000fxh7k1qa8gbtd","tag_id":"cltl4oxes0046xh7k6hew8dhg","_id":"cltl4oxet004sxh7k3kkz2snc"},{"post_id":"cltl4oxei000hxh7k0xuo6ggs","tag_id":"cltl4oxes0047xh7kg8bq575r","_id":"cltl4oxet004vxh7k5ftefef8"},{"post_id":"cltl4oxei000hxh7k0xuo6ggs","tag_id":"cltl4oxes004axh7kdh417zp3","_id":"cltl4oxet004wxh7k8gwu1mqc"},{"post_id":"cltl4oxei000hxh7k0xuo6ggs","tag_id":"cltl4oxes004dxh7k0gww98jp","_id":"cltl4oxet004yxh7kg5tb5w9b"},{"post_id":"cltl4oxei000hxh7k0xuo6ggs","tag_id":"cltl4oxes004gxh7kaoq3173t","_id":"cltl4oxet004zxh7k19p6evjr"},{"post_id":"cltl4oxei000hxh7k0xuo6ggs","tag_id":"cltl4oxes004kxh7k5co2ep2e","_id":"cltl4oxet0051xh7kdvzjdxkn"},{"post_id":"cltl4oxei000hxh7k0xuo6ggs","tag_id":"cltl4oxes004nxh7ke5mq0ibm","_id":"cltl4oxet0052xh7k948x8lte"},{"post_id":"cltl4oxei000hxh7k0xuo6ggs","tag_id":"cltl4oxeo001sxh7kegqn7ff5","_id":"cltl4oxet0054xh7k0xnd0tms"},{"post_id":"cltl4oxei000hxh7k0xuo6ggs","tag_id":"cltl4oxet004txh7k91p7ajon","_id":"cltl4oxet0055xh7k0qpaa8or"},{"post_id":"cltl4oxej000mxh7k0gmq2mji","tag_id":"cltl4oxeg0005xh7khank0y94","_id":"cltl4oxet0057xh7kee0a6u0v"},{"post_id":"cltl4oxej000mxh7k0gmq2mji","tag_id":"cltl4oxek000qxh7k14wog3li","_id":"cltl4oxet0058xh7k4yjq90qp"},{"post_id":"cltl4oxej000mxh7k0gmq2mji","tag_id":"cltl4oxet004xxh7k6vcq0rfb","_id":"cltl4oxet005axh7k9tkodzlu"},{"post_id":"cltl4oxej000mxh7k0gmq2mji","tag_id":"cltl4oxet0050xh7kew2b65ub","_id":"cltl4oxet005bxh7kf2ho5mre"},{"post_id":"cltl4oxej000mxh7k0gmq2mji","tag_id":"cltl4oxej000kxh7k4mh56brx","_id":"cltl4oxet005cxh7k2iy85xxl"},{"post_id":"cltl4oxej000nxh7kc2lgbk3d","tag_id":"cltl4oxeg0005xh7khank0y94","_id":"cltl4oxeu005rxh7k3v4rhiqi"},{"post_id":"cltl4oxej000nxh7kc2lgbk3d","tag_id":"cltl4oxet0053xh7k3l294op5","_id":"cltl4oxeu005sxh7kcdim668j"},{"post_id":"cltl4oxej000nxh7kc2lgbk3d","tag_id":"cltl4oxej000kxh7k4mh56brx","_id":"cltl4oxeu005uxh7kd5hod6gn"},{"post_id":"cltl4oxej000nxh7kc2lgbk3d","tag_id":"cltl4oxek000qxh7k14wog3li","_id":"cltl4oxeu005vxh7k6ed778d9"},{"post_id":"cltl4oxej000nxh7kc2lgbk3d","tag_id":"cltl4oxet0059xh7k1hqhcxbj","_id":"cltl4oxeu005xxh7k45ihgwd0"},{"post_id":"cltl4oxej000nxh7kc2lgbk3d","tag_id":"cltl4oxet005dxh7kbbvy8hbj","_id":"cltl4oxeu005yxh7kfyssh80n"},{"post_id":"cltl4oxej000nxh7kc2lgbk3d","tag_id":"cltl4oxet005exh7k3bfue7h4","_id":"cltl4oxeu0060xh7k8q3xd7uo"},{"post_id":"cltl4oxej000nxh7kc2lgbk3d","tag_id":"cltl4oxet005fxh7kd5fz3keb","_id":"cltl4oxeu0061xh7k1emt976q"},{"post_id":"cltl4oxej000nxh7kc2lgbk3d","tag_id":"cltl4oxet005gxh7ka0n7csv5","_id":"cltl4oxeu0063xh7kcjud8ubv"},{"post_id":"cltl4oxej000nxh7kc2lgbk3d","tag_id":"cltl4oxeo001hxh7kc5qi9ef7","_id":"cltl4oxeu0064xh7k04py02vb"},{"post_id":"cltl4oxej000nxh7kc2lgbk3d","tag_id":"cltl4oxet005ixh7kb9um7ou9","_id":"cltl4oxev0066xh7k8ffb4boc"},{"post_id":"cltl4oxej000nxh7kc2lgbk3d","tag_id":"cltl4oxeu005jxh7k75a3d4sr","_id":"cltl4oxev0067xh7kba35a5mn"},{"post_id":"cltl4oxej000nxh7kc2lgbk3d","tag_id":"cltl4oxes0040xh7khish02ig","_id":"cltl4oxev0068xh7kb6v0hrkm"},{"post_id":"cltl4oxej000nxh7kc2lgbk3d","tag_id":"cltl4oxeu005lxh7k5wm7csac","_id":"cltl4oxev006axh7k4easbkz0"},{"post_id":"cltl4oxej000nxh7kc2lgbk3d","tag_id":"cltl4oxeu005mxh7kb9gqgy6k","_id":"cltl4oxev006bxh7k6v4m5stg"},{"post_id":"cltl4oxej000nxh7kc2lgbk3d","tag_id":"cltl4oxeu005nxh7kcsfm7yf2","_id":"cltl4oxev006dxh7k0r5geqvq"},{"post_id":"cltl4oxej000nxh7kc2lgbk3d","tag_id":"cltl4oxeu005oxh7kac9v31fc","_id":"cltl4oxev006exh7kh04xbv9z"},{"post_id":"cltl4oxej000nxh7kc2lgbk3d","tag_id":"cltl4oxeu005pxh7kdlg035y5","_id":"cltl4oxev006gxh7k1fokdssd"},{"post_id":"cltl4oxel000rxh7kh777euay","tag_id":"cltl4oxeg0005xh7khank0y94","_id":"cltl4oxew006uxh7kd0g1b08x"},{"post_id":"cltl4oxel000rxh7kh777euay","tag_id":"cltl4oxet004xxh7k6vcq0rfb","_id":"cltl4oxew006vxh7kcmidb1qz"},{"post_id":"cltl4oxel000rxh7kh777euay","tag_id":"cltl4oxeo001hxh7kc5qi9ef7","_id":"cltl4oxew006xxh7khlgyasz4"},{"post_id":"cltl4oxel000rxh7kh777euay","tag_id":"cltl4oxek000qxh7k14wog3li","_id":"cltl4oxew006yxh7kcmxr7px0"},{"post_id":"cltl4oxel000rxh7kh777euay","tag_id":"cltl4oxeu005wxh7k4h0w8u5e","_id":"cltl4oxew0070xh7k2esh3ybv"},{"post_id":"cltl4oxel000rxh7kh777euay","tag_id":"cltl4oxeu005zxh7kh6dv8423","_id":"cltl4oxew0071xh7kch859tnw"},{"post_id":"cltl4oxel000rxh7kh777euay","tag_id":"cltl4oxeu0062xh7kfro8hoiw","_id":"cltl4oxew0073xh7k44btchik"},{"post_id":"cltl4oxel000rxh7kh777euay","tag_id":"cltl4oxeu0065xh7k29jsd8v4","_id":"cltl4oxew0074xh7k5dwd9e2e"},{"post_id":"cltl4oxel000rxh7kh777euay","tag_id":"cltl4oxev0069xh7kgufo3xyx","_id":"cltl4oxew0076xh7kay7afwxx"},{"post_id":"cltl4oxel000rxh7kh777euay","tag_id":"cltl4oxev006cxh7kes1wh3vs","_id":"cltl4oxew0077xh7k92etawuy"},{"post_id":"cltl4oxel000rxh7kh777euay","tag_id":"cltl4oxev006fxh7k6rng5u0v","_id":"cltl4oxew0079xh7k55jz8c2q"},{"post_id":"cltl4oxel000rxh7kh777euay","tag_id":"cltl4oxev006hxh7k4cn8hcq9","_id":"cltl4oxew007axh7k87bz8v6m"},{"post_id":"cltl4oxel000rxh7kh777euay","tag_id":"cltl4oxej000kxh7k4mh56brx","_id":"cltl4oxew007cxh7k6iz3axmp"},{"post_id":"cltl4oxel000rxh7kh777euay","tag_id":"cltl4oxev006ixh7kgdcd29dc","_id":"cltl4oxew007dxh7kc7ndawdj"},{"post_id":"cltl4oxel000rxh7kh777euay","tag_id":"cltl4oxev006jxh7k1ejk02sv","_id":"cltl4oxew007fxh7kb5f57osx"},{"post_id":"cltl4oxel000rxh7kh777euay","tag_id":"cltl4oxev006kxh7khf8cducv","_id":"cltl4oxew007gxh7kcydcas6p"},{"post_id":"cltl4oxel000rxh7kh777euay","tag_id":"cltl4oxev006lxh7k09zw95ci","_id":"cltl4oxew007ixh7k3iv740ob"},{"post_id":"cltl4oxel000rxh7kh777euay","tag_id":"cltl4oxev006mxh7kcncec60u","_id":"cltl4oxex007jxh7k5lsi3qcm"},{"post_id":"cltl4oxel000rxh7kh777euay","tag_id":"cltl4oxev006nxh7kbyzd7r0f","_id":"cltl4oxex007lxh7kfruie3o0"},{"post_id":"cltl4oxel000rxh7kh777euay","tag_id":"cltl4oxev006oxh7kaehpf2kl","_id":"cltl4oxex007mxh7kha08d2wo"},{"post_id":"cltl4oxel000rxh7kh777euay","tag_id":"cltl4oxeo001kxh7kazm95q38","_id":"cltl4oxex007nxh7k9erk1sah"},{"post_id":"cltl4oxel000rxh7kh777euay","tag_id":"cltl4oxev006qxh7k6wcvhswi","_id":"cltl4oxex007pxh7kb05odzes"},{"post_id":"cltl4oxel000rxh7kh777euay","tag_id":"cltl4oxev006rxh7k63p98qte","_id":"cltl4oxex007qxh7kb32x190q"},{"post_id":"cltl4oxel000rxh7kh777euay","tag_id":"cltl4oxev006sxh7k32yp31ad","_id":"cltl4oxex007sxh7kadb18we1"},{"post_id":"cltl4oxel000sxh7k05fk4sya","tag_id":"cltl4oxeg0005xh7khank0y94","_id":"cltl4oxex007txh7k7pv6g5oa"},{"post_id":"cltl4oxel000sxh7k05fk4sya","tag_id":"cltl4oxet005fxh7kd5fz3keb","_id":"cltl4oxex007vxh7k3716evfa"},{"post_id":"cltl4oxel000sxh7k05fk4sya","tag_id":"cltl4oxes004nxh7ke5mq0ibm","_id":"cltl4oxex007wxh7k1zvq3a6i"},{"post_id":"cltl4oxel000sxh7k05fk4sya","tag_id":"cltl4oxet005exh7k3bfue7h4","_id":"cltl4oxex007yxh7kblticc8x"},{"post_id":"cltl4oxel000sxh7k05fk4sya","tag_id":"cltl4oxet005gxh7ka0n7csv5","_id":"cltl4oxex007zxh7k1y6i3m46"},{"post_id":"cltl4oxel000sxh7k05fk4sya","tag_id":"cltl4oxej000kxh7k4mh56brx","_id":"cltl4oxex0081xh7k36ky2cfh"},{"post_id":"cltl4oxel000sxh7k05fk4sya","tag_id":"cltl4oxew0075xh7kd5geekx0","_id":"cltl4oxex0082xh7kgimmbw1d"},{"post_id":"cltl4oxel000sxh7k05fk4sya","tag_id":"cltl4oxew0078xh7kglay1tj4","_id":"cltl4oxex0084xh7k3ztlcypq"},{"post_id":"cltl4oxem000wxh7kge2td07k","tag_id":"cltl4oxeo001exh7kgdn55uua","_id":"cltl4oxey008axh7kaf49cx32"},{"post_id":"cltl4oxem000wxh7kge2td07k","tag_id":"cltl4oxew007exh7kewrv7u8o","_id":"cltl4oxey008bxh7kb0paejak"},{"post_id":"cltl4oxem000wxh7kge2td07k","tag_id":"cltl4oxep0026xh7k2xhl61nn","_id":"cltl4oxey008dxh7kebbg1dlj"},{"post_id":"cltl4oxem000wxh7kge2td07k","tag_id":"cltl4oxex007kxh7kgaqn0nnw","_id":"cltl4oxey008exh7k0r0fgnsi"},{"post_id":"cltl4oxem000wxh7kge2td07k","tag_id":"cltl4oxeo001hxh7kc5qi9ef7","_id":"cltl4oxey008gxh7kdeoe31lf"},{"post_id":"cltl4oxem000wxh7kge2td07k","tag_id":"cltl4oxex007rxh7k0wxb4wnj","_id":"cltl4oxey008hxh7kcndpdncq"},{"post_id":"cltl4oxem000wxh7kge2td07k","tag_id":"cltl4oxeo001sxh7kegqn7ff5","_id":"cltl4oxey008jxh7kgiwjgmfe"},{"post_id":"cltl4oxem000wxh7kge2td07k","tag_id":"cltl4oxex007xxh7k9exihhsl","_id":"cltl4oxey008kxh7kckrxah1f"},{"post_id":"cltl4oxem000wxh7kge2td07k","tag_id":"cltl4oxex0080xh7k8285833d","_id":"cltl4oxey008mxh7kfnc1gdyb"},{"post_id":"cltl4oxem000wxh7kge2td07k","tag_id":"cltl4oxex0083xh7kcy0v7td3","_id":"cltl4oxey008nxh7k2coq7wfc"},{"post_id":"cltl4oxem000wxh7kge2td07k","tag_id":"cltl4oxex0085xh7k6n8q8ah4","_id":"cltl4oxey008pxh7k1m9320f9"},{"post_id":"cltl4oxem000wxh7kge2td07k","tag_id":"cltl4oxex0086xh7ke1ztd0nq","_id":"cltl4oxey008qxh7k6bpyhudw"},{"post_id":"cltl4oxem000wxh7kge2td07k","tag_id":"cltl4oxes0043xh7k2kxh10wn","_id":"cltl4oxey008sxh7k8b2x0dq4"},{"post_id":"cltl4oxem000wxh7kge2td07k","tag_id":"cltl4oxex0088xh7k3ivpfxk4","_id":"cltl4oxey008txh7ke1dgboku"},{"post_id":"cltl4oxem000yxh7kee7oacqe","tag_id":"cltl4oxex0089xh7kc78g7555","_id":"cltl4oxey008xxh7kdekrbgac"},{"post_id":"cltl4oxem000yxh7kee7oacqe","tag_id":"cltl4oxey008cxh7k6gnu5v7b","_id":"cltl4oxey008yxh7k90wn65qx"},{"post_id":"cltl4oxem000yxh7kee7oacqe","tag_id":"cltl4oxep0026xh7k2xhl61nn","_id":"cltl4oxey008zxh7k9iw6a2kw"},{"post_id":"cltl4oxem000yxh7kee7oacqe","tag_id":"cltl4oxeq0034xh7k002s4pqb","_id":"cltl4oxey0090xh7k403j13ig"},{"post_id":"cltl4oxem000yxh7kee7oacqe","tag_id":"cltl4oxeq002txh7ka5atb1wn","_id":"cltl4oxey0091xh7k3b63b33f"},{"post_id":"cltl4oxem000yxh7kee7oacqe","tag_id":"cltl4oxey008oxh7khcnpb0fy","_id":"cltl4oxey0092xh7kd5y1cmkb"},{"post_id":"cltl4oxem000yxh7kee7oacqe","tag_id":"cltl4oxey008rxh7ke95rfcgv","_id":"cltl4oxey0093xh7keixpfbls"},{"post_id":"cltl4oxem000yxh7kee7oacqe","tag_id":"cltl4oxey008uxh7k6lw9e97m","_id":"cltl4oxey0094xh7k26u62zto"},{"post_id":"cltl4oxem000yxh7kee7oacqe","tag_id":"cltl4oxeo001hxh7kc5qi9ef7","_id":"cltl4oxey0095xh7k3jm825b4"},{"post_id":"cltl4oxem000yxh7kee7oacqe","tag_id":"cltl4oxex0083xh7kcy0v7td3","_id":"cltl4oxey0096xh7k87auhjpv"}],"Tag":[{"name":"life","_id":"cltl4oxeg0005xh7khank0y94"},{"name":"ä¸­ç§‹","_id":"cltl4oxei000dxh7k3v9vg1ls"},{"name":"ä¸­æ–‡","_id":"cltl4oxej000kxh7k4mh56brx"},{"name":"00","_id":"cltl4oxek000qxh7k14wog3li"},{"name":"wedding","_id":"cltl4oxem000vxh7k5w1p8d7e"},{"name":"ä¸­ç§‹-middle-autumn","_id":"cltl4oxen0012xh7kcnebc3oc"},{"name":"å›½åº†-national-day","_id":"cltl4oxen0016xh7k7vsndyq6"},{"name":"åº”åŸ","_id":"cltl4oxen001axh7k627279ga"},{"name":"æ­¦æ±‰-wuhan","_id":"cltl4oxen001cxh7k9je4bmj2"},{"name":"paper","_id":"cltl4oxeo001exh7kgdn55uua"},{"name":"research","_id":"cltl4oxeo001hxh7kc5qi9ef7"},{"name":"cfd","_id":"cltl4oxeo001kxh7kazm95q38"},{"name":"dataset","_id":"cltl4oxeo001nxh7k2yws3zeq"},{"name":"english","_id":"cltl4oxeo001sxh7kegqn7ff5"},{"name":"pinn","_id":"cltl4oxeo001txh7k4fpn407l"},{"name":"fno","_id":"cltl4oxeo001uxh7kfhrac527"},{"name":"physics","_id":"cltl4oxeo001vxh7kc6cufb5s"},{"name":"machine-learning","_id":"cltl4oxeo001wxh7kc7dl7ghh"},{"name":"deep-learning","_id":"cltl4oxeo001xxh7k52kjcz1e"},{"name":"deeponet","_id":"cltl4oxeo001yxh7k786udfz2"},{"name":"ai4science","_id":"cltl4oxeo001zxh7keq2406bh"},{"name":"ai-alignment","_id":"cltl4oxep0023xh7k91jpa4xc"},{"name":"llm","_id":"cltl4oxep0026xh7k2xhl61nn"},{"name":"gpt","_id":"cltl4oxep0029xh7karbxddgz"},{"name":"activation-modification","_id":"cltl4oxep002cxh7kch8h39zw"},{"name":"adaptation","_id":"cltl4oxep002gxh7kdv61gdf4"},{"name":"model-editing","_id":"cltl4oxep002jxh7k4g111uwu"},{"name":"representation-engineering","_id":"cltl4oxep002kxh7k99023bmu"},{"name":"fine-tuning","_id":"cltl4oxep002lxh7kbk6594qr"},{"name":"parameter-efficient-tuning","_id":"cltl4oxep002mxh7k0fuy6d90"},{"name":"nlp","_id":"cltl4oxeq002txh7ka5atb1wn"},{"name":"long-context","_id":"cltl4oxeq002wxh7k11lraz4y"},{"name":"benchmark","_id":"cltl4oxeq002zxh7kgms546b8"},{"name":"recurrence","_id":"cltl4oxeq0032xh7kbub7f1t4"},{"name":"linear-attention","_id":"cltl4oxeq0033xh7kh8189guc"},{"name":"transformer","_id":"cltl4oxeq0034xh7k002s4pqb"},{"name":"activation-engineering","_id":"cltl4oxeq0038xh7kgyvv6tq1"},{"name":"interpretability","_id":"cltl4oxeq003exh7kahxrblg0"},{"name":"rl","_id":"cltl4oxer003hxh7kh1m1hyfk"},{"name":"alignment","_id":"cltl4oxer003ixh7kcfzxdza6"},{"name":"maze","_id":"cltl4oxer003kxh7k55md4emg"},{"name":"ethics","_id":"cltl4oxer003xxh7kgkyp9vgf"},{"name":"safety","_id":"cltl4oxer003yxh7k8m2l3fvy"},{"name":"é¢å£æ™ºèƒ½","_id":"cltl4oxes0040xh7khish02ig"},{"name":"modelbest","_id":"cltl4oxes0041xh7k1urp35be"},{"name":"tutorial","_id":"cltl4oxes0042xh7kb7ifej2q"},{"name":"eren","_id":"cltl4oxes0043xh7k2kxh10wn"},{"name":"chatgpt","_id":"cltl4oxes0044xh7k9spvftri"},{"name":"claude","_id":"cltl4oxes0045xh7k06c5dz5i"},{"name":"ä¸‰ä½“","_id":"cltl4oxes0046xh7k6hew8dhg"},{"name":"algorithm","_id":"cltl4oxes0047xh7kg8bq575r"},{"name":"binary-search","_id":"cltl4oxes004axh7kdh417zp3"},{"name":"rust","_id":"cltl4oxes004dxh7k0gww98jp"},{"name":"python","_id":"cltl4oxes004gxh7kaoq3173t"},{"name":"c++","_id":"cltl4oxes004kxh7k5co2ep2e"},{"name":"test","_id":"cltl4oxes004nxh7ke5mq0ibm"},{"name":"code","_id":"cltl4oxet004txh7k91p7ajon"},{"name":"school","_id":"cltl4oxet004xxh7k6vcq0rfb"},{"name":"graduation","_id":"cltl4oxet0050xh7kew2b65ub"},{"name":"blog","_id":"cltl4oxet0053xh7k3l294op5"},{"name":"ç­¾è¯","_id":"cltl4oxet0059xh7k1hqhcxbj"},{"name":"hexo","_id":"cltl4oxet005dxh7kbbvy8hbj"},{"name":"hugo","_id":"cltl4oxet005exh7k3bfue7h4"},{"name":"jekyll","_id":"cltl4oxet005fxh7kd5fz3keb"},{"name":"static-site-generator","_id":"cltl4oxet005gxh7ka0n7csv5"},{"name":"ç¾½æ¯›çƒ","_id":"cltl4oxet005ixh7kb9um7ou9"},{"name":"work","_id":"cltl4oxeu005jxh7k75a3d4sr"},{"name":"æ«å¶","_id":"cltl4oxeu005lxh7k5wm7csac"},{"name":"ä¸­å›½","_id":"cltl4oxeu005mxh7kb9gqgy6k"},{"name":"æŒªå¨","_id":"cltl4oxeu005nxh7kcsfm7yf2"},{"name":"markdown","_id":"cltl4oxeu005oxh7kac9v31fc"},{"name":"å®¿èˆ","_id":"cltl4oxeu005pxh7kdlg035y5"},{"name":"å­©å­ä»¬","_id":"cltl4oxeu005wxh7k4h0w8u5e"},{"name":"å§é¾™","_id":"cltl4oxeu005zxh7kh6dv8423"},{"name":"å‡¤é›","_id":"cltl4oxeu0062xh7kfro8hoiw"},{"name":"éª†é›","_id":"cltl4oxeu0065xh7k29jsd8v4"},{"name":"é»„å¸","_id":"cltl4oxev0069xh7kgufo3xyx"},{"name":"å°ç»¿","_id":"cltl4oxev006cxh7kes1wh3vs"},{"name":"çŒ«å’ª","_id":"cltl4oxev006fxh7k6rng5u0v"},{"name":"åœŸé¸¡","_id":"cltl4oxev006hxh7k4cn8hcq9"},{"name":"ç†Š","_id":"cltl4oxev006ixh7kgdcd29dc"},{"name":"ğŸ»","_id":"cltl4oxev006jxh7k1ejk02sv"},{"name":"ğŸ±","_id":"cltl4oxev006kxh7khf8cducv"},{"name":"ğŸ‡","_id":"cltl4oxev006lxh7k09zw95ci"},{"name":"ğŸ°","_id":"cltl4oxev006mxh7kcncec60u"},{"name":"ğŸŠ","_id":"cltl4oxev006nxh7kbyzd7r0f"},{"name":"emoren","_id":"cltl4oxev006oxh7kaehpf2kl"},{"name":"acl","_id":"cltl4oxev006qxh7k6wcvhswi"},{"name":"lillesand","_id":"cltl4oxev006rxh7k63p98qte"},{"name":"åŠ æ‹¿å¤§","_id":"cltl4oxev006sxh7k32yp31ad"},{"name":"html","_id":"cltl4oxew0075xh7kd5geekx0"},{"name":"liquid","_id":"cltl4oxew0078xh7kglay1tj4"},{"name":"arxiv","_id":"cltl4oxew007exh7kewrv7u8o"},{"name":"knowledge","_id":"cltl4oxex007kxh7kgaqn0nnw"},{"name":"emnlp","_id":"cltl4oxex007rxh7k0wxb4wnj"},{"name":"model editing","_id":"cltl4oxex007xxh7k9exihhsl"},{"name":"in-context-learning","_id":"cltl4oxex0080xh7k8285833d"},{"name":"ai","_id":"cltl4oxex0083xh7kcy0v7td3"},{"name":"serac","_id":"cltl4oxex0085xh7k6n8q8ah4"},{"name":"rome","_id":"cltl4oxex0086xh7ke1ztd0nq"},{"name":"mend","_id":"cltl4oxex0088xh7k3ivpfxk4"},{"name":"language modeling","_id":"cltl4oxex0089xh7kc78g7555"},{"name":"language models","_id":"cltl4oxey008cxh7k6gnu5v7b"},{"name":"rnn","_id":"cltl4oxey008oxh7khcnpb0fy"},{"name":"deep learning","_id":"cltl4oxey008rxh7ke95rfcgv"},{"name":"machine learning","_id":"cltl4oxey008uxh7k6lw9e97m"}]}}