<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>CFDBench: A Comprehensive Benchmark for Machine Learning Methods in Fluid Dynamics</title>
    <url>/2023/09/16/CFDBench/</url>
    <content><![CDATA[<p><a href="https://www.github.com/luo-yining/CFDBench">Code</a> | <a href="...">Paper (on hold by ArXiv)</a> | <a href="https://www.preprints.org/manuscript/202309.1550/v1">Paper (preprints.org)</a> | <a href="https://zhuanlan.zhihu.com/p/656033757">çŸ¥ä¹</a></p>
<p>I did this work with my girlfriend, whose research direction is computational fluid dynamics (CFD). We observed that there are numerous research works in applying deep learning (DL) to solve CFD problems. E.g., <a href="https://github.com/198808xc/Pangu-Weather">Pangu-Weather</a> have shown that DL methods can not only be more accurate than the best numerical methods, but can also be multiple magnitudes faster.</p>
<span id="more"></span>
<p>However, there is no standard benchmark for evaluating the performance of different DL methods. Therefore, we constructed CFDBench.</p>
<hr>
<h2 id="Abstract">Abstract</h2>
<p>In recent years, applying deep learning to solve physics problems has attracted much attention. Data-driven deep learning methods produce operators that can learn solutions to the whole system of partial differential equations. However, the existing methods are only evaluated on simple flow equations (e.g., Burger's equation), and only consider the generalization ability on different initial conditions. In this paper, we construct CFDBench, a benchmark with four classic problems in computational fluid dynamics (CFD): lid-driven cavity flow, laminar boundary layer flow in circular tubes, dam flows through the steps, and periodic Karman vortex street. Each flow problem includes data with different boundary conditions, fluid physical properties, and domain geometry. Compared to existing datasets, the advantages of CFDBench are (1) comprehensive. It contains common physical parameters such as velocity, pressure, and cavity fraction. (2) realistic. It is very suitable for deep learning solutions of fluid mechanics equations. (3) challenging. It has a certain learning difficulty, prompting to find models with strong learning ability. (4) standardized. CFDBench facilitates a comprehensive and fair comparison of different deep learning methods for CFD. We make appropriate modifications to popular deep neural networks to apply them to CFDBench and enable the accommodation of more changing inputs. The evaluation on CFDBench reveals some new shortcomings of existing works and we propose possible directions for solving such problems.</p>
]]></content>
      <categories>
        <category>Research</category>
      </categories>
      <tags>
        <tag>research</tag>
        <tag>paper</tag>
        <tag>cfd</tag>
        <tag>dataset</tag>
        <tag>00</tag>
        <tag>english</tag>
        <tag>pinn</tag>
        <tag>fno</tag>
        <tag>physics</tag>
        <tag>machine-learning</tag>
        <tag>deep-learning</tag>
        <tag>deeponet</tag>
        <tag>ai4science</tag>
      </tags>
  </entry>
  <entry>
    <title>2023å¹´ä¸­ç§‹å’Œå›½åº†</title>
    <url>/2023/10/05/2023%E4%B8%AD%E7%A7%8B/</url>
    <content><![CDATA[<p>ä»Šå¹´å›½åº† ğŸ‡¨ğŸ‡³ å’Œä¸­ç§‹ ğŸ¥® ä¸€èµ·æ”¾å‡ï¼Œæˆ‘è·Ÿ 00 ä¸€èµ·å›æ¥åº”åŸå‚åŠ å¥¹å ‚å§å’Œåˆä¸­åŒå­¦çš„å©šç¤¼<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>ï¼Œ
ä½åœ¨å¥¹å®¶é‡Œåä¸ªå¤œæ™š<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup>ã€‚ç¬¬äºŒæ¬¡è§å®¶é•¿ï¼Œä¹Ÿç®—æ˜¯æŒºé¡ºåˆ©ï¼Œä½†æ˜¯æ¯å¤©éƒ½ä¼šè§åˆ°é™Œç”Ÿäººï¼Œæœ‰ç‚¹ç´¯ï¼Œåº†å¹¸çš„æ˜¯ï¼Œæ„Ÿè§‰åˆ° 00 èƒ½æ¥å—è·Ÿæˆ‘å®¶äººç”Ÿæ´»åœ¨ä¸€èµ·ã€‚ä¸€å·åˆ°ä¸‰å·æˆ‘ä»¬å»æ­¦æ±‰ç©äº†ä¸‰å¤©ï¼Œè¶…çº§å¼€å¿ƒï¼Œè·Ÿå¥¹åœ¨ä¸€èµ·è¿é€›å•†åœºéƒ½æ˜¯å¼€å¿ƒçš„ï¼</p>
<h2 id="å°å¿åŸçš„æ°›å›´">å°å¿åŸçš„æ°›å›´</h2>
<p>åº”åŸè·Ÿæˆ‘æƒ³è±¡ä¸­çš„å°å¿åŸå¾ˆåƒï¼Œä¹Ÿæ˜¯å¾ˆå¤šè¿œæˆ¿äº²æˆšï¼Œä¹ ä¿—ä¹Ÿè®©äººå¾ˆçƒ¦ã€‚æ•¬é…’ã€éšåœ°æ‰”åƒåœ¾ã€å®¤å†…æŠ½çƒŸã€å…«å¦äººå®¶çš„ç§äº‹ã€è¯´è¯ç²—é„™ã€è„ã€è¯´äº†ä¸è¦è¿˜éè¦ç»™äººå®¶â€¦â€¦è€Œä¸”ç¡®å®èƒ½æ˜æ˜¾æ„Ÿè§‰åˆ°ï¼Œè¿™é‡Œçš„äººçš„ç´ è´¨çš„å¹³å‡æ°´å¹³æŒºä½çš„ï¼Œå°¤å…¶æ˜¯ä¸Šä¸€è¾ˆã€‚çœŸçš„å¾ˆè®¨åŒåƒå¸­ï¼Œ00 ä¹Ÿæ˜¯ï¼Œè¿™äº›ä¹ ä¿—çš„éº»çƒ¦ç¨‹åº¦è®© 00 éƒ½ä¸æƒ³ç»“å©šäº†â€¦â€¦</p>
<span id="more"></span>
<p>ä½†æ˜¯æ— æ‰€è°“äº†ï¼Œä¹‹åèƒ½è·Ÿ 00 åœ¨ä¸€èµ·å°±å¥½ï¼Œé™¤äº†å›æ¥è¿‡èŠ‚åº”è¯¥ä¹Ÿå¾ˆå°‘æœºä¼šæœ‰è”ç³»ã€‚</p>
<h2 id="æ­¦æ±‰">æ­¦æ±‰</h2>
<p>ä¸€å·åˆ°ä¸‰å·å»äº†æ­¦æ±‰æ—…æ¸¸ã€‚æ—©ä¸Šäº”ç‚¹å¤šè·Ÿ 00 çš„ â€äºŒå¦ˆâ€œï¼ˆå…¶å®æ˜¯å©¶ï¼Œå”å”çš„è€å©†ï¼‰åè½¦å»æ­¦æ±‰ï¼Œåäº†ä¸€ä¸ªå°æ—¶ã€‚ä»–ä»¬è¿™ä¹ˆæ—©æ˜¯å› ä¸ºè¦å»è°ˆå©šç¤¼çš„äº‹æƒ…ï¼Œç„¶åå®³æ€•å µè½¦ã€‚æˆ‘ä»¬åœ¨é…’åº—æ—è¾¹ä¸‹æ¥ï¼Œé‚£æ—¶å€™â€œäºŒå¦ˆâ€ä¸‹åœ°é“ç«™ä¸Šå•æ‰€ï¼Œç„¶å 00 éè¦ç»™å¥¹ä¹°åŒ…å­ï¼ˆä¸ºäº†ç¤¼è²Œï¼‰ï¼Œç„¶åå¥¹æœ€åè¿˜æ˜¯æ‹’ç»äº†ï¼Œå¯¼è‡´æˆ‘ä»¬å¾—è‡ªå·±åƒä¸‹åŒ…å­ã€‚è™½ç„¶åŒ…å­æ²¡æœ‰ä¸å¥½åƒï¼Œä½†æ˜¯æˆ‘å°±å¾ˆè®¨åŒè¿™ç§æ˜çŸ¥äººå®¶ä¸è¦è¿˜éè¦ä¹°çš„è¡Œä¸ºã€‚</p>
<p>ä¹‹åæˆ‘ä»¬å»é…’åº—çš„æ—¶å€™ï¼Œè¿˜æ²¡æœ‰æˆ¿å­ï¼Œæˆ‘ä»¬å¯„å­˜äº†è¡Œæå°±ç›´æ¥å»æ–°å¤©åœ°ä¹°äº†æ¯éœ¸ç‹èŒ¶å§¬çš„å¥¶èŒ¶ï¼Œç„¶åå»äº†å¤å¾·å¯ºã€‚ç½‘ä¸Šè¯´ä¸å¯ä»¥ç©¿ç€æš´éœ²ï¼Œä½†æ˜¯æ„Ÿè§‰è·¯äººç©¿ç€è¿˜æ˜¯å¾ˆæš´éœ²ã€‚</p>
<p><img src="/2023/10/05/2023%E4%B8%AD%E7%A7%8B/%E6%96%B0%E5%A4%A9%E5%9C%B0-%E9%9C%B8%E7%8E%8B%E8%8C%B6%E5%A7%AC.png" alt="" title="åœ¨æ­¦æ±‰æ–°å¤©åœ°ä¹°éœ¸ç‹èŒ¶å§¬ã€‚"></p>
<p>ä¹‹åè¿˜å»äº†è§£æ”¾å…¬å›­å’Œä¸­å±±å…¬å›­ï¼Œéƒ½æŒºä¸é”™çš„ã€‚å¤§åŸå¸‚å°±æ˜¯å¥½ã€‚é‡Œé¢çœ‹åˆ°äº†å¾ˆå¥½çœ‹çš„å»ºç­‘ç‰©ã€‚åœ¨ä¸­å±±å…¬å›­æˆ‘ä»¬é—®äº†ä¸¤ä¸ªå°å­©å€Ÿç”¨ç¾½æ¯›çƒæ‹å­æ¥æ‰“äº†å‡ ä¸‹ã€‚ä¹‹ååœ¨ä¸€ä¸ªç›¸äº²è§’<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup>æ—è¾¹è·Ÿå¥¹çš„é«˜ä¸­åŒå­¦ï¼Œå½­åŒï¼Œä¼šåˆï¼Œç„¶åé€›äº†ä¸€ä¸‹ç›¸äº²è§’ã€‚ä¹‹åæˆ‘ä»¬è¿˜åäº†ä¸€ä¸‹è¿‡å±±è½¦ï¼ˆå…¬å›­é‡Œé¢æœ‰è¿‡å±±è½¦è¿˜æ˜¯ç¬¬ä¸€æ¬¡è§ï¼‰ã€‚</p>
<p><img src="/2023/10/05/2023%E4%B8%AD%E7%A7%8B/%E8%A7%A3%E6%94%BE%E5%85%AC%E5%9B%AD%E4%B8%AD%E9%97%B4.png" alt="" title="è§£æ”¾å…¬å›­ä¸­é—´çš„ä¸€ä¸ªå¾ˆå¤šå¡”çš„åœ°æ–¹ã€‚"></p>
<p>æ™šä¸Šå°±å»è·Ÿå¥¹çš„é«˜ä¸­åŒå­¦ä¸€èµ·åƒé¥­ã€‚</p>
<p>ç¬¬äºŒå¤©æˆ‘ä»¬å…ˆåœ¨åœ°é“ç«™å‰ªäº†å¤´å‘ï¼Œç„¶åå»å®é€šå¯ºï¼Œæ™šä¸Šå»æ­¦å•†æ¢¦æ—¶ä»£ã€‚è¿™ä¸ªå•†åœºè§„æ ¼è¶…çº§é«˜ï¼Œè¿˜æŒºå¥½ç©çš„ã€‚ç¬¬ä¸€æ¬¡çœ‹åˆ°ç´¢å°¼ä¸“å–åº—ï¼Œè¿˜æœ‰ Pico ä¸“å–åº—ã€‚é‡Œé¢è¿˜æœ‰æ»‘é›ªçš„åœ°æ–¹ï¼Œä½†æ˜¯å¤ªè´µçš„ã€‚æˆ‘ä»¬è¿˜å»äº†ä¼˜è¡£åº“ï¼Œä¹°äº†ä¸€äº›è¡£æœï¼Œå‘ç°è¿˜æŒºä¾¿å®œçš„ã€‚ä»¥å‰éƒ½ä¼šè§‰å¾—é€›è¡—è´­ç‰©å¾ˆæ— èŠï¼Œä½†æ˜¯è·Ÿå¥¹åœ¨ä¸€èµ·è¿è¿é€›è¡—ä¹°è¡£æœéƒ½æ˜¯å¼€å¿ƒçš„ã€‚</p>
<p>æ™šä¸Šæˆ‘ä»¬è·Ÿå¥¹â€œå¤§å“¥â€ï¼ˆå…¶å®æ˜¯å ‚å“¥ï¼‰å’Œä»–è€å©†ä¸€èµ·åƒé¥­ï¼Œåƒäº†é­”å®—çƒ¤è‚‰ï¼Œç„¶åå–äº†èŒ¶é¢œæ‚¦è‰²ã€‚æ€»ä½“æ¥è¯´ä¹ŸæŒºé¡ºåˆ©çš„ï¼Œæ„Ÿè§‰ä»–ä»¬ä¹Ÿä¸éš¾ç›¸å¤„ã€‚</p>
<p><img src="/2023/10/05/2023%E4%B8%AD%E7%A7%8B/%E6%AD%A6%E5%95%86%E6%A2%A6%E6%97%B6%E4%BB%A3.png" alt="" title="æ­¦å•†æ¢¦æ—¶ä»£é‡Œé¢çš„ç¾é£Ÿè¡—ä¹°é²œè™¾æ±¤åŒ…"></p>
<p>ç¬¬ä¸‰å¤©æˆ‘ä»¬å»äº†æ¬¢ä¹è°·ï¼æ˜¯æˆ‘ä»¬ç¬¬ä¸€æ¬¡ä¸€èµ·å»æ¸¸ä¹åœºï¼ç©äº†ä¸€ä¸ªè¿‡å±±è½¦ï¼Œç„¶ååšäº†å¤ªé˜³é£è½¦ï¼Œ00 å°±å¤´æ™•æƒ³åäº†ï¼Œæœç„¶è¿˜æ˜¯ä¸è¡Œâ€¦â€¦ä½†æ˜¯æ²¡äº‹ï¼Œè¿˜æ˜¯æŒºå¼€å¿ƒçš„ã€‚æ’é˜Ÿè¿‡ç¨‹ä¸­è¿˜é‡åˆ°äº†æ’é˜Ÿçš„äººï¼Œå¥½æ¶å¿ƒï¼</p>
<p>æ™šä¸Šæˆ‘ä»¬è·Ÿä¸€äº›äººï¼ˆå…±ä¸ƒä¸ªäººï¼‰ä¸€èµ·æ‹¼è½¦å›æ¥åº”åŸï¼Œå±…ç„¶æ¯”ç«è½¦è¿˜ä¾¿å®œï¼Œä¸é”™ã€‚å›æ¥å·²ç»11ç‚¹äº†ï¼Œç„¶åå›å®¶æ”¾ä¸‹è¡Œæç®±ä¹‹ååˆå‡ºå»æ‰¾å¥¹åˆä¸­åŒå­¦ä¸€èµ·åƒå®µå¤œã€‚</p>
<p><img src="/2023/10/05/2023%E4%B8%AD%E7%A7%8B/%E6%AD%A6%E6%B1%89%E6%AC%A2%E4%B9%90%E8%B0%B7.png" alt="" title="åœ¨æ­¦æ±‰æ¬¢ä¹è°·ç©è€ã€‚"></p>
<h2 id="å…¬äº‹">å…¬äº‹</h2>
<p>è¿™ä¸ªå‡æœŸæœ‰ç‚¹é•¿ï¼Œæ„Ÿè§‰æœ‰å¾ˆå¤šæ´»éƒ½æ²¡æœ‰å¹²ã€‚æ¯å¤©éƒ½å¾ˆå¤šäº‹æƒ…ï¼Œæ„Ÿè§‰è¿™é‡Œçš„äººå¤ªé—²äº†ï¼Œåº”è¯¥è®©ä»–ä»¬å¤šä¸Šç­å“ˆå“ˆå“ˆã€‚å¤æ–‡å­—ç¿»è¯‘çš„å·¥ä½œè¿˜æ²¡æœ‰å¹²å®Œï¼Œç›®å‰æ„Ÿè§‰æ•ˆæœä¸æ˜¯å¾ˆå¥½ï¼Œæˆ‘ä¹Ÿä¸æƒ³å¹²è¿™ä¸ªäº†ï¼Œæ„Ÿè§‰å¾ˆæµªè´¹æˆ‘çš„æ—¶é—´â€¦â€¦è‡³äºå¯¹é½ç¥ç»å…ƒï¼Œè²Œä¼¼ç°æœ‰æ–¹æ³•éƒ½æ— æ³•ç”¨åœ¨è‡ªå›å½’æ¨¡å‹ä¸Šé¢ï¼Œä½†æ˜¯å¯¹é½é—®é¢˜å¥½åƒä¹‹åè‡ªå›å½’æ¨¡å‹æ‰ä¼šå‡ºç°ã€‚ä¸çŸ¥é“æ˜¯ä¸æ˜¯æˆ‘æ²¡æœ‰æ‰¾åˆ°ï¼Œç›®å‰è¿˜æ²¡æœ‰æ‰¾åˆ°ä¸€ç¯‡ç ”ç©¶ç¥ç»å…ƒå¯¹ç”Ÿæˆç»“æœçš„å½±å“çš„å·¥ä½œã€‚<a href="https://www.github.com/kmeng01/rome">ROME</a> çš„ Causal Tracing æ„Ÿè§‰å¯ä»¥ç”¨ï¼Œè¿™ä¸¤å¤©å¾—èµ¶ç´§åšç‚¹ä¸œè¥¿å‡ºæ¥ã€‚</p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>27å·æ˜¯åˆä¸­åŒå­¦ï¼ˆé­é™ˆï¼‰çš„å©šç¤¼ï¼Œ5å·æ˜¯å ‚å§ï¼ˆéª†å“é¢–ï¼‰çš„å©šç¤¼ã€‚ <a href="#fnref1" class="footnote-backref">â†©ï¸</a></p>
</li>
<li id="fn2" class="footnote-item"><p>ä¹æœˆäºŒåå…­æ—¥å›æ¥ï¼Œåæœˆä¸ƒæ—¥èµ°ã€‚åé«˜é“åˆ°åŒ—äº¬ï¼Œç„¶ååšç«è½¦åˆ°åº”åŸã€‚ <a href="#fnref2" class="footnote-backref">â†©ï¸</a></p>
</li>
<li id="fn3" class="footnote-item"><p>ä¹‹å‰åœ¨ä¸Šæµ·éƒ½æ²¡æ‰¾åˆ°ã€‚ <a href="#fnref3" class="footnote-backref">â†©ï¸</a></p>
</li>
</ol>
</section>
]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>00</tag>
        <tag>life</tag>
        <tag>ä¸­ç§‹</tag>
        <tag>ä¸­æ–‡</tag>
        <tag>wedding</tag>
        <tag>ä¸­ç§‹-middle-autumn</tag>
        <tag>å›½åº†-national-day</tag>
        <tag>åº”åŸ</tag>
        <tag>æ­¦æ±‰-wuhan</tag>
      </tags>
  </entry>
  <entry>
    <title>Activation Addition (ActAdd)</title>
    <url>/2023/10/07/actadd/</url>
    <content><![CDATA[<p><a href="https://arxiv.org/abs/2308.10248">Paper</a></p>
<p>TLDR: Propose <strong>ActAdd</strong>, a method for controlling model behavior during inference by modifying activations with a bias term that is learned from a pair of prompt.</p>
<p>Summary:</p>
<ul>
<li>Propose <strong>ActAdd</strong>, a method for controlling model behavior by modifying activations at inference time.</li>
<li>Steering vectors are computed by taking the activation differences that result from pairs of prompts. The vectors are added as bias during inference.</li>
<li>ActAdd provides control over high-level properties of the output, and preserves off-target model performance, and requires little computational and implementational costs.</li>
</ul>
<span id="more"></span>
<blockquote>
<p>The recently popular <a href="https://arxiv.org/abs/2310.01405">representation engineering paper</a> (RepE) seems to be largely inspired by this work.</p>
</blockquote>
<h2 id="Background">Background</h2>
<p>The authors propose to compute steering vectors to steer the model's behavior. They call such methods <em>activation engineering</em>. They make the following contributions:</p>
<ul>
<li>Find that combining forward passes works well in GPT-2, despite it was not trained for this.</li>
<li>The proposed method, <strong>ActAdd</strong>, is efficient, requiring no gradient descent or labeled data.</li>
</ul>
<p>The difference between ActAdd and existing steering vector methods is that they find the vectors via one of the following.</p>
<ul>
<li>Differences after fine-tuning</li>
<li>Per-query gradient-based search</li>
<li>Linear probes + differences in truthy attention heads</li>
</ul>
<p>In contrast, ActAdd uses the difference between prompt pairs instead.</p>
<h2 id="Method">Method</h2>
<p>The method is really, really simple. Simply manually contruct a pair of prompts, and compute the difference between the activations. Then, add the difference as a bias term to the activations during inference. The algorithm is as follows.</p>
<p><img src="/2023/10/07/actadd/alg.png" alt="" title="The algorithm of ActAdd"></p>
<p>As shown, this method has two hyperparameters, the amount of drift $c$, and the modified layer $l$, and requires two manually constructed prompts $(p_+, p_-)$. How to more effectively construct these prompts is not discussed in this paper.</p>
<h2 id="Result">Result</h2>
<p><img src="/2023/10/07/actadd/result.png" alt="" title="Main results."></p>
<h2 id="My-Thoughts">My Thoughts</h2>
<p>The effectiveness of this method is a strong evidence that input and output features are represented as linear directions in representational space, but we still have no explanation for why such linearity arises naturally in LLMs. The fact that this actually works is very thought-provoking. However, ActAdd start to see degraded performance on off-target inputs when we drive the activations to far off, and the steering sometimes simply fails, this may indicate that the optimal steering path is not linear, which I believe is reasonable. This is somewhat similar to neuron attribution methods, many features/skills/knowledge cannot be attributed to single neurons (they are distributed across neurons), but existing neuron attribution methods still work well because, by change, some features are primarily determined by the activity or state of a single neuron (or a small set of neurons).</p>
<p>We can also draw a parallel with <a href="https://arxiv.org/abs/2106.10199">BitFit</a>, which shows that tuning only (a subset of) the bias terms and the task-specific classifier head in a transformer model can achieve tuning performance comparable to full parameter finetuning. BitFit did only experiments on <strong>encoder models</strong>, in which case bias terms can be seen as steering vectors in the hidden representation space, therefore, ActAdd and BitFit differs only in the training signel. ActAdd uses the difference between the representation of a pair of (positive and negative) prompts, while BitFit propagates the human annotation from the classification head. For <strong>autoregressive models</strong>, ActAdd is more expressive because each token can be steered independently, while BitFit can only steer the whole sequence.</p>
<p>Interestingly, the fact that difference (or other arithmetics) in hidden representation are useful signal for some semantics has been shown in the era of learning word vectors, the fact that this can be used as a training signal is pretty neat.</p>
<p>The author also discussed the difference between activation engineering and adaptation methods, but the empirical results were not enough to show that ActAdd can be used as a substitute for adaptation. E.g., we cannot practically adapt the model into performing machine translation with ActAdd, because there is no negative prompt for translation. But perhaps upcoming works can apply this method to replace fine-tuning (or other adaptation methods). The realization may be a promising way to effiicently control any arbitrary model behavior without backpropagation<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>.</p>
<p>Nevertheless, this method is extremely interesting, and I feel like many existing methods can be improved by taking inspirations from this work.</p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p><a href="https://arxiv.org/abs/2305.17333">MeZO</a> is one alternative, but the training time of MeZO is almost the same as fine-tuning. <a href="#fnref1" class="footnote-backref">â†©ï¸</a></p>
</li>
</ol>
</section>
]]></content>
      <categories>
        <category>Paper Note</category>
      </categories>
      <tags>
        <tag>llm</tag>
        <tag>english</tag>
        <tag>ai-alignment</tag>
        <tag>gpt</tag>
        <tag>activation-modification</tag>
        <tag>adaptation</tag>
        <tag>model-editing</tag>
        <tag>representation-engineering</tag>
        <tag>fine-tuning</tag>
        <tag>parameter-efficient-tuning</tag>
      </tags>
  </entry>
  <entry>
    <title>(EREN) Robust and Scalable Model Editing for Large Language Models</title>
    <url>/2024/03/14/eren/</url>
    <content><![CDATA[<p><a href="https://www.github.com/chen-yingfa/eren">GitHub</a> | <a href="...">Paper (upcoming)</a></p>
<p><strong>TL;DR</strong>: A reader is augmented with a growing notebook that caches all edits in natural texts, and the reader retrieves relevant edits and make inference based on them. This achieves SOTA in model editing in QA and fact-checking.</p>
<span id="more"></span>
<hr>
<h2 id="Introduction">Introduction</h2>
<p><img src="/2024/03/14/eren/framework.png" alt=""></p>
<p>This work introduces a model editing method that addresses two issues with existing model editors:</p>
<ol>
<li>They cannot handle multiple sequential edits.</li>
<li>They cannot be applied to black-box models.</li>
</ol>
<blockquote>
<p>The reason we want to apply multiple sequential edits is that in practical applications, new knowledge appear in an on-line manner and the user might want to update the model's knowledge immediately.</p>
</blockquote>
<h2 id="Method">Method</h2>
<p>In summary, the edited LLM is complemented with a notebook that caches all edits in natural text. For each question, the model first determines whether the input is relevant to any edit, if so, it makes a prediction based on the notebook. Else, it directly answers the question using its memorized knowledge.</p>
<p>We find that LLMs, even when instruction-tuned, are not readily controllable by their context, i.e. the notebook. In particular, they are not robust to irrelevant context<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>, resulting in changed predictions on unrelated inputs. Also, the number of edits may to too large to fit into the input context of the LLM. Addressing these two issues, we propose to (1) split inference into two steps, and (2) use a dual-encoder retrieval framework to perform rough relevance estimation. Figure 1 (above) shows the overall framework of our method.<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup></p>
<h2 id="Result">Result</h2>
<p>From the following results on CounterFact (QA editing task) and FEVER (fact-checking editing task), we conclude that our LLM editor significantly outperforms existing methods.</p>
<h3 id="Metrics">Metrics</h3>
<ul>
<li>ES (Edit Success): the percentage of examples where the model correctly answers the question after the edit.</li>
<li>BP (Behavior Preservation): the percentage of unrelated examples whose output were not changed by the edit.</li>
<li>EQ (Edit Quality): the harmonic mean of ES and BP.<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup></li>
</ul>
<p>A perfect model editor should have an EQ of 1.</p>
<p><img src="/2024/03/14/eren/results.png" alt=""></p>
<p>The reason MEND and ROME performs so bad is that after many <strong>sequential</strong> edits, the model parameters are changed to much that the models start to output unintelligible tokens. In the case of fact-checking, randomly guessing "yes" or "no" should result in a 50% accuracy. However, since we use exact match to compute the accuracy, both MEND and ROME have much lower accuracy than random guess.</p>
<h3 id="Different-Number-of-Edits">Different Number of Edits</h3>
<p>For methods that modify the model parameters, each edit will have detrimental effects on the model's performance. Moreover, for methods like ROME that depend on precomputing layer statistic of the unedited model, that statistics will be gradually more inaccurate after every edit (recomputing that statistics is too costly). Therefore, after many sequential edits, the model will start to spit out gibberish, as evident in the Figure below.</p>
<p><img src="/2024/03/14/eren/diff-edit-cnt.png" alt=""></p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>which has been concluded in several previous works <a href="#fnref1" class="footnote-backref">â†©ï¸</a></p>
</li>
<li id="fn2" class="footnote-item"><p>For more details, please read the paper or contact me through e-mail. <a href="#fnref2" class="footnote-backref">â†©ï¸</a></p>
</li>
<li id="fn3" class="footnote-item"><p>Using harmonic mean because we want the model editor to have both high ES and BP. A naive editor that ignores all edits can have a BP of 1, but ES of 0. <a href="#fnref3" class="footnote-backref">â†©ï¸</a></p>
</li>
</ol>
</section>
]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>llm</tag>
        <tag>research</tag>
        <tag>ai</tag>
        <tag>paper</tag>
        <tag>english</tag>
        <tag>arxiv</tag>
        <tag>knowledge</tag>
        <tag>emnlp</tag>
        <tag>model editing</tag>
        <tag>in-context-learning</tag>
        <tag>serac</tag>
        <tag>rome</tag>
        <tag>eren</tag>
        <tag>mend</tag>
      </tags>
  </entry>
  <entry>
    <title>InfiniteBench: Extending Long Context Evaluation Beyond 100K Tokens</title>
    <url>/2024/01/10/infinitebench/</url>
    <content><![CDATA[<p><a href="http://www.github.com/OpenBMB/InfiniteBench">Code</a> | <a href="https://arxiv.org/abs/2402.13718">Paper</a></p>
<p>The first benchmark for evaluating the effectiveness of LLMs in handling more than 100k tokens!</p>
<blockquote>
<p>In the paper, we name it $\infty$-Bench, but I will sometimes use "InfiniteBench" in this blog post for better readability.</p>
</blockquote>
<p>Finally got some time to write this blog, been so busy lately! I have been in a fairly long duration of research hiatus, meanwhile the field of NLP has been revolutionized by an overwhelming number of new LLMs. Finally, I was able to arrive at some productive and meaningful work in this new era of research, as a second author. In this blog post, I will introduce this work that I have been working on recently.</p>
<span id="more"></span>
<h2 id="Background">Background</h2>
<p>The advent of LLMs have shown many promising results, but many practice applications (e.g., agents, document/webpage reading, long text summarization, etc.) are greatly limited by the context length constraint. Therefore, many works have strived to increase the length of the context that LLMs can accept. However, current "long-sequence" benchmarks all fall below 100k tokens, and is therefore not able to evaluate the effectiveness of many long-context LLMs. Our work, $\infty$-Bench</p>
<h2 id="The-Data">The Data</h2>
<p>The data consists of language tasks from diverse domains (math, code, novels), two languages (English and Chinese). Half of the tasks are automatically generated, which is desirable for optionally further scaling the context lengths to any arbitrary lengths.</p>
<p>Following shows the statistics of the tasks in our benchmark.</p>
<p><img src="/2024/01/10/infinitebench/data-stat-pie.png" alt="" title="Data statistics. The angle of segments is proportional to the number of examples, and the radius is proportional to the average example lengths (sum of input and output tokens)."></p>
<h2 id="Results">Results</h2>
<p>We tested SOTA proprietary and open-source LLMs at the time of evaluation. The result is shown below. We can see that in most tasks, the performance is far from satisfactory in practical applications.</p>
<p><img src="/2024/01/10/infinitebench/results.png" alt="" title="Results of some SOTA long-context LLMs on our InfiniteBench"></p>
<h2 id="Thoughts-on-the-Future-of-Long-Context-Research">Thoughts on the Future of Long-Context Research</h2>
<p>Our lab have been investing much efforts in long-context LLMs lately. Particularly, we are interested in developing LLMs that can accept infinite input lengths, or what some of my colleagues call streaming language models (i.e., models that operate on streaming inputs). I believe that the transformer architecture is inherently incapable of processing infinite-length inputs. This is the research direction that I have been focusing on.</p>
<h3 id="Inherent-Limitations-of-Transformers">Inherent Limitations of Transformers</h3>
<p>The most obvious reason is the quadratic complexity. A large number of research papers have focused on reducing the computational cost of the self-attention mechanism. But most SOTA LLMs at the moment are still dense attention layers, and rely on using Flash-Attention to speedup the computation. However, through discussion with various researchers, I have found that many people now believe that the attention computation is fast enough for most practical applications. I strongly disagree with this view. Firstly, the computational cost translates to the operational cost and emission that results from the usage of LLMs, which is of great concern. Secondly, my experience with ChatGPT (especially when using GPT-4) is that it is often not fast enough. Especially when it tends to produce many irrelevant lead-up sentences, I often find that I can find the answer using search engines before ChatGPT gives me the answer. Thirdly, current LLMs typically only have less than 100k in context length, however, as we apply them on contexts with millions of tokens, the speed of processing these tokens becomes unacceptable in most applications. For instance, in our experiments with InfiniteBench, applying a 7B model on 128k tokens using one A100 GPU takes 8~11 minutes to simply read the input<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>.</p>
<blockquote>
<p>This is not the only drawback of the transformer architecture, but that is out of the scope of this discussion.</p>
</blockquote>
<h3 id="Possible-Paths">Possible Paths</h3>
<p>I do not believe making small tweaks to the self-attention mechanism will solve the problem. Yep, we need new model architectures. Two architectures that I find promising are <strong>linear attention</strong> and <strong>state-space models</strong> (SSMs). Since these architectures have been widely discussed in the research community, I will not describe them in detail here. Instead, I want to express my opinion on the future of these architectures.</p>
<p>I like to think of different language model architectures from the perspective of compressing the history<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup>. The way transformers work is that they feed the last $L$ tokens without any compression to the model in each step. This means that the model remembers everything perfectly up to $L$ previous tokens, and remembers <strong>nothing</strong> about the history before that.</p>
<p>In contrast, SSM and models with linear attention can learn to automatically choose what information retain about the past, which more closely resembles how humans memorize, and provides a smoother curve of forgettance (which is likely beneficial because I think that a blurry remembrance is much better than complete forgettal beyond $n$ tokens). I firmly believe that this is the right direction to go. We are very likely to see a surge of LLMs with $O(1)$ inference cost (for one token) in the upcoming five years, and this can drastically reduce the computational costs and increase their applicability in real-world applications.</p>
<p>Some people may want to refute by saying "but recurrent models are much weaker than transformers". The thing is, most of such comparison are done in settings where the input does not exceed the context window of the transformer models. In other words, we only evaluate transformers on cases where it has perfect memory. In fact, I believe that within the context windows of a transformer model, it should be the upper bounds for the performance of recurrent models, which holds a lossful compression of the window. Moreover, I think that further research down the line can drastically improve the performance of recurrent models (actually any possible linear language models) over self-attention-based language models.</p>
<p>Another thing to note is that, I have noticed that people like to align the parameter count of different LLMs during comparison, but for models with different architecture, this is a bad practice. In practice, we likely care more about the cost of maintenance, the speed of inference or training, and memory usage, etc. For instance, <a href="https://arxiv.org/abs/2307.08621">RetNet</a>'s training throughput is actually faster than a transformer with <a href="https://github.com/Dao-AILab/flash-attention">Flash-Attention</a>. Imagine how fast RetNet + Flash-Attention can be. For applications, if a model is 10x faster than ChatGPT, but just slightly underperforms it, it is very likely that I will choose that over ChatGPT.</p>
<h2 id="Additional-Notes">Additional Notes</h2>
<p>I am currently working on a linear attention model, but the field is changing so fast. I expect that this project will end within the next three months, because if not, my ideas will likely become obselete due to new works being released. Stay tuned.</p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>I know that this is much faster than humans, but we expect AI to be faster than humans, especially considering they cost so much power to run. <a href="#fnref1" class="footnote-backref">â†©ï¸</a></p>
</li>
<li id="fn2" class="footnote-item"><p>This perspective is not new and has been discussed in many papers. <a href="#fnref2" class="footnote-backref">â†©ï¸</a></p>
</li>
</ol>
</section>
]]></content>
      <categories>
        <category>Research</category>
      </categories>
      <tags>
        <tag>llm</tag>
        <tag>transformer</tag>
        <tag>nlp</tag>
        <tag>research</tag>
        <tag>long-context</tag>
        <tag>benchmark</tag>
        <tag>recurrence</tag>
        <tag>linear-attention</tag>
      </tags>
  </entry>
  <entry>
    <title>Some Binary Search</title>
    <url>/2023/09/14/some_binary_search/</url>
    <content><![CDATA[<p>A binary search with C++:</p>
<figure class="highlight c++"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> T&gt;</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">bin_search</span><span class="params">(vector&lt;T&gt;&amp; arr, T target)</span> </span>{</span><br><span class="line">    <span class="type">int</span> left = <span class="number">0</span>, right = arr.<span class="built_in">size</span>() - <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span> (left &lt;= right) {</span><br><span class="line">        <span class="type">int</span> mid = (left + right) / <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">if</span> (arr[mid] == target) {</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        } <span class="keyword">else</span> <span class="keyword">if</span> (arr[mid] &lt; target) {</span><br><span class="line">            left = mid + <span class="number">1</span>;</span><br><span class="line">        } <span class="keyword">else</span> {</span><br><span class="line">            right = mid - <span class="number">1</span>;</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">return</span> left;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>The same thing with Rust:</p>
<figure class="highlight rust"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">fn</span> <span class="title function_">bin_search</span>&lt;T: <span class="built_in">Ord</span>&gt;(arr: &amp;<span class="type">Vec</span>&lt;T&gt;, target: &amp;T) <span class="punctuation">-&gt;</span> <span class="type">usize</span> {</span><br><span class="line">    <span class="keyword">let</span> <span class="keyword">mut </span><span class="variable">left</span> = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">let</span> <span class="keyword">mut </span><span class="variable">right</span> = arr.<span class="title function_ invoke__">len</span>() - <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span> left &lt;= right {</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">mid</span> = (left + right) / <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">if</span> arr[mid] == *target {</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        } <span class="keyword">else</span> <span class="keyword">if</span> arr[mid] &lt; *target {</span><br><span class="line">            left = mid + <span class="number">1</span>;</span><br><span class="line">        } <span class="keyword">else</span> {</span><br><span class="line">            right = mid - <span class="number">1</span>;</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">    left</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>And with Python:</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">bin_search</span>(<span class="params">arr: <span class="built_in">list</span>, target</span>):</span><br><span class="line">    left = <span class="number">0</span></span><br><span class="line">    right = <span class="built_in">len</span>(arr) - <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> left &lt;= right:</span><br><span class="line">        mid = (left + right) // <span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> arr[mid] == target:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">elif</span> arr[mid] &lt; target:</span><br><span class="line">            left = mid + <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            right = mid - <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> left</span><br></pre></td></tr></tbody></table></figure>]]></content>
      <categories>
        <category>Test</category>
      </categories>
      <tags>
        <tag>english</tag>
        <tag>algorithm</tag>
        <tag>binary-search</tag>
        <tag>rust</tag>
        <tag>python</tag>
        <tag>c++</tag>
        <tag>test</tag>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title>Interpreting a Maze-Solving Network</title>
    <url>/2023/10/07/interpreting-a-maze-solving-network/</url>
    <content><![CDATA[<p><a href="https://www.lesswrong.com/s/sCGfFb5DPfjEmtEdn">The blog post</a></p>
<p>I can't believe I haven't read this until now. This is mind-provoking, and the result is an important step towards understanding neural networks.</p>
<span id="more"></span>
<p>The culmination of this blog post is the exciting work of <a href="/2023/10/07/actadd/">Activation Addition</a>, which I believe is one important work that inspired the recently <a href="https://arxiv.org/abs/2310.01405">Representation Engineering</a> work.</p>
]]></content>
      <categories>
        <category>Thoughts</category>
      </categories>
      <tags>
        <tag>llm</tag>
        <tag>english</tag>
        <tag>representation-engineering</tag>
        <tag>activation-engineering</tag>
        <tag>interpretability</tag>
        <tag>rl</tag>
        <tag>alignment</tag>
        <tag>maze</tag>
      </tags>
  </entry>
  <entry>
    <title>Safety and Ethical Concerns of Large Language Models</title>
    <url>/2023/09/19/llm-safety-and-ethics/</url>
    <content><![CDATA[<p>I will be holding a seminar at ModelBest (é¢å£æ™ºèƒ½) in Sep 20, 2023 in Beijing, Haidian, ç§‘æŠ€å›­. The seminar will be in Chinese, and it's called "å¤§æ¨¡å‹å®‰å…¨ä¸ä¼¦ç†é—®é¢˜" (translation: Safety and Ethical Concerns of Large Language Models). Below is a list of references.</p>
<span id="more"></span>
<h2 id="Introduction">Introduction</h2>
<ul>
<li>Galactica: A Large Language Model for Science</li>
<li><a href="https://openai.com/research/gpt-4">https://openai.com/research/gpt-4</a></li>
<li>SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions</li>
<li>Bias and Fairness in Large Language Models: A Survey</li>
<li>A Survey of Safety and Trustworthiness of Large Language Models through the Lens of Verification and Validation</li>
</ul>
<h2 id="Evaluation-Methods">Evaluation Methods</h2>
<ul>
<li>A General Language Assistant as a Laboratory for Alignment, Anthropic</li>
<li>Safety Assessment of Chinese Large Language Models</li>
<li>Semantics derived automatically from language corpora contain human-like biases</li>
<li>StereoSet: Measuring stereotypical bias in pretrained language models</li>
</ul>
<h3 id="Instruction-Attacks">Instruction Attacks</h3>
<ul>
<li>Toxicity in CHATGPT: Analyzing Persona-assigned Language Models â­ï¸</li>
<li>Large Language Models are Zero-Shot Reasoners â­ï¸</li>
<li>On Second Thought, Letâ€™s Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning â­ï¸</li>
<li>Prompting GPT-3 To Be Reliable</li>
<li>Universal and Transferable Adversarial Attacks on Aligned Language Models â­ï¸</li>
<li>Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment â­ï¸â­ï¸</li>
</ul>
<h3 id="Exaggerated-Safety">Exaggerated Safety</h3>
<ul>
<li>XSTEST: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models â­ï¸</li>
<li>Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions â­ï¸</li>
</ul>
<h2 id="Alignment-Methods">Alignment Methods</h2>
<ul>
<li>Aligning language models to follow instructions â­ï¸</li>
<li>Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback â­ï¸</li>
<li>SELF-INSTRUCT: Aligning Language Models with Self-Generated Instructions â­ï¸â­ï¸</li>
<li>Pretraining Language Models with Human Preferences â­ï¸</li>
<li>LIMA: Less Is More for Alignment</li>
<li><a href="https://openai.com/blog/our-approach-to-alignment-research">https://openai.com/blog/our-approach-to-alignment-research</a> (Aug 2022)</li>
<li><a href="https://openai.com/blog/our-approach-to-alignment-research">https://openai.com/blog/our-approach-to-alignment-research</a> (Jul 2023) â­ï¸</li>
</ul>
<p>â­ï¸: important</p>
<p>â­ï¸â­ï¸: very important</p>
<h2 id="My-Thoughts">My Thoughts</h2>
<p>AI alignment is extremely important, and we know very little about it right now. In my everyday use of ChatGPT, it occasionally refuses to help me. This is presumably because it thinks that assisting me is harmful, while it's actually not. This is a problem of "exaggerated safety", and it is very similar to the overgeneralization model editing, which is a problem I have worked on previous (see my publication, <a href="../../../../2023/09/14/EREN/">EREN</a>). I think using classifier on top (a simple safe guard), along with prompting methods<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup> and currect alignment methods is a viable solution (seem to work fairly well in ChatGPT), but as we can see from the <a href="https://www.anthropic.com/index/claude-2">technical report of Claude 2</a>, the helpfulness of the model significantly drops after alignment. Therefore, I think minimizing the sacrifice in helpfulness will be an important direction of future research.</p>
<p>Another concern is that there is no concensus on the goal of alignment. In fact, many people think that the fact that role-playing can be used to jailbreak alignment is not a bad thing per se, especially regarding toxicity, because if the user explicitly tells the AI to role-play a person that slurs a lot, the user expects slurs (one kind of toxicity).</p>
<p>Moreover, the entire meaning of alignment research might be undermined by the fact that AI system can be unaligned pretty easily. This concern is specially severe for works that focus on reducing the cost of alignment, because the same techniques might be used to effectively unalign AI systems.<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup></p>
<p>All in all, AI alignment is a sub-field of better controllability of AI system, and I can foresee that it will be a hot research topic for the upcoming five years.</p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>Basically prepending an prefix that tells it what is unethical and unsafe. <a href="#fnref1" class="footnote-backref">â†©ï¸</a></p>
</li>
<li id="fn2" class="footnote-item"><p>Does there exist a way to make AI impossible to unalign? This reminds me of the "mind stamping" (Chinese: æ€æƒ³é’¢å°) from the Three Body Problem, a novel by Liu Cixin. <a href="#fnref2" class="footnote-backref">â†©ï¸</a></p>
</li>
</ol>
</section>
]]></content>
      <categories>
        <category>Thoughts</category>
      </categories>
      <tags>
        <tag>llm</tag>
        <tag>research</tag>
        <tag>english</tag>
        <tag>machine-learning</tag>
        <tag>life</tag>
        <tag>ai-alignment</tag>
        <tag>eren</tag>
        <tag>ethics</tag>
        <tag>safety</tag>
        <tag>é¢å£æ™ºèƒ½</tag>
        <tag>modelbest</tag>
        <tag>tutorial</tag>
        <tag>chatgpt</tag>
        <tag>claude</tag>
        <tag>ä¸‰ä½“</tag>
      </tags>
  </entry>
  <entry>
    <title>ä¸´è¿‘2023æš‘å‡ï¼Œ00å¸ˆå§ç­”è¾©ï¼Œæ™šä¸Šæ‰“çƒ</title>
    <url>/2023/05/18/%E4%B8%B4%E8%BF%912023%E6%9A%91%E5%81%87/</url>
    <content><![CDATA[<p>ä»Šå¤©ç¡åˆ°ä¹ç‚¹æ‰é†’æ¥ï¼Œè¿˜æ˜¯è¢«00æ‰“ç”µè¯å«é†’çš„ã€‚å»è¿‡äº†ä¸ªæ—©ï¼Œç„¶åå»ä¸Šã€Šæ·±åº¦å­¦ä¹ ã€‹ã€‚</p>
<p>ä»Šå¤©00çš„å¸ˆå§ç­”è¾©ï¼Œä¸‹åˆä¸‰ç‚¹å»äº†ï¼Œå½“æ—¶æˆ‘åœ¨ç¡åˆè§‰ã€‚æ„Ÿè§‰ä¹‹åå¥¹æœ‰ç‚¹emoï¼Œä½†æ˜¯å¥¹ä¸æ‰¿è®¤ï¼Œä¸çŸ¥é“ä¸ºä»€ä¹ˆã€‚ç„¶åèŠäº†å¾ˆå¤šå…³äºæœªæ¥ï¼Œç»“å©šã€ç”Ÿå­©å­ã€æ‰¾å·¥ä½œç­‰äº‹æƒ…ã€‚æ„Ÿè§‰ä¹Ÿæ²¡æœ‰å¾ˆå¤§çš„é—®é¢˜ï¼Œä½†æ˜¯00æ€»æ˜¯æŠŠä¸œè¥¿çœ‹å¾—å¾ˆç°æš—ï¼Œå¾ˆç„¦è™‘ã€‚</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>00</tag>
        <tag>life</tag>
        <tag>ä¸­æ–‡</tag>
        <tag>school</tag>
        <tag>graduation</tag>
      </tags>
  </entry>
  <entry>
    <title>æ›´æ–°ä¸ªäººä¸»é¡µ</title>
    <url>/2023/09/16/%E6%9B%B4%E6%96%B0%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5/</url>
    <content><![CDATA[<p>ä¹‹å‰æœ‰è¿‡ä¸ªäººä¸»é¡µï¼Œä½†æ˜¯ä¸€ç›´æ²¡æœ‰å¼„å¥½ï¼Œæ›´æ²¡æœ‰æ›´æ–°ã€‚æœ€è¿‘æˆ‘å°†è‡ªå·±çš„ GitHub çš„ç”¨æˆ·åæ”¹äº†ï¼Œå¯¼è‡´ä¹‹å‰çš„ GitHub Pages å¤±æ•ˆäº†ï¼Œå°±è¶æœºé‡æ–°æ­å»ºä¸ªäººä¸»é¡µã€‚</p>
<p>å…œå…œè½¬è½¬ï¼Œè¿˜æ˜¯å†³å®šä½¿ç”¨ Hexoã€‚ä»¥å‰ç”¨è¿‡ Jekyllï¼Œè§‰å¾—è¿˜è¡Œï¼Œä½†æ˜¯çœŸçš„ä¸æƒ³ç”¨ Rubyï¼ŒHugo åˆå¤ªéº»çƒ¦ã€‚</p>
<span id="more"></span>
<p>é€‰äº†å¥½ä¹…ä¸»é¢˜ï¼ŒHexo å®£ä¼ è¯´æœ‰å¾ˆå¤šä¸»é¢˜ï¼Œä½†æ˜¯å®˜ç½‘ä¸Šä¸åˆ° 400 ä¸ªä¸»é¢˜ï¼Œè€Œä¸”å¤§éƒ¨åˆ†éƒ½ä¸ç¬¦åˆæˆ‘çš„å®¡ç¾æˆ–è€…è¦æ±‚ã€‚æˆ‘æƒ³è¦çš„é£æ ¼æ˜¯ç®€çº¦ï¼Œç°ä»£ï¼Œéœ€è¦åŒæ—¶æ”¯æŒé»‘æš—å’Œç™½äº®æ¨¡å¼ï¼Œéœ€è¦æœ‰ä»£ç é«˜äº®ä¸”æ˜¯ä»£ç æ˜¯ç­‰æ¬¾å­—ä½“ã€‚æœ€æ¥è¿‘æˆ‘çš„è¦æ±‚å°±æ˜¯<a href="https://www.github.com/xbmlz/hexo-theme-maple">Maple</a>ä¸»é¢˜ã€‚å¯æ˜¯ä»ç„¶æ— æ³•æ»¡è¶³æˆ‘çš„è¦æ±‚ï¼Œæ‰€ä»¥æˆ‘ä¿®æ”¹äº†ä¸€äº›æ ¼å¼ï¼ˆåŸç‰ˆç”šè‡³æœ‰ä¸€äº›é¢œè‰² bugï¼‰ï¼Œæ·»åŠ äº†è‡ªå·±çš„ä¸€äº›å†…å®¹ï¼Œç»“æœæ˜¯ä¸€ä¸ªå«åš<a href="https://www.github.com/chen-yingfa/hexo-theme-fengye">æ«å¶</a>çš„ä¸»é¢˜ã€‚</p>
<h2 id="æ—¥è®°">æ—¥è®°</h2>
<p>ä»Šå¤©æ—©ä¸Šä¸ƒç‚¹åŠèµ·æ¥ ğŸ›ï¼Œæ‰“ç”µè¯ ğŸ“± å«é†’00ï¼ˆç»ˆäºæœ‰ä¸€æ¬¡æ˜¯æˆ‘æ‰“ç”µè¯äº†å“ˆå“ˆå“ˆå“ˆï¼‰ï¼Œç„¶åå»æ ¸ç ”é™¢ä¿±ä¹éƒ¨åœ¨ç»¼ä½“æ‰“ç¾½æ¯›çƒ ğŸ¸ï¼Œåæ¥å‘ç°ä»–ä»¬å…¶å®çº¦äº†è¥¿ä½“ï¼Œä½†æ˜¯æˆ‘è·Ÿ00è‡ªå·±åœ¨è¹­ä¸€ä¸ªç©ºåœºå°±ä¸ç®¡äº†ï¼Œå…«ç‚¹åŠå·¦å³æœ‰äººæ¥äº†æˆ‘ä»¬å°±å»è¿‡æ—©ï¼Œç„¶åå»æˆ‘å®¿èˆ ğŸ¡ã€‚</p>
<p>ä¹‹åç‚¹äº†åº“è¿ªï¼Œç„¶åå»äº†å­¦æ ¡å—è¾¹çš„ä¸€ä¸ªè¶…å¸‚ï¼Œä¹°äº†ä¸€å¤§åŒ…è–¯ç‰‡å’Œä¸€ä¸ªæ¦´è²ï¼ç„¶åå°±åœ¨å®¿èˆæ²¡æœ‰åƒåˆé¥­ï¼Œç›´æ¥å¾…åˆ°æ™šé¥­ã€‚ä¸­åˆçš„æ—¶å€™è¿˜æ‹äº†è§†é¢‘ ğŸ“·ï¼Œä¸­é—´è¿˜å·®ç‚¹è¯´åˆ°00emoäº†ï¼Œå“ˆå“ˆå“ˆå“ˆã€‚</p>
<p>ä»Šå¤© 00 ä¸‹åˆå››ç‚¹å’Œæ™šä¸Šä¸ƒç‚¹éƒ½æœ‰ç›´æ’­è¯¾ ğŸ‘©ğŸ»â€ğŸ«ï¼Œéƒ½æ˜¯çœŸæ­£å¼€è¯¾ï¼Œä¸‹åˆçš„åœ¨æˆ‘å®¿èˆå¼€çš„ï¼Œå¥½åƒå¾ˆæˆåŠŸï¼Œè™½ç„¶æ‹–å ‚äº†ä¸€ç‚¹ç‚¹ã€‚æ™šä¸Šçš„åœ¨å¥¹è‡ªå·±å®¿èˆï¼Œè²Œä¼¼ä¹Ÿæ‹–å ‚äº†ï¼Œ00 è¯´æœ‰å¥½å¤šäººã€‚</p>
<p>æ™šä¸Šä¹ç‚¹å»æ‰“ç¾½æ¯›çƒäº† ğŸ¸ï¼Œå¸¦ä¸Šç›¸æœºå½•äº†æ‰“çƒçš„è§†é¢‘ï¼Œç„¶åå›å»æ´—æ¾¡ï¼Œæ™šä¸Šå»æ—å¤§åŒ—è·¯çš„å®¶ ğŸ¡ã€‚</p>
<h2 id="æœ€è¿‘">æœ€è¿‘</h2>
<p>æœ€è¿‘å¥½å¿™ï¼Œæ–°å­¦æœŸé©¬ä¸Šå°±è¦å¼€å§‹äº†ï¼Œè¿™é‡Œæ€»ç»“ä¸€ä¸‹æš‘å‡å¼€å§‹åˆ°æ­¤æ¯”è¾ƒé‡è¦çš„äº‹æƒ…å§ã€‚</p>
<p>è¿™ä¸ªæš‘å‡æ¬å‡ºæ ¡åˆæ¬å›æ¥äº†ï¼ŒæŠ˜è…¾äº†åˆè´¹é’± ğŸ’°ï¼Œå­¦æ ¡çœŸçš„å¥½æ¶å¿ƒï¼Œä¹‹å‰è¯´äº†å¤§æ¦‚ç‡æ˜¯ä¸ä¼šæœ‰å®¿èˆï¼Œç°åœ¨å°±æœ‰å¾ˆå¤šç©ºçš„æˆ¿é—´ã€‚</p>
<p>æœŸæœ«å‰è·Ÿå¯¼å¸ˆç¡®å®šäº†è¦è¯»åšäº†ï¼Œæˆ‘è·Ÿä»–æˆ‘æƒ³è¦ä¸‰å¹´æ¯•ä¸šï¼Œä»–è¯´æ²¡æœ‰é—®é¢˜ï¼Œå¸Œæœ›çœŸçš„æ˜¯å¯ä»¥å§ï¼Œæˆ‘ä»¬å®éªŒå®¤å¥½åƒåŸºæœ¬éƒ½æ˜¯ç›´åšç”Ÿï¼Œæ™®åšçš„åº”è¯¥éƒ½æ˜¯å››å¹´å§ã€‚00 ä¹Ÿç¡®å®šäº†ä¸ä¼šè¯»åšäº†ï¼Œæœ€è¿‘åœ¨æŠ•ç®€å†ï¼ŒOppo å¥½åƒå·²ç»æ‹¿åˆ°äº† offerï¼Œä½†æ˜¯ä»–ä»¬åŒ—äº¬æ²¡æœ‰éƒ¨é—¨ï¼Œæ‰€ä»¥ 00 ä¸æƒ³å»ï¼Œæˆ‘ä¹Ÿä¸æƒ³å¥¹å»ã€‚å¥½åƒäº’è”ç½‘ä»¥å¤–å¾ˆå¤šå…¬å¸éƒ½ä¸åœ¨åŒ—äº¬â€¦â€¦</p>
<p>æˆ‘çš„è®ºæ–‡ ğŸ“ƒ ERENï¼ˆä»¥å‰å«åš EmoRenï¼‰æŠ•å‡ºå»äº†ï¼Œä¸Šå‘¨ rebuttal ç»“æœå‡ºæ¥äº†ï¼Œä¸æ˜¯å¾ˆç†æƒ³ï¼Œæœ¬æ¥ soundess æ˜¯ 433ï¼ŒExcitement 323ï¼Œrebuttal ç»“æŸåç¬¬ä¸€ä¸ªå®¡ç¨¿äººå°† soundness è°ƒä½äº†ã€‚å­¦é•¿è¯´ä¸»ä¼šè®®ä¼°è®¡æ²¡æœ‰æœºä¼šäº†ï¼ŒFindings è¿˜æœ‰å¸Œæœ›ï¼Œæˆ‘å…¶å®æ— æ‰€è°“æ˜¯ä¸æ˜¯ Findingsï¼Œæ„Ÿè§‰å­¦é•¿åè€Œæœ‰ç‚¹ä»‹æ„ã€‚</p>
<p>è¢«å®éªŒå®¤çš„å­¦é•¿å­¦å§æ‹‰å»é¢å£æ™ºèƒ½<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>å»å¹²æ´»ï¼Œè·Ÿå…¬å¸çš„ä¸šåŠ¡æ²¡å•¥å…³ç³»ï¼Œå°±æ˜¯æŠŠæˆ‘çš„å·¥ä½æ¬äº†ï¼Œå¯èƒ½ä¸æƒ³å ç”¨éš”å£å®éªŒå®¤çš„ä½ç½®å§ ğŸ˜‚ ä½†æ˜¯æˆ‘çœŸçš„ä¸æƒ³å» ğŸ˜­ ä¸èƒ½è·Ÿ 00 å¾…åœ¨ä¸€èµ·äº†ã€‚æˆ‘ç°åœ¨å°±æ˜¯ä¸€å‘¨å¯èƒ½å»ä¸¤ä¸‰å¤© ğŸ˜‚</p>
<p>æœ€è¿‘è¿˜ç”³è¯·äº†ç­¾è¯ï¼Œå†³å®šäº†å¯’å‡ 00 è·Ÿæˆ‘ä¸€èµ·å›å®¶ï¼åœ¨å®¶å¾…ä¸€æ•´ä¸ªæœˆï¼Œå¥½ç¥å¥‡ï¼Œè§‰å¾—æˆ‘ä»¬çš„å…³ç³»å‘å±•å¾—å¥½é¡ºåˆ©ã€‚é©¬ä¸Šçš„å›½åº† ğŸ‡¨ğŸ‡³ æˆ‘ä¼šè·Ÿ 00 å›å»æ­¦æ±‰å’Œåº”åŸå‚åŠ å¥¹é«˜ä¸­åŒå­¦å’Œè¡¨å§çš„å©šç¤¼ ğŸ’‘ï¼Œé¡ºä¾¿è¿˜ä¼šçœ‹å¥¹çš„å¤–å…¬å¤–å©†ã€‚</p>
<p><img src="/%E6%9B%B4%E6%96%B0%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5/%E4%B8%AD%E5%9B%BD%E6%8A%A4%E7%85%A7.jpg" alt=""></p>
<blockquote>
<p>00 çš„æŠ¤ç…§ï¼Œæ˜¯æˆ‘å‘å¾€çš„èº«ä»½ï¼</p>
</blockquote>
<p><img src="/%E6%9B%B4%E6%96%B0%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5/%E7%94%B3%E8%AF%B7%E7%AD%BE%E8%AF%81.jpg" alt=""></p>
<blockquote>
<p>ç­¾è¯ä¸­å¿ƒé—¨å£ï¼Œä¸æ˜¯å¤§ä½¿é¦†ï¼Œå¾ˆå¤šä¸ªå›½å®¶ç»Ÿä¸€åŠç†ç­¾è¯çš„åœ°æ–¹ã€‚</p>
</blockquote>
<p><img src="/%E6%9B%B4%E6%96%B0%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5/%E5%90%83%E6%BE%B3%E9%97%A8%E8%8F%9C.jpg" alt=""></p>
<blockquote>
<p>åœ¨ç”³è¯·ç­¾è¯çš„åœ°æ–¹æ—è¾¹çš„ä¸€ä¸ªå¾ˆé«˜çº§çš„å•†åœºé‡Œé¢åƒæ¾³é—¨é¤ã€‚</p>
</blockquote>
<hr>
<h2 id="ä¸€äº›-Markdown-æ¸²æŸ“å™¨æµ‹è¯•">ä¸€äº› Markdown æ¸²æŸ“å™¨æµ‹è¯•</h2>
<p>æµ‹è¯•ä¸€ä¸‹å…¬å¼çš„æ¸²æŸ“ã€‚<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup></p>
<p>$$
\theta_i \leftarrow  \frac{\partial}{\partial  \theta_i} \mathcal L( y, f(x; \theta))
$$</p>
<p>å…¶ä¸­ $\mathcal L$ æ˜¯æŸå¤±å‡½æ•°ï¼Œ$f(\cdot; \theta)$ ç”± $\theta$ å‚æ•°åŒ–çš„æ¨¡å‹ã€‚</p>
<p>é‚£ <code>ä»£ç </code> å‘¢ï¼Ÿ<code>codell1i0oO</code> å¯ä»¥å—ï¼Ÿ</p>
<p>ä¸€ä¸ªè¡¨æ ¼ï¼š</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>0.1</td>
</tr>
<tr>
<td>B</td>
<td>0.2</td>
</tr>
<tr>
<td>C</td>
<td>0.3</td>
</tr>
</tbody>
</table>
<p>ä¸€ä¸ªåµŒå¥—åˆ—è¡¨ï¼š</p>
<ul>
<li>
<p>a</p>
<ul>
<li>
<p>x</p>
</li>
<li>
<p>asdf</p>
<ul>
<li>æ‡‚å¾—éƒ½æ‡‚</li>
</ul>
</li>
<li>
<p>å¥½å¥½</p>
<ol>
<li>
<p>item 1</p>
</li>
<li>
<p>item 2</p>
<ol>
<li>sdf</li>
<li>sdf</li>
</ol>
</li>
<li>
<p>item 3</p>
</li>
</ol>
</li>
</ul>
</li>
<li>
<p>asdf</p>
</li>
</ul>
<p>ä¸€ä¸ªä»£åŠåˆ—è¡¨ï¼š</p>
<ul class="contains-task-list">
<li class="task-list-item"><input class="task-list-item-checkbox" disabled="" type="checkbox"> ä¹°èœ</li>
<li class="task-list-item"><input class="task-list-item-checkbox" disabled="" type="checkbox"> åšé¥­</li>
<li class="task-list-item"><input class="task-list-item-checkbox" checked="" disabled="" type="checkbox"> è·‘æ­¥</li>
<li class="task-list-item"><input class="task-list-item-checkbox" checked="" disabled="" type="checkbox"> æ‰“ç¾½æ¯›çƒ</li>
<li class="task-list-item"><input class="task-list-item-checkbox" checked="" disabled="" type="checkbox"> å†™è®ºæ–‡</li>
<li class="task-list-item"><input class="task-list-item-checkbox" disabled="" type="checkbox"> æç§‘ç ”</li>
</ul>
<p>auto-links: <a href="http://www.hexo.io">www.hexo.io</a></p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>æˆ‘å¯¼å¸ˆå’ŒçŸ¥ä¹å­µåŒ–çš„çš„å…¬å¸ <a href="#fnref1" class="footnote-backref">â†©ï¸</a></p>
</li>
<li id="fn2" class="footnote-item"><p>æˆ‘ç°åœ¨ç”¨äº† <a href="https://github.com/hexojs/hexo-renderer-markdown-it">hexo-rendered-markdown-it</a> æ¥ä»£æ›¿åŸæ¥ Hexo çš„æ¸²æŸ“å™¨ã€‚åŸæ¥çš„æ¸²æŸ“å™¨æ˜¯ Marked (hexo-renderer-marked) <a href="#fnref2" class="footnote-backref">â†©ï¸</a></p>
</li>
</ol>
</section>
]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>research</tag>
        <tag>00</tag>
        <tag>life</tag>
        <tag>ä¸­æ–‡</tag>
        <tag>é¢å£æ™ºèƒ½</tag>
        <tag>blog</tag>
        <tag>ç­¾è¯</tag>
        <tag>hexo</tag>
        <tag>hugo</tag>
        <tag>jekyll</tag>
        <tag>static-site-generator</tag>
        <tag>ç¾½æ¯›çƒ</tag>
        <tag>work</tag>
        <tag>æ«å¶</tag>
        <tag>ä¸­å›½</tag>
        <tag>æŒªå¨</tag>
        <tag>markdown</tag>
        <tag>å®¿èˆ</tag>
      </tags>
  </entry>
  <entry>
    <title>ç¬¬ä¸€ä¸ªå¸–å­ï¼Œçå†™ç‚¹ä¸œè¥¿</title>
    <url>/2023/05/17/%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%B8%96%E5%AD%90%EF%BC%8C%E7%9E%8E%E5%86%99%E7%82%B9%E4%B8%9C%E8%A5%BF/</url>
    <content><![CDATA[<p>ç°åœ¨æ˜¯ 2023 å¹´äº”æœˆåä¸ƒï¼Œé©¬ä¸Šç¡•å£«ä¸€å¹´çº§å°±ç»“æŸï¼Œåœ¨æ¸…åå›­å·²ç»å¿«äº”å¹´äº†ï¼Œæ„Ÿè§‰å¯¹æˆ‘äººç”Ÿçš„å½±å“çœŸçš„å·¨å¤§ã€‚è¿™ä¸€å¹´è®¤è¯†äº†å¾ˆå¯çˆ±çš„ 00ï¼Œå¸Œæœ›å¯ä»¥ä¸€ç›´èµ°ä¸‹å»ã€‚</p>
<p>æˆ‘å’Œ 00 çš„å­©å­ä»¬ï¼š</p>
<span id="more"></span>
<ul>
<li>å§é¾™ï¼šè°ƒçš®çš„è‚¥çŒ« ğŸ±</li>
<li>å°ç»¿ï¼šå–œæ¬¢å’¬ä¸œè¥¿çš„é³„é±¼ ğŸŠ</li>
<li>éª†é›ï¼šè¶…çº§å¤§çš„åœŸé¸¡ï¼ğŸ°</li>
<li>å‡¤é›ï¼šä¸è°ƒçš®çš„çŒ«å’ª ğŸ±</li>
<li>é»„å¸ï¼šæ›´å¤§çš„å·¨å…” ğŸ°</li>
<li>å†…å­˜æ¡ï¼šç™½è‰²çš„ç†Šç†Š ğŸ»</li>
<li>é—ªå…‰ç¯ï¼šç°è‰²çš„ç†Šç†Š ğŸ»</li>
</ul>
<h2 id="ç°åœ¨è¦åšçš„äº‹æƒ…">ç°åœ¨è¦åšçš„äº‹æƒ…</h2>
<ul>
<li>
<p>æŠŠ EmoRen æŠ•äº†</p>
<ul>
<li>èƒ½ä¸èƒ½è¡Œå•Š</li>
</ul>
</li>
<li>
<p>è·‘ CFD çš„ä¸¹ç‚‰è°ƒå¥½</p>
<ul>
<li>å¥½éš¾å‘€</li>
</ul>
</li>
<li>
<p>å†™å®Œä½œä¸š</p>
<ul>
<li>NLPå’ŒDLçš„å¤§ä½œä¸šï¼</li>
</ul>
</li>
<li>
<p>æå®šå»ACLçš„æ‰‹ç»­</p>
<ul>
<li>å»åŠ æ‹¿å¤§ï¼Œç„¶åå›æŒªå¨ä¸€ä¸¤å‘¨ï¼Œç„¶åå›æ¥è·Ÿ 00 å»å—äº¬ï¼Œæˆ‘ä¸ç”¨ç­¾è¯ï¼Œä½†æ˜¯è¿˜æ˜¯æœ‰å¾ˆå¤šæ‰‹ç»­ã€‚</li>
</ul>
</li>
<li>
<p>å†™å¥½å¼€é¢˜æŠ¥å‘Š</p>
<ul>
<li>è¿˜ä¸çŸ¥é“åšå•¥å‘¢</li>
</ul>
</li>
</ul>
<h2 id="æˆ‘çš„å®¶ä¹¡-Lillesand">æˆ‘çš„å®¶ä¹¡ Lillesand</h2>
<p><img src="/2023/05/17/%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%B8%96%E5%AD%90%EF%BC%8C%E7%9E%8E%E5%86%99%E7%82%B9%E4%B8%9C%E8%A5%BF/lillesand0.jpg" alt=""></p>
<p><img src="/2023/05/17/%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%B8%96%E5%AD%90%EF%BC%8C%E7%9E%8E%E5%86%99%E7%82%B9%E4%B8%9C%E8%A5%BF/lillesand1.jpg" alt=""></p>
<p>å¥½ä¹…æ²¡æœ‰å›å»äº†ï¼Œä¸Šä¸€æ¬¡å›æŒªå¨ä¹Ÿæ²¡æœ‰å›å»</p>
]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>research</tag>
        <tag>cfd</tag>
        <tag>00</tag>
        <tag>life</tag>
        <tag>ä¸­æ–‡</tag>
        <tag>school</tag>
        <tag>å­©å­ä»¬</tag>
        <tag>å§é¾™</tag>
        <tag>å‡¤é›</tag>
        <tag>éª†é›</tag>
        <tag>é»„å¸</tag>
        <tag>å°ç»¿</tag>
        <tag>çŒ«å’ª</tag>
        <tag>åœŸé¸¡</tag>
        <tag>ç†Š</tag>
        <tag>ğŸ»</tag>
        <tag>ğŸ±</tag>
        <tag>ğŸ‡</tag>
        <tag>ğŸ°</tag>
        <tag>ğŸŠ</tag>
        <tag>emoren</tag>
        <tag>acl</tag>
        <tag>lillesand</tag>
        <tag>åŠ æ‹¿å¤§</tag>
      </tags>
  </entry>
  <entry>
    <title>ç¬¬ä¸€ç¯‡</title>
    <url>/2022/10/27/%E7%AC%AC%E4%B8%80%E7%AF%87/</url>
    <content><![CDATA[<p>ä¹‹å‰å°è¯•ç”¨ Hugo æ¥éƒ¨ç½²ï¼Œå‘ç° Hugo ä¸ä»…æŒºå¤æ‚ï¼Œè€Œä¸”è¿˜æœ‰å¾ˆå¤šå°é—®é¢˜ï¼Œå¯èƒ½è¿™å°±æ˜¯é€Ÿåº¦å¸¦æ¥çš„ä»£ä»·å§ã€‚ä½†æ˜¯å…¶å®æˆ‘ä¹Ÿä¸æ˜¯å†™å¾ˆå¤šå†…å®¹ï¼Œæ‰€ä»¥ Jekyll çš„é€Ÿåº¦åº”è¯¥æ˜¯å¤Ÿç”¨çš„ã€‚</p>
<p>Jekyll æ”¯æŒåœ¨ markdown å†…å®¹é‡Œé¢ç”¨ Liquid template tags æ¥ç”ŸæˆåŠ¨æ€å†…å®¹ï¼Œæ¯”å¦‚æ ¹æ® front matter ä¸­çš„ tagsï¼Œç»™æ¯ä¸ª tag ç”Ÿæˆ html divã€‚å¦‚ä¸‹ liquid è¯­æ³•ï¼š</p>

<figure class="highlight html"><table><tbody><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span>&gt;</span></span><br><span class="line">{% for tag in site.tags %}</span><br><span class="line">    <span class="tag">&lt;<span class="name">a</span> <span class="attr">style</span>=<span class="string">"background-color: blue;"</span>&gt;</span>#{{ tag[0] }}<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">{% endfor %}</span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></tbody></table></figure>

<span id="more"></span>
<p>ä¼šæ ¹æ® post markdown æ–‡ä»¶ä¸­çš„ front matter ä¸­å®šä¹‰çš„ tagsï¼š</p>
<figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line">---</span><br><span class="line"><span class="section">tags: some tags here</span></span><br><span class="line"><span class="section">---</span></span><br></pre></td></tr></tbody></table></figure>
<p>ç”Ÿæˆç›¸åº”çš„ html divï¼š</p>
<figure class="highlight html"><table><tbody><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"post-tags"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"/tags/life"</span> <span class="attr">class</span>=<span class="string">"tag-card"</span>&gt;</span>life<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"/tags/update"</span> <span class="attr">class</span>=<span class="string">"tag-card"</span>&gt;</span>update<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"/tags/learn"</span> <span class="attr">class</span>=<span class="string">"tag-card"</span>&gt;</span>learn<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"/tags/important"</span> <span class="attr">class</span>=<span class="string">"tag-card"</span>&gt;</span>important<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"/tags/jekyll"</span> <span class="attr">class</span>=<span class="string">"tag-card"</span>&gt;</span>jekyll<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"/tags/hugo"</span> <span class="attr">class</span>=<span class="string">"tag-card"</span>&gt;</span>hugo<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"/tags/static-site-generator"</span> <span class="attr">class</span>=<span class="string">"tag-card"</span>&gt;</span>static-site-generator<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></tbody></table></figure>
<hr>
<p>ä¸çŸ¥é“å†™å•¥ï¼Œå°±å†™ä¸€ä¸ª Python çš„äºŒåˆ†æœç´¢å§ï¼š</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">bin_search</span>(<span class="params">arr: <span class="built_in">list</span>, target: <span class="type">Any</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Return the smallest index such that when target is inserted at that index,</span></span><br><span class="line"><span class="string">    the array will remain sorted.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    lo, hi = <span class="number">0</span>, <span class="built_in">len</span>(arr)</span><br><span class="line">    <span class="keyword">while</span> lo &lt; hi:</span><br><span class="line">        m = (lo + hi) // <span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> arr[m] &lt; target:</span><br><span class="line">            lo = m + <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            hi = m</span><br><span class="line">    <span class="keyword">return</span> lo</span><br></pre></td></tr></tbody></table></figure>]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>life</tag>
        <tag>ä¸­æ–‡</tag>
        <tag>test</tag>
        <tag>hugo</tag>
        <tag>jekyll</tag>
        <tag>static-site-generator</tag>
        <tag>html</tag>
        <tag>liquid</tag>
      </tags>
  </entry>
</search>
