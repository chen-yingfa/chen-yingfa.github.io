<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>2024 国庆之后</title>
      <link href="/2024/10/10/2024%E5%9B%BD%E5%BA%86%E4%B9%8B%E5%90%8E/"/>
      <url>/2024/10/10/2024%E5%9B%BD%E5%BA%86%E4%B9%8B%E5%90%8E/</url>
      
        <content type="html"><![CDATA[<p>刚刚放完国庆假，从东莞回来了北京继续我的博士生涯。这几个月感觉事情特别多，虽然很充实，但也很累，刚好这个七天长假可以让我喘口气。很久没有写博客了，上一次关于我自己的博客内容好像就是去年国庆之后的。刚好过一年，也可以当作一个年度总结吧。</p><span id="more"></span><p>今年最主要的几件事情如下。</p><ul><li>从硕士变成了博士。</li><li>女朋友毕业后，去了东莞工作，并且跟她一起建设了一个小家，也把娃基本都带过去了（现在我宿舍只剩下大白和两个猪）。</li><li>女朋友跟我一起回挪威见我的家长了，也是她第一次出国。</li><li>家人来了中国参加我的毕业典礼，顺便和女朋友家人见了面，定了亲。</li><li>当上了实验室里的研究小组组长，参与了公司的运行，很有打工人的感觉。</li><li>认识了很多做科研的人，对科研的认知进步了超级多，也看了超级多论文，找到了自己喜欢的小领域，感觉得心应手，idea 也超级多。</li><li>当上了 NLP 课的助教，是一个很有意思的体验。</li></ul><h2 id="科研篇">科研篇</h2><p>去年暑假被一位学姐拉到导师公司坐着，因为环境好又有钱，然后后面就顺理成章跟她一起做了科研项目，进入了新的小组（刚好一直带我的学长也快要毕业了）。后面，好像是五月份左右，这边的小组组长因为要出去实习，让我当上了组长，感觉非常不一样，一开始压力还挺大的，也觉得自己能力不够，德不配位。但是其实还行，大家也都是为了做科研而已，就是多了很多跟别的组拉扯的情况。同时，来到这边之后找到了自己的新方向了：RNN 和长文本。特别喜欢这种，有点小众，同时还影响力挺大的研究方向，就是一开始看论文有点吃力，毕竟很多基础理论跟现在火热的 Transformer 有比较大的出入，研究难点也很不一样。但是这样才好，同行少一点，看论文的压力也少一点（顺便吐槽一下，现在论文真的太多了，每次放完假都觉得错过了无数篇论文！）。另外，这段时间也把楚简论文投了 ARR，评分不是很好最近就改投 COLING 了。同时也挂了 arXiv，但是这种工作感觉影响力就不是很大，虽然也是首个相关数据集，肯定能拿到一些引用的。感觉我数据集的工作还挺多的，哈哈哈哈哈。</p><p>前几天 10 月 2 日投了 ICLR，《Stuffed Mamba: State Collapse and State Capacity of RNN-Based Long-Context Modeling》，是我感觉比较满意的一个研究工作，做了也很久，感觉影响力应该会不错。然后今天凌晨两点把它放到 arXiv，争取一下靠前一点的位置。但是这篇工作只是开胃菜，是一个关于模型记忆能力的探索和一些崩溃现象的分析，后面还是得做实际的模型改动来提高模型性能，这才是我向往代表性工作，但是还是挺难的，虽然说 idea 很多，但是机器学习的研究就是一个反复试错的过程，大部分结果还是会跟猜想有很大的出入的。老师想要我训一个很强的 Mamba 版 MiniCPM，但是我觉得不做结构上的改动的这种训练没有什么科学贡献，个人还是希望做科学贡献，哈哈哈哈哈。</p><h2 id="生活篇">生活篇</h2><p>之前最后一年跟 00 在学校每天都会见面，玩耍。 国庆结束后没多久我们就 10 月 27 日到 31 日一起去了东莞参观公司，感觉环境很不错，就是东莞这个城市很破旧，人均素质也挺低。没办法。29 日去了深圳玩，见了已经工作了的于泽华和 00 的堂姐。11 月 17 日，跟 00 去了孝感市的安陆市参加她高中同学，金洁，的婚礼，好羡慕人家可以这么早结婚。但是习俗确实好麻烦……后面 00 找了个实习，是【比特大陆】，在丰台区，中关村壹号对面，离我们实验室相关公司的【启元实验室】挺接近的。有时候我也会去启元上班，然后就可以一起下班了。</p><p>11 月 26 日，我们一起玩耍了一天，先是在新的体育馆里溜冰了，然后去了四道口的【汤泉良子】泡温泉，这里感觉挺舒服干净的，还有自助榴莲吃！12 月 13 日，学校下大雪了，我们一起堆雪人了！其实就是滚了一个大雪球，然后开始往对方身上扔雪球。12 月 24 日，我们再泡温泉了。之后就到元旦了，一起在朝阳区的蓝色港湾跨年了，人超级多，差点没有找到地方吃饭。最后在一个西餐厅找到了位置，味道还行，就是有点贵。然后在星巴克里坐到了 12 点，一起在二楼露台倒数。下一天我们去了【比特大陆】参加元旦的活动，有很多小游戏可以玩，还一起弄了一个灯笼和「流体蚂蚁」（其实就是把一个蚂蚁磨具，就是比特大陆的吉祥物，染上各种染料）。1 月 5 日晚上我们一起做绿皮车去哈尔滨，感觉这次去的地方都挺好玩的。天气是真的冷。然后还买了新的娃，一只猪（其实是卡皮巴拉）！</p><p>后面春节，00 跟我一起回去挪威见我的家人，还带了一些礼物（茶叶和一个狐狸玩偶）在我家里住一个月，住在我姐姐的房间里，这也是她首次出国，申请签证还得去了一趟三里屯，但是审核倒是很快通过。在我家里其实就是每天宅在家里，我这边不像 00 家里，不需要跟各种亲戚朋友社交，所以就是每天在家里跟 00 分享我的一些小时候的东西，比如跟我一起打 PS3，小时候的食物。然后做了很多面包，感觉特别好吃！！而且跟她一起做饭就挺好玩的。还有吃了很多 00 没有吃过的东西，比如一些做法的牛排、鹿肉、鲨鱼肉、一些鱼、披萨、羊奶芝士、苹果派、热狗、某种带米粒的酸奶（Rislunsj）。后面还 roadtrip 去了我老家 Lillesand，看到了我老家的房子、我的小学中学。后面还看到了我的一个初中和高中的同学 Naomi，发现很多人都离开了小镇，但是她说有一些人会回来当老师。另外，我、00 跟我兄弟姐妹还一起拼了三维拼图，做了茶颜悦色的奶茶，感觉很温馨，很有家的感觉。我们还一起坐船去了哥本哈根，感觉这个城市比奥斯陆更加适合旅游，历史气息也更多（历史也确实更加悠长）。后面 2 月 21 日回来北京了。</p><p>4 月 9 日的时候，我们一起去了天津玩，坐高铁非常快，可是，我们是自驾去的！00 刚拿的驾照就上高速，哈哈哈哈哈，中间有几次确实很刺激。天津感觉可玩性不是很高，而且很多地方其实也挺破的，当然还是比东莞好太多了。4 月 27 日我们带着猫咪一起去了大连、烟台和长岛。提前放的五一假期，吃了很多海鲜。这边感觉也是挺破的，但是有些地方感觉很新奇，而且也看到了沙滩。后面是坐船去烟台的，船上感觉其实还行，是自己的房间，但是在烟台没呆多久就直接坐船去长岛了。长岛上酒店是在景点里面的，但是特别破，甚至感觉有点不是很正规，里面也有不少虫子。外面的景色不错，但是属实有点无聊，我们就是各种拍照。</p><p><img src="/2024/10/10/2024%E5%9B%BD%E5%BA%86%E4%B9%8B%E5%90%8E/%E5%A4%A7%E8%BF%9E%E7%8C%AB%E5%92%AA%E5%90%83%E9%A5%AD.jpg" alt="" title="刚到大连，带着猫咪在酒店旁边吃海鲜。"></p><p>5 月 10 日 00 毕设答辩，我在后面听着，感觉 00 被老师们的问题难到了，但是其实感觉不难回答的，就是老师们比较笨，理解不了这些新技术。最后当然还是顺利的。5 月 24 日 00 回家了一趟，因为在学校没有事情干，我 25 日硕士毕设答辩，非常顺利，我老师感觉也不怎么管。后面直到暑假的时间就没有什么特殊的了，我还是照常上班，00 就每天在玩耍，每隔一段时间就会一起出去逛街吃饭。感觉这段时间 00 因为没有事情干，又开始经常 emo 了，觉得我不理她。结果就是我们逛街频率增加了，但是其实相比于其他情侣好像也不算很频繁。6 月 22 日和 23 日，跟 00 一起拍了毕业照，第一天是我们俩穿汉服自己拍照，第二天是约了她的一个本科师弟，一个会收钱拍毕业照的人。</p><p>暑假的时候，我们家人都来了参加毕业典礼，我们分别跟各自的家人在北京旅游了一段时间，然后她跟我家人一起去了日本大阪、奈良和京都旅行。感觉还挺好玩的，00 特别喜欢奈良的鹿。然后 7 月 12 号跟 00 一起坐飞机去了东莞，办理入职，找房子。最后租下来长安花园的房子，有点破旧，但是有小区。入住后发现有蟑螂，特别大（有些能到 5cm），还会飞！我跟 00 都从来没有见过，被吓蒙了。后面就是成天杀虫。但是因为我很快就走了，只能尽快买下基础家具。00 也经常吐槽我不陪她，感觉很无奈。</p><p>后面跟家人旅游一段时间后，八月一号左右就回去跟 00 待一会儿。然后回去北京干活。后面中秋回去了一次，那时候跟 00 一起搬家到新的 Oppo 工业园。然后就是这次国庆假期回去了。新工业园的居住环境比长安花园好太多了，果然新的房子就是好。而且还有各种舒适的配套设施，路干净、宽敞、平。楼下有理发店、便利店。两百米外有食堂和运动场所，包含羽毛球、乒乓球、健身房、台球室等等。后面还有瑞幸和 KFC 来开张。</p><p><img src="/2024/10/10/2024%E5%9B%BD%E5%BA%86%E4%B9%8B%E5%90%8E/%E9%87%8D%E5%BA%86.jpg" alt="" title="跟我家人在观音桥商圈逛街。"></p><p><img src="/2024/10/10/2024%E5%9B%BD%E5%BA%86%E4%B9%8B%E5%90%8E/%E5%9B%BD%E5%BA%86%E8%BF%87%E5%AE%B6%E5%AE%B6.jpg" alt="" title="国庆跟 00 在公司宿舍过家家。"></p><p>生活也算是稳定下来了，突然觉得很有希望，而且感觉生活也挺美好的，钱也不少，如果不用担心买房根本花不完。我好好做科研后面收入也会很不错。希望将来的一年我能做出效率加倍，做出自己觉得有意义的科研，00 也可以继续快乐、阳光、向上！希望我们都活的精彩！</p>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> research </tag>
            
            <tag> 00 </tag>
            
            <tag> life </tag>
            
            <tag> 国庆 </tag>
            
            <tag> 放假 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>(EREN) Robust and Scalable Model Editing for Large Language Models</title>
      <link href="/2024/03/14/eren/"/>
      <url>/2024/03/14/eren/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www.github.com/chen-yingfa/eren">GitHub</a> | <a href="...">Paper (upcoming)</a></p><p><strong>TL;DR</strong>: A reader is augmented with a growing notebook that caches all edits in natural texts, and the reader retrieves relevant edits and make inference based on them. This achieves SOTA in model editing in QA and fact-checking.</p><span id="more"></span><hr><h2 id="Introduction">Introduction</h2><p><img src="/2024/03/14/eren/framework.png" alt=""></p><p>This work introduces a model editing method that addresses two issues with existing model editors:</p><ol><li>They cannot handle multiple sequential edits.</li><li>They cannot be applied to black-box models.</li></ol><blockquote><p>The reason we want to apply multiple sequential edits is that in practical applications, new knowledge appear in an on-line manner and the user might want to update the model's knowledge immediately.</p></blockquote><h2 id="Method">Method</h2><p>In summary, the edited LLM is complemented with a notebook that caches all edits in natural text. For each question, the model first determines whether the input is relevant to any edit, if so, it makes a prediction based on the notebook. Else, it directly answers the question using its memorized knowledge.</p><p>We find that LLMs, even when instruction-tuned, are not readily controllable by their context, i.e. the notebook. In particular, they are not robust to irrelevant context<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>, resulting in changed predictions on unrelated inputs. Also, the number of edits may to too large to fit into the input context of the LLM. Addressing these two issues, we propose to (1) split inference into two steps, and (2) use a dual-encoder retrieval framework to perform rough relevance estimation. Figure 1 (above) shows the overall framework of our method.<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup></p><h2 id="Result">Result</h2><p>From the following results on CounterFact (QA editing task) and FEVER (fact-checking editing task), we conclude that our LLM editor significantly outperforms existing methods.</p><h3 id="Metrics">Metrics</h3><ul><li>ES (Edit Success): the percentage of examples where the model correctly answers the question after the edit.</li><li>BP (Behavior Preservation): the percentage of unrelated examples whose output were not changed by the edit.</li><li>EQ (Edit Quality): the harmonic mean of ES and BP.<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup></li></ul><p>A perfect model editor should have an EQ of 1.</p><p><img src="/2024/03/14/eren/results.png" alt=""></p><p>The reason MEND and ROME performs so bad is that after many <strong>sequential</strong> edits, the model parameters are changed to much that the models start to output unintelligible tokens. In the case of fact-checking, randomly guessing "yes" or "no" should result in a 50% accuracy. However, since we use exact match to compute the accuracy, both MEND and ROME have much lower accuracy than random guess.</p><h3 id="Different-Number-of-Edits">Different Number of Edits</h3><p>For methods that modify the model parameters, each edit will have detrimental effects on the model's performance. Moreover, for methods like ROME that depend on precomputing layer statistic of the unedited model, that statistics will be gradually more inaccurate after every edit (recomputing that statistics is too costly). Therefore, after many sequential edits, the model will start to spit out gibberish, as evident in the Figure below.</p><p><img src="/2024/03/14/eren/diff-edit-cnt.png" alt=""></p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p>which has been concluded in several previous works <a href="#fnref1" class="footnote-backref">↩︎</a></p></li><li id="fn2" class="footnote-item"><p>For more details, please read the paper or contact me through e-mail. <a href="#fnref2" class="footnote-backref">↩︎</a></p></li><li id="fn3" class="footnote-item"><p>Using harmonic mean because we want the model editor to have both high ES and BP. A naive editor that ignores all edits can have a BP of 1, but ES of 0. <a href="#fnref3" class="footnote-backref">↩︎</a></p></li></ol></section>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> llm </tag>
            
            <tag> research </tag>
            
            <tag> ai </tag>
            
            <tag> paper </tag>
            
            <tag> english </tag>
            
            <tag> arxiv </tag>
            
            <tag> knowledge </tag>
            
            <tag> emnlp </tag>
            
            <tag> model editing </tag>
            
            <tag> in-context-learning </tag>
            
            <tag> serac </tag>
            
            <tag> rome </tag>
            
            <tag> eren </tag>
            
            <tag> mend </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>InfiniteBench: Extending Long Context Evaluation Beyond 100K Tokens</title>
      <link href="/2024/01/10/infinitebench/"/>
      <url>/2024/01/10/infinitebench/</url>
      
        <content type="html"><![CDATA[<p><a href="http://www.github.com/OpenBMB/InfiniteBench">Code</a> | <a href="https://arxiv.org/abs/2402.13718">Paper</a></p><p>The first benchmark for evaluating the effectiveness of LLMs in handling more than 100k tokens!</p><blockquote><p>In the paper, we name it $\infty$-Bench, but I will sometimes use "InfiniteBench" in this blog post for better readability.</p></blockquote><p>Finally got some time to write this blog, been so busy lately! I have been in a fairly long duration of research hiatus, meanwhile the field of NLP has been revolutionized by an overwhelming number of new LLMs. Finally, I was able to arrive at some productive and meaningful work in this new era of research, as a second author. In this blog post, I will introduce this work that I have been working on recently.</p><span id="more"></span><h2 id="Background">Background</h2><p>The advent of LLMs have shown many promising results, but many practice applications (e.g., agents, document/webpage reading, long text summarization, etc.) are greatly limited by the context length constraint. Therefore, many works have strived to increase the length of the context that LLMs can accept. However, current "long-sequence" benchmarks all fall below 100k tokens, and is therefore not able to evaluate the effectiveness of many long-context LLMs. Our work, $\infty$-Bench</p><h2 id="The-Data">The Data</h2><p>The data consists of language tasks from diverse domains (math, code, novels), two languages (English and Chinese). Half of the tasks are automatically generated, which is desirable for optionally further scaling the context lengths to any arbitrary lengths.</p><p>Following shows the statistics of the tasks in our benchmark.</p><p><img src="/2024/01/10/infinitebench/data-stat-pie.png" alt="" title="Data statistics. The angle of segments is proportional to the number of examples, and the radius is proportional to the average example lengths (sum of input and output tokens)."></p><h2 id="Results">Results</h2><p>We tested SOTA proprietary and open-source LLMs at the time of evaluation. The result is shown below. We can see that in most tasks, the performance is far from satisfactory in practical applications.</p><p><img src="/2024/01/10/infinitebench/results.png" alt="" title="Results of some SOTA long-context LLMs on our InfiniteBench"></p><h2 id="Thoughts-on-the-Future-of-Long-Context-Research">Thoughts on the Future of Long-Context Research</h2><p>Our lab have been investing much efforts in long-context LLMs lately. Particularly, we are interested in developing LLMs that can accept infinite input lengths, or what some of my colleagues call streaming language models (i.e., models that operate on streaming inputs). I believe that the transformer architecture is inherently incapable of processing infinite-length inputs. This is the research direction that I have been focusing on.</p><h3 id="Inherent-Limitations-of-Transformers">Inherent Limitations of Transformers</h3><p>The most obvious reason is the quadratic complexity. A large number of research papers have focused on reducing the computational cost of the self-attention mechanism. But most SOTA LLMs at the moment are still dense attention layers, and rely on using Flash-Attention to speedup the computation. However, through discussion with various researchers, I have found that many people now believe that the attention computation is fast enough for most practical applications. I strongly disagree with this view. Firstly, the computational cost translates to the operational cost and emission that results from the usage of LLMs, which is of great concern. Secondly, my experience with ChatGPT (especially when using GPT-4) is that it is often not fast enough. Especially when it tends to produce many irrelevant lead-up sentences, I often find that I can find the answer using search engines before ChatGPT gives me the answer. Thirdly, current LLMs typically only have less than 100k in context length, however, as we apply them on contexts with millions of tokens, the speed of processing these tokens becomes unacceptable in most applications. For instance, in our experiments with InfiniteBench, applying a 7B model on 128k tokens using one A100 GPU takes 8~11 minutes to simply read the input<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>.</p><blockquote><p>This is not the only drawback of the transformer architecture, but that is out of the scope of this discussion.</p></blockquote><h3 id="Possible-Paths">Possible Paths</h3><p>I do not believe making small tweaks to the self-attention mechanism will solve the problem. Yep, we need new model architectures. Two architectures that I find promising are <strong>linear attention</strong> and <strong>state-space models</strong> (SSMs). Since these architectures have been widely discussed in the research community, I will not describe them in detail here. Instead, I want to express my opinion on the future of these architectures.</p><p>I like to think of different language model architectures from the perspective of compressing the history<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup>. The way transformers work is that they feed the last $L$ tokens without any compression to the model in each step. This means that the model remembers everything perfectly up to $L$ previous tokens, and remembers <strong>nothing</strong> about the history before that.</p><p>In contrast, SSM and models with linear attention can learn to automatically choose what information retain about the past, which more closely resembles how humans memorize, and provides a smoother curve of forgettance (which is likely beneficial because I think that a blurry remembrance is much better than complete forgettal beyond $n$ tokens). I firmly believe that this is the right direction to go. We are very likely to see a surge of LLMs with $O(1)$ inference cost (for one token) in the upcoming five years, and this can drastically reduce the computational costs and increase their applicability in real-world applications.</p><p>Some people may want to refute by saying "but recurrent models are much weaker than transformers". The thing is, most of such comparison are done in settings where the input does not exceed the context window of the transformer models. In other words, we only evaluate transformers on cases where it has perfect memory. In fact, I believe that within the context windows of a transformer model, it should be the upper bounds for the performance of recurrent models, which holds a lossful compression of the window. Moreover, I think that further research down the line can drastically improve the performance of recurrent models (actually any possible linear language models) over self-attention-based language models.</p><p>Another thing to note is that, I have noticed that people like to align the parameter count of different LLMs during comparison, but for models with different architecture, this is a bad practice. In practice, we likely care more about the cost of maintenance, the speed of inference or training, and memory usage, etc. For instance, <a href="https://arxiv.org/abs/2307.08621">RetNet</a>'s training throughput is actually faster than a transformer with <a href="https://github.com/Dao-AILab/flash-attention">Flash-Attention</a>. Imagine how fast RetNet + Flash-Attention can be. For applications, if a model is 10x faster than ChatGPT, but just slightly underperforms it, it is very likely that I will choose that over ChatGPT.</p><h2 id="Additional-Notes">Additional Notes</h2><p>I am currently working on a linear attention model, but the field is changing so fast. I expect that this project will end within the next three months, because if not, my ideas will likely become obselete due to new works being released. Stay tuned.</p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p>I know that this is much faster than humans, but we expect AI to be faster than humans, especially considering they cost so much power to run. <a href="#fnref1" class="footnote-backref">↩︎</a></p></li><li id="fn2" class="footnote-item"><p>This perspective is not new and has been discussed in many papers. <a href="#fnref2" class="footnote-backref">↩︎</a></p></li></ol></section>]]></content>
      
      
      <categories>
          
          <category> Research </category>
          
      </categories>
      
      
        <tags>
            
            <tag> llm </tag>
            
            <tag> transformer </tag>
            
            <tag> nlp </tag>
            
            <tag> research </tag>
            
            <tag> long-context </tag>
            
            <tag> benchmark </tag>
            
            <tag> recurrence </tag>
            
            <tag> linear-attention </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Interpreting a Maze-Solving Network</title>
      <link href="/2023/10/07/interpreting-a-maze-solving-network/"/>
      <url>/2023/10/07/interpreting-a-maze-solving-network/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www.lesswrong.com/s/sCGfFb5DPfjEmtEdn">The blog post</a></p><p>I can't believe I haven't read this until now. This is mind-provoking, and the result is an important step towards understanding neural networks.</p><span id="more"></span><p>The culmination of this blog post is the exciting work of <a href="/2023/10/07/actadd/">Activation Addition</a>, which I believe is one important work that inspired the recently <a href="https://arxiv.org/abs/2310.01405">Representation Engineering</a> work.</p>]]></content>
      
      
      <categories>
          
          <category> Thoughts </category>
          
      </categories>
      
      
        <tags>
            
            <tag> llm </tag>
            
            <tag> english </tag>
            
            <tag> representation-engineering </tag>
            
            <tag> activation-engineering </tag>
            
            <tag> interpretability </tag>
            
            <tag> rl </tag>
            
            <tag> alignment </tag>
            
            <tag> maze </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Activation Addition (ActAdd)</title>
      <link href="/2023/10/07/actadd/"/>
      <url>/2023/10/07/actadd/</url>
      
        <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/2308.10248">Paper</a></p><p>TLDR: Propose <strong>ActAdd</strong>, a method for controlling model behavior during inference by modifying activations with a bias term that is learned from a pair of prompt.</p><p>Summary:</p><ul><li>Propose <strong>ActAdd</strong>, a method for controlling model behavior by modifying activations at inference time.</li><li>Steering vectors are computed by taking the activation differences that result from pairs of prompts. The vectors are added as bias during inference.</li><li>ActAdd provides control over high-level properties of the output, and preserves off-target model performance, and requires little computational and implementational costs.</li></ul><span id="more"></span><blockquote><p>The recently popular <a href="https://arxiv.org/abs/2310.01405">representation engineering paper</a> (RepE) seems to be largely inspired by this work.</p></blockquote><h2 id="Background">Background</h2><p>The authors propose to compute steering vectors to steer the model's behavior. They call such methods <em>activation engineering</em>. They make the following contributions:</p><ul><li>Find that combining forward passes works well in GPT-2, despite it was not trained for this.</li><li>The proposed method, <strong>ActAdd</strong>, is efficient, requiring no gradient descent or labeled data.</li></ul><p>The difference between ActAdd and existing steering vector methods is that they find the vectors via one of the following.</p><ul><li>Differences after fine-tuning</li><li>Per-query gradient-based search</li><li>Linear probes + differences in truthy attention heads</li></ul><p>In contrast, ActAdd uses the difference between prompt pairs instead.</p><h2 id="Method">Method</h2><p>The method is really, really simple. Simply manually contruct a pair of prompts, and compute the difference between the activations. Then, add the difference as a bias term to the activations during inference. The algorithm is as follows.</p><p><img src="/2023/10/07/actadd/alg.png" alt="" title="The algorithm of ActAdd"></p><p>As shown, this method has two hyperparameters, the amount of drift $c$, and the modified layer $l$, and requires two manually constructed prompts $(p_+, p_-)$. How to more effectively construct these prompts is not discussed in this paper.</p><h2 id="Result">Result</h2><p><img src="/2023/10/07/actadd/result.png" alt="" title="Main results."></p><h2 id="My-Thoughts">My Thoughts</h2><p>The effectiveness of this method is a strong evidence that input and output features are represented as linear directions in representational space, but we still have no explanation for why such linearity arises naturally in LLMs. The fact that this actually works is very thought-provoking. However, ActAdd start to see degraded performance on off-target inputs when we drive the activations to far off, and the steering sometimes simply fails, this may indicate that the optimal steering path is not linear, which I believe is reasonable. This is somewhat similar to neuron attribution methods, many features/skills/knowledge cannot be attributed to single neurons (they are distributed across neurons), but existing neuron attribution methods still work well because, by change, some features are primarily determined by the activity or state of a single neuron (or a small set of neurons).</p><p>We can also draw a parallel with <a href="https://arxiv.org/abs/2106.10199">BitFit</a>, which shows that tuning only (a subset of) the bias terms and the task-specific classifier head in a transformer model can achieve tuning performance comparable to full parameter finetuning. BitFit did only experiments on <strong>encoder models</strong>, in which case bias terms can be seen as steering vectors in the hidden representation space, therefore, ActAdd and BitFit differs only in the training signel. ActAdd uses the difference between the representation of a pair of (positive and negative) prompts, while BitFit propagates the human annotation from the classification head. For <strong>autoregressive models</strong>, ActAdd is more expressive because each token can be steered independently, while BitFit can only steer the whole sequence.</p><p>Interestingly, the fact that difference (or other arithmetics) in hidden representation are useful signal for some semantics has been shown in the era of learning word vectors, the fact that this can be used as a training signal is pretty neat.</p><p>The author also discussed the difference between activation engineering and adaptation methods, but the empirical results were not enough to show that ActAdd can be used as a substitute for adaptation. E.g., we cannot practically adapt the model into performing machine translation with ActAdd, because there is no negative prompt for translation. But perhaps upcoming works can apply this method to replace fine-tuning (or other adaptation methods). The realization may be a promising way to effiicently control any arbitrary model behavior without backpropagation<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>.</p><p>Nevertheless, this method is extremely interesting, and I feel like many existing methods can be improved by taking inspirations from this work.</p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p><a href="https://arxiv.org/abs/2305.17333">MeZO</a> is one alternative, but the training time of MeZO is almost the same as fine-tuning. <a href="#fnref1" class="footnote-backref">↩︎</a></p></li></ol></section>]]></content>
      
      
      <categories>
          
          <category> Paper Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> llm </tag>
            
            <tag> english </tag>
            
            <tag> ai-alignment </tag>
            
            <tag> gpt </tag>
            
            <tag> activation-modification </tag>
            
            <tag> adaptation </tag>
            
            <tag> model-editing </tag>
            
            <tag> representation-engineering </tag>
            
            <tag> fine-tuning </tag>
            
            <tag> parameter-efficient-tuning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2023年中秋和国庆</title>
      <link href="/2023/10/05/2023%E4%B8%AD%E7%A7%8B/"/>
      <url>/2023/10/05/2023%E4%B8%AD%E7%A7%8B/</url>
      
        <content type="html"><![CDATA[<p>今年国庆 🇨🇳 和中秋 🥮 一起放假，我跟 00 一起回来应城参加她堂姐和初中同学的婚礼<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>，住在她家里十个夜晚<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup>。第二次见家长，也算是挺顺利，但是每天都会见到陌生人，有点累，庆幸的是，感觉到 00 能接受跟我家人生活在一起。一号到三号我们去武汉玩了三天，超级开心，跟她在一起连逛商场都是开心的！</p><h2 id="小县城的氛围">小县城的氛围</h2><p>应城跟我想象中的小县城很像，也是很多远房亲戚，习俗也让人很烦。敬酒、随地扔垃圾、室内抽烟、八卦人家的私事、说话粗鄙、脏、说了不要还非要给人家……而且确实能明显感觉到，这里的人的素质的平均水平挺低的，尤其是上一辈。真的很讨厌吃席，00 也是，这些习俗的麻烦程度让 00 都不想结婚了……</p><span id="more"></span><p>但是无所谓了，之后能跟 00 在一起就好，除了回来过节应该也很少机会有联系。</p><h2 id="武汉">武汉</h2><p>一号到三号去了武汉旅游。早上五点多跟 00 的 ”二妈“（其实是婶，叔叔的老婆）坐车去武汉，坐了一个小时。他们这么早是因为要去谈婚礼的事情，然后害怕堵车。我们在酒店旁边下来，那时候“二妈”下地铁站上厕所，然后 00 非要给她买包子（为了礼貌），然后她最后还是拒绝了，导致我们得自己吃下包子。虽然包子没有不好吃，但是我就很讨厌这种明知人家不要还非要买的行为。</p><p>之后我们去酒店的时候，还没有房子，我们寄存了行李就直接去新天地买了杯霸王茶姬的奶茶，然后去了古德寺。网上说不可以穿着暴露，但是感觉路人穿着还是很暴露。</p><p><img src="/2023/10/05/2023%E4%B8%AD%E7%A7%8B/%E6%96%B0%E5%A4%A9%E5%9C%B0-%E9%9C%B8%E7%8E%8B%E8%8C%B6%E5%A7%AC.png" alt="" title="在武汉新天地买霸王茶姬。"></p><p>之后还去了解放公园和中山公园，都挺不错的。大城市就是好。里面看到了很好看的建筑物。在中山公园我们问了两个小孩借用羽毛球拍子来打了几下。之后在一个相亲角<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup>旁边跟她的高中同学，彭双，会合，然后逛了一下相亲角。之后我们还坐了一下过山车（公园里面有过山车还是第一次见）。</p><p><img src="/2023/10/05/2023%E4%B8%AD%E7%A7%8B/%E8%A7%A3%E6%94%BE%E5%85%AC%E5%9B%AD%E4%B8%AD%E9%97%B4.png" alt="" title="解放公园中间的一个很多塔的地方。"></p><p>晚上就去跟她的高中同学一起吃饭。</p><p>第二天我们先在地铁站剪了头发，然后去宝通寺，晚上去武商梦时代。这个商场规格超级高，还挺好玩的。第一次看到索尼专卖店，还有 Pico 专卖店。里面还有滑雪的地方，但是太贵的。我们还去了优衣库，买了一些衣服，发现还挺便宜的。以前都会觉得逛街购物很无聊，但是跟她在一起连连逛街买衣服都是开心的。</p><p>晚上我们跟她“大哥”（其实是堂哥）和他老婆一起吃饭，吃了魔宗烤肉，然后喝了茶颜悦色。总体来说也挺顺利的，感觉他们也不难相处。</p><p><img src="/2023/10/05/2023%E4%B8%AD%E7%A7%8B/%E6%AD%A6%E5%95%86%E6%A2%A6%E6%97%B6%E4%BB%A3.png" alt="" title="武商梦时代里面的美食街买鲜虾汤包"></p><p>第三天我们去了欢乐谷！是我们第一次一起去游乐场！玩了一个过山车，然后做了太阳飞车，00 就头晕想吐了，果然还是不行……但是没事，还是挺开心的。排队过程中还遇到了插队的人，好恶心！</p><p>晚上我们跟一些人（共七个人）一起拼车回来应城，居然比火车还便宜，不错。回来已经11点了，然后回家放下行李箱之后又出去找她初中同学一起吃宵夜。</p><p><img src="/2023/10/05/2023%E4%B8%AD%E7%A7%8B/%E6%AD%A6%E6%B1%89%E6%AC%A2%E4%B9%90%E8%B0%B7.png" alt="" title="在武汉欢乐谷玩耍。"></p><h2 id="公事">公事</h2><p>这个假期有点长，感觉有很多活都没有干。每天都很多事情，感觉这里的人太闲了，应该让他们多上班哈哈哈。古文字翻译的工作还没有干完，目前感觉效果不是很好，我也不想干这个了，感觉很浪费我的时间……至于对齐神经元，貌似现有方法都无法用在自回归模型上面，但是对齐问题好像之后自回归模型才会出现。不知道是不是我没有找到，目前还没有找到一篇研究神经元对生成结果的影响的工作。<a href="https://www.github.com/kmeng01/rome">ROME</a> 的 Causal Tracing 感觉可以用，这两天得赶紧做点东西出来。</p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p>27号是初中同学（魏陈）的婚礼，5号是堂姐（骆卓颖）的婚礼。 <a href="#fnref1" class="footnote-backref">↩︎</a></p></li><li id="fn2" class="footnote-item"><p>九月二十六日回来，十月七日走。坐高铁到北京，然后做火车到应城。 <a href="#fnref2" class="footnote-backref">↩︎</a></p></li><li id="fn3" class="footnote-item"><p>之前在上海都没找到。 <a href="#fnref3" class="footnote-backref">↩︎</a></p></li></ol></section>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 00 </tag>
            
            <tag> life </tag>
            
            <tag> 中秋 </tag>
            
            <tag> 中文 </tag>
            
            <tag> wedding </tag>
            
            <tag> 中秋-middle-autumn </tag>
            
            <tag> 国庆-national-day </tag>
            
            <tag> 应城 </tag>
            
            <tag> 武汉-wuhan </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Safety and Ethical Concerns of Large Language Models</title>
      <link href="/2023/09/19/llm-safety-and-ethics/"/>
      <url>/2023/09/19/llm-safety-and-ethics/</url>
      
        <content type="html"><![CDATA[<p>I will be holding a seminar at ModelBest (面壁智能) in Sep 20, 2023 in Beijing, Haidian, 科技园. The seminar will be in Chinese, and it's called "大模型安全与伦理问题" (translation: Safety and Ethical Concerns of Large Language Models). Below is a list of references.</p><span id="more"></span><h2 id="Introduction">Introduction</h2><ul><li>Galactica: A Large Language Model for Science</li><li><a href="https://openai.com/research/gpt-4">https://openai.com/research/gpt-4</a></li><li>SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions</li><li>Bias and Fairness in Large Language Models: A Survey</li><li>A Survey of Safety and Trustworthiness of Large Language Models through the Lens of Verification and Validation</li></ul><h2 id="Evaluation-Methods">Evaluation Methods</h2><ul><li>A General Language Assistant as a Laboratory for Alignment, Anthropic</li><li>Safety Assessment of Chinese Large Language Models</li><li>Semantics derived automatically from language corpora contain human-like biases</li><li>StereoSet: Measuring stereotypical bias in pretrained language models</li></ul><h3 id="Instruction-Attacks">Instruction Attacks</h3><ul><li>Toxicity in CHATGPT: Analyzing Persona-assigned Language Models ⭐️</li><li>Large Language Models are Zero-Shot Reasoners ⭐️</li><li>On Second Thought, Let’s Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning ⭐️</li><li>Prompting GPT-3 To Be Reliable</li><li>Universal and Transferable Adversarial Attacks on Aligned Language Models ⭐️</li><li>Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment ⭐️⭐️</li></ul><h3 id="Exaggerated-Safety">Exaggerated Safety</h3><ul><li>XSTEST: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models ⭐️</li><li>Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions ⭐️</li></ul><h2 id="Alignment-Methods">Alignment Methods</h2><ul><li>Aligning language models to follow instructions ⭐️</li><li>Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback ⭐️</li><li>SELF-INSTRUCT: Aligning Language Models with Self-Generated Instructions ⭐️⭐️</li><li>Pretraining Language Models with Human Preferences ⭐️</li><li>LIMA: Less Is More for Alignment</li><li><a href="https://openai.com/blog/our-approach-to-alignment-research">https://openai.com/blog/our-approach-to-alignment-research</a> (Aug 2022)</li><li><a href="https://openai.com/blog/our-approach-to-alignment-research">https://openai.com/blog/our-approach-to-alignment-research</a> (Jul 2023) ⭐️</li></ul><p>⭐️: important</p><p>⭐️⭐️: very important</p><h2 id="My-Thoughts">My Thoughts</h2><p>AI alignment is extremely important, and we know very little about it right now. In my everyday use of ChatGPT, it occasionally refuses to help me. This is presumably because it thinks that assisting me is harmful, while it's actually not. This is a problem of "exaggerated safety", and it is very similar to the overgeneralization model editing, which is a problem I have worked on previous (see my publication, <a href="../../../../2023/09/14/EREN/">EREN</a>). I think using classifier on top (a simple safe guard), along with prompting methods<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup> and currect alignment methods is a viable solution (seem to work fairly well in ChatGPT), but as we can see from the <a href="https://www.anthropic.com/index/claude-2">technical report of Claude 2</a>, the helpfulness of the model significantly drops after alignment. Therefore, I think minimizing the sacrifice in helpfulness will be an important direction of future research.</p><p>Another concern is that there is no concensus on the goal of alignment. In fact, many people think that the fact that role-playing can be used to jailbreak alignment is not a bad thing per se, especially regarding toxicity, because if the user explicitly tells the AI to role-play a person that slurs a lot, the user expects slurs (one kind of toxicity).</p><p>Moreover, the entire meaning of alignment research might be undermined by the fact that AI system can be unaligned pretty easily. This concern is specially severe for works that focus on reducing the cost of alignment, because the same techniques might be used to effectively unalign AI systems.<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup></p><p>All in all, AI alignment is a sub-field of better controllability of AI system, and I can foresee that it will be a hot research topic for the upcoming five years.</p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p>Basically prepending an prefix that tells it what is unethical and unsafe. <a href="#fnref1" class="footnote-backref">↩︎</a></p></li><li id="fn2" class="footnote-item"><p>Does there exist a way to make AI impossible to unalign? This reminds me of the "mind stamping" (Chinese: 思想钢印) from the Three Body Problem, a novel by Liu Cixin. <a href="#fnref2" class="footnote-backref">↩︎</a></p></li></ol></section>]]></content>
      
      
      <categories>
          
          <category> Thoughts </category>
          
      </categories>
      
      
        <tags>
            
            <tag> llm </tag>
            
            <tag> research </tag>
            
            <tag> english </tag>
            
            <tag> machine-learning </tag>
            
            <tag> life </tag>
            
            <tag> ai-alignment </tag>
            
            <tag> eren </tag>
            
            <tag> ethics </tag>
            
            <tag> safety </tag>
            
            <tag> 面壁智能 </tag>
            
            <tag> modelbest </tag>
            
            <tag> tutorial </tag>
            
            <tag> chatgpt </tag>
            
            <tag> claude </tag>
            
            <tag> 三体 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>更新个人主页</title>
      <link href="/2023/09/16/%E6%9B%B4%E6%96%B0%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5/"/>
      <url>/2023/09/16/%E6%9B%B4%E6%96%B0%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5/</url>
      
        <content type="html"><![CDATA[<p>之前有过个人主页，但是一直没有弄好，更没有更新。最近我将自己的 GitHub 的用户名改了，导致之前的 GitHub Pages 失效了，就趁机重新搭建个人主页。</p><p>兜兜转转，还是决定使用 Hexo。以前用过 Jekyll，觉得还行，但是真的不想用 Ruby，Hugo 又太麻烦。</p><span id="more"></span><p>选了好久主题，Hexo 宣传说有很多主题，但是官网上不到 400 个主题，而且大部分都不符合我的审美或者要求。我想要的风格是简约，现代，需要同时支持黑暗和白亮模式，需要有代码高亮且是代码是等款字体。最接近我的要求就是<a href="https://www.github.com/xbmlz/hexo-theme-maple">Maple</a>主题。可是仍然无法满足我的要求，所以我修改了一些格式（原版甚至有一些颜色 bug），添加了自己的一些内容，结果是一个叫做<a href="https://www.github.com/chen-yingfa/hexo-theme-fengye">枫叶</a>的主题。</p><h2 id="日记">日记</h2><p>今天早上七点半起来 🛏，打电话 📱 叫醒00（终于有一次是我打电话了哈哈哈哈），然后去核研院俱乐部在综体打羽毛球 🏸，后来发现他们其实约了西体，但是我跟00自己在蹭一个空场就不管了，八点半左右有人来了我们就去过早，然后去我宿舍 🏡。</p><p>之后点了库迪，然后去了学校南边的一个超市，买了一大包薯片和一个榴莲！然后就在宿舍没有吃午饭，直接待到晚饭。中午的时候还拍了视频 📷，中间还差点说到00emo了，哈哈哈哈。</p><p>今天 00 下午四点和晚上七点都有直播课 👩🏻‍🏫，都是真正开课，下午的在我宿舍开的，好像很成功，虽然拖堂了一点点。晚上的在她自己宿舍，貌似也拖堂了，00 说有好多人。</p><p>晚上九点去打羽毛球了 🏸，带上相机录了打球的视频，然后回去洗澡，晚上去林大北路的家 🏡。</p><h2 id="最近">最近</h2><p>最近好忙，新学期马上就要开始了，这里总结一下暑假开始到此比较重要的事情吧。</p><p>这个暑假搬出校又搬回来了，折腾了又费钱 💰，学校真的好恶心，之前说了大概率是不会有宿舍，现在就有很多空的房间。</p><p>期末前跟导师确定了要读博了，我跟他我想要三年毕业，他说没有问题，希望真的是可以吧，我们实验室好像基本都是直博生，普博的应该都是四年吧。00 也确定了不会读博了，最近在投简历，Oppo 好像已经拿到了 offer，但是他们北京没有部门，所以 00 不想去，我也不想她去。好像互联网以外很多公司都不在北京……</p><p>我的论文 📃 EREN（以前叫做 EmoRen）投出去了，上周 rebuttal 结果出来了，不是很理想，本来 soundess 是 433，Excitement 323，rebuttal 结束后第一个审稿人将 soundness 调低了。学长说主会议估计没有机会了，Findings 还有希望，我其实无所谓是不是 Findings，感觉学长反而有点介意。</p><p>被实验室的学长学姐拉去面壁智能<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>去干活，跟公司的业务没啥关系，就是把我的工位搬了，可能不想占用隔壁实验室的位置吧 😂 但是我真的不想去 😭 不能跟 00 待在一起了。我现在就是一周可能去两三天 😂</p><p>最近还申请了签证，决定了寒假 00 跟我一起回家！在家待一整个月，好神奇，觉得我们的关系发展得好顺利。马上的国庆 🇨🇳 我会跟 00 回去武汉和应城参加她高中同学和表姐的婚礼 💑，顺便还会看她的外公外婆。</p><p><img src="/%E6%9B%B4%E6%96%B0%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5/%E4%B8%AD%E5%9B%BD%E6%8A%A4%E7%85%A7.jpg" alt=""></p><blockquote><p>00 的护照，是我向往的身份！</p></blockquote><p><img src="/%E6%9B%B4%E6%96%B0%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5/%E7%94%B3%E8%AF%B7%E7%AD%BE%E8%AF%81.jpg" alt=""></p><blockquote><p>签证中心门口，不是大使馆，很多个国家统一办理签证的地方。</p></blockquote><p><img src="/%E6%9B%B4%E6%96%B0%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5/%E5%90%83%E6%BE%B3%E9%97%A8%E8%8F%9C.jpg" alt=""></p><blockquote><p>在申请签证的地方旁边的一个很高级的商场里面吃澳门餐。</p></blockquote><hr><h2 id="一些-Markdown-渲染器测试">一些 Markdown 渲染器测试</h2><p>测试一下公式的渲染。<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup></p><p>$$\theta_i \leftarrow  \frac{\partial}{\partial  \theta_i} \mathcal L( y, f(x; \theta))$$</p><p>其中 $\mathcal L$ 是损失函数，$f(\cdot; \theta)$ 由 $\theta$ 参数化的模型。</p><p>那 <code>代码</code> 呢？<code>codell1i0oO</code> 可以吗？</p><p>一个表格：</p><table><thead><tr><th>Method</th><th>Accuracy</th></tr></thead><tbody><tr><td>A</td><td>0.1</td></tr><tr><td>B</td><td>0.2</td></tr><tr><td>C</td><td>0.3</td></tr></tbody></table><p>一个嵌套列表：</p><ul><li><p>a</p><ul><li><p>x</p></li><li><p>asdf</p><ul><li>懂得都懂</li></ul></li><li><p>好好</p><ol><li><p>item 1</p></li><li><p>item 2</p><ol><li>sdf</li><li>sdf</li></ol></li><li><p>item 3</p></li></ol></li></ul></li><li><p>asdf</p></li></ul><p>一个代办列表：</p><ul class="contains-task-list"><li class="task-list-item"><input class="task-list-item-checkbox" disabled="" type="checkbox"> 买菜</li><li class="task-list-item"><input class="task-list-item-checkbox" disabled="" type="checkbox"> 做饭</li><li class="task-list-item"><input class="task-list-item-checkbox" checked="" disabled="" type="checkbox"> 跑步</li><li class="task-list-item"><input class="task-list-item-checkbox" checked="" disabled="" type="checkbox"> 打羽毛球</li><li class="task-list-item"><input class="task-list-item-checkbox" checked="" disabled="" type="checkbox"> 写论文</li><li class="task-list-item"><input class="task-list-item-checkbox" disabled="" type="checkbox"> 搞科研</li></ul><p>auto-links: <a href="http://www.hexo.io">www.hexo.io</a></p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p>我导师和知乎孵化的的公司 <a href="#fnref1" class="footnote-backref">↩︎</a></p></li><li id="fn2" class="footnote-item"><p>我现在用了 <a href="https://github.com/hexojs/hexo-renderer-markdown-it">hexo-rendered-markdown-it</a> 来代替原来 Hexo 的渲染器。原来的渲染器是 Marked (hexo-renderer-marked) <a href="#fnref2" class="footnote-backref">↩︎</a></p></li></ol></section>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> research </tag>
            
            <tag> 00 </tag>
            
            <tag> life </tag>
            
            <tag> 中文 </tag>
            
            <tag> 面壁智能 </tag>
            
            <tag> blog </tag>
            
            <tag> 签证 </tag>
            
            <tag> hexo </tag>
            
            <tag> hugo </tag>
            
            <tag> jekyll </tag>
            
            <tag> static-site-generator </tag>
            
            <tag> 羽毛球 </tag>
            
            <tag> work </tag>
            
            <tag> 枫叶 </tag>
            
            <tag> 中国 </tag>
            
            <tag> 挪威 </tag>
            
            <tag> markdown </tag>
            
            <tag> 宿舍 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CFDBench: A Comprehensive Benchmark for Machine Learning Methods in Fluid Dynamics</title>
      <link href="/2023/09/16/CFDBench/"/>
      <url>/2023/09/16/CFDBench/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www.github.com/luo-yining/CFDBench">Code</a> | <a href="...">Paper (on hold by ArXiv)</a> | <a href="https://www.preprints.org/manuscript/202309.1550/v1">Paper (preprints.org)</a> | <a href="https://zhuanlan.zhihu.com/p/656033757">知乎</a></p><p>I did this work with my girlfriend, whose research direction is computational fluid dynamics (CFD). We observed that there are numerous research works in applying deep learning (DL) to solve CFD problems. E.g., <a href="https://github.com/198808xc/Pangu-Weather">Pangu-Weather</a> have shown that DL methods can not only be more accurate than the best numerical methods, but can also be multiple magnitudes faster.</p><span id="more"></span><p>However, there is no standard benchmark for evaluating the performance of different DL methods. Therefore, we constructed CFDBench.</p><hr><h2 id="Abstract">Abstract</h2><p>In recent years, applying deep learning to solve physics problems has attracted much attention. Data-driven deep learning methods produce operators that can learn solutions to the whole system of partial differential equations. However, the existing methods are only evaluated on simple flow equations (e.g., Burger's equation), and only consider the generalization ability on different initial conditions. In this paper, we construct CFDBench, a benchmark with four classic problems in computational fluid dynamics (CFD): lid-driven cavity flow, laminar boundary layer flow in circular tubes, dam flows through the steps, and periodic Karman vortex street. Each flow problem includes data with different boundary conditions, fluid physical properties, and domain geometry. Compared to existing datasets, the advantages of CFDBench are (1) comprehensive. It contains common physical parameters such as velocity, pressure, and cavity fraction. (2) realistic. It is very suitable for deep learning solutions of fluid mechanics equations. (3) challenging. It has a certain learning difficulty, prompting to find models with strong learning ability. (4) standardized. CFDBench facilitates a comprehensive and fair comparison of different deep learning methods for CFD. We make appropriate modifications to popular deep neural networks to apply them to CFDBench and enable the accommodation of more changing inputs. The evaluation on CFDBench reveals some new shortcomings of existing works and we propose possible directions for solving such problems.</p>]]></content>
      
      
      <categories>
          
          <category> Research </category>
          
      </categories>
      
      
        <tags>
            
            <tag> research </tag>
            
            <tag> paper </tag>
            
            <tag> cfd </tag>
            
            <tag> dataset </tag>
            
            <tag> 00 </tag>
            
            <tag> english </tag>
            
            <tag> pinn </tag>
            
            <tag> fno </tag>
            
            <tag> physics </tag>
            
            <tag> machine-learning </tag>
            
            <tag> deep-learning </tag>
            
            <tag> deeponet </tag>
            
            <tag> ai4science </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Some Binary Search</title>
      <link href="/2023/09/14/some_binary_search/"/>
      <url>/2023/09/14/some_binary_search/</url>
      
        <content type="html"><![CDATA[<p>A binary search with C++:</p><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> T&gt;</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">bin_search</span><span class="params">(vector&lt;T&gt;&amp; arr, T target)</span> </span>{</span><br><span class="line">    <span class="type">int</span> left = <span class="number">0</span>, right = arr.<span class="built_in">size</span>() - <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span> (left &lt;= right) {</span><br><span class="line">        <span class="type">int</span> mid = (left + right) / <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">if</span> (arr[mid] == target) {</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        } <span class="keyword">else</span> <span class="keyword">if</span> (arr[mid] &lt; target) {</span><br><span class="line">            left = mid + <span class="number">1</span>;</span><br><span class="line">        } <span class="keyword">else</span> {</span><br><span class="line">            right = mid - <span class="number">1</span>;</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">return</span> left;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>The same thing with Rust:</p><figure class="highlight rust"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fn</span> <span class="title function_">bin_search</span>&lt;T: <span class="built_in">Ord</span>&gt;(arr: &amp;<span class="type">Vec</span>&lt;T&gt;, target: &amp;T) <span class="punctuation">-&gt;</span> <span class="type">usize</span> {</span><br><span class="line">    <span class="keyword">let</span> <span class="keyword">mut </span><span class="variable">left</span> = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">let</span> <span class="keyword">mut </span><span class="variable">right</span> = arr.<span class="title function_ invoke__">len</span>() - <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span> left &lt;= right {</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">mid</span> = (left + right) / <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">if</span> arr[mid] == *target {</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        } <span class="keyword">else</span> <span class="keyword">if</span> arr[mid] &lt; *target {</span><br><span class="line">            left = mid + <span class="number">1</span>;</span><br><span class="line">        } <span class="keyword">else</span> {</span><br><span class="line">            right = mid - <span class="number">1</span>;</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">    left</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>And with Python:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">bin_search</span>(<span class="params">arr: <span class="built_in">list</span>, target</span>):</span><br><span class="line">    left = <span class="number">0</span></span><br><span class="line">    right = <span class="built_in">len</span>(arr) - <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> left &lt;= right:</span><br><span class="line">        mid = (left + right) // <span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> arr[mid] == target:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">elif</span> arr[mid] &lt; target:</span><br><span class="line">            left = mid + <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            right = mid - <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> left</span><br></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> Test </category>
          
      </categories>
      
      
        <tags>
            
            <tag> english </tag>
            
            <tag> algorithm </tag>
            
            <tag> binary-search </tag>
            
            <tag> rust </tag>
            
            <tag> python </tag>
            
            <tag> c++ </tag>
            
            <tag> test </tag>
            
            <tag> code </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>临近2023暑假，00师姐答辩，晚上打球</title>
      <link href="/2023/05/18/%E4%B8%B4%E8%BF%912023%E6%9A%91%E5%81%87/"/>
      <url>/2023/05/18/%E4%B8%B4%E8%BF%912023%E6%9A%91%E5%81%87/</url>
      
        <content type="html"><![CDATA[<p>今天睡到九点才醒来，还是被00打电话叫醒的。去过了个早，然后去上《深度学习》。</p><p>今天00的师姐答辩，下午三点去了，当时我在睡午觉。感觉之后她有点emo，但是她不承认，不知道为什么。然后聊了很多关于未来，结婚、生孩子、找工作等事情。感觉也没有很大的问题，但是00总是把东西看得很灰暗，很焦虑。</p><span id="more"></span>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 00 </tag>
            
            <tag> life </tag>
            
            <tag> 中文 </tag>
            
            <tag> school </tag>
            
            <tag> graduation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第一个帖子，瞎写点东西</title>
      <link href="/2023/05/17/%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%B8%96%E5%AD%90%EF%BC%8C%E7%9E%8E%E5%86%99%E7%82%B9%E4%B8%9C%E8%A5%BF/"/>
      <url>/2023/05/17/%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%B8%96%E5%AD%90%EF%BC%8C%E7%9E%8E%E5%86%99%E7%82%B9%E4%B8%9C%E8%A5%BF/</url>
      
        <content type="html"><![CDATA[<p>现在是 2023 年五月十七，马上硕士一年级就结束，在清华园已经快五年了，感觉对我人生的影响真的巨大。这一年认识了很可爱的 00，希望可以一直走下去。</p><p>我和 00 的孩子们：</p><span id="more"></span><ul><li>卧龙：调皮的肥猫 🐱</li><li>小绿：喜欢咬东西的鳄鱼 🐊</li><li>骆雁：超级大的土鸡！🐰</li><li>凤雏：不调皮的猫咪 🐱</li><li>黄帝：更大的巨兔 🐰</li><li>内存条：白色的熊熊 🐻</li><li>闪光灯：灰色的熊熊 🐻</li></ul><h2 id="现在要做的事情">现在要做的事情</h2><ul><li><p>把 EmoRen 投了</p><ul><li>能不能行啊</li></ul></li><li><p>跑 CFD 的丹炉调好</p><ul><li>好难呀</li></ul></li><li><p>写完作业</p><ul><li>NLP和DL的大作业！</li></ul></li><li><p>搞定去ACL的手续</p><ul><li>去加拿大，然后回挪威一两周，然后回来跟 00 去南京，我不用签证，但是还是有很多手续。</li></ul></li><li><p>写好开题报告</p><ul><li>还不知道做啥呢</li></ul></li></ul><h2 id="我的家乡-Lillesand">我的家乡 Lillesand</h2><p><img src="/2023/05/17/%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%B8%96%E5%AD%90%EF%BC%8C%E7%9E%8E%E5%86%99%E7%82%B9%E4%B8%9C%E8%A5%BF/lillesand0.jpg" alt=""></p><p><img src="/2023/05/17/%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%B8%96%E5%AD%90%EF%BC%8C%E7%9E%8E%E5%86%99%E7%82%B9%E4%B8%9C%E8%A5%BF/lillesand1.jpg" alt=""></p><p>好久没有回去了，上一次回挪威也没有回去</p>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> research </tag>
            
            <tag> cfd </tag>
            
            <tag> 00 </tag>
            
            <tag> life </tag>
            
            <tag> 中文 </tag>
            
            <tag> school </tag>
            
            <tag> 孩子们 </tag>
            
            <tag> 卧龙 </tag>
            
            <tag> 凤雏 </tag>
            
            <tag> 骆雁 </tag>
            
            <tag> 黄帝 </tag>
            
            <tag> 小绿 </tag>
            
            <tag> 猫咪 </tag>
            
            <tag> 土鸡 </tag>
            
            <tag> 熊 </tag>
            
            <tag> 🐻 </tag>
            
            <tag> 🐱 </tag>
            
            <tag> 🐇 </tag>
            
            <tag> 🐰 </tag>
            
            <tag> 🐊 </tag>
            
            <tag> emoren </tag>
            
            <tag> acl </tag>
            
            <tag> lillesand </tag>
            
            <tag> 加拿大 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第一篇</title>
      <link href="/2022/10/27/%E7%AC%AC%E4%B8%80%E7%AF%87/"/>
      <url>/2022/10/27/%E7%AC%AC%E4%B8%80%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<p>之前尝试用 Hugo 来部署，发现 Hugo 不仅挺复杂，而且还有很多小问题，可能这就是速度带来的代价吧。但是其实我也不是写很多内容，所以 Jekyll 的速度应该是够用的。</p><p>Jekyll 支持在 markdown 内容里面用 Liquid template tags 来生成动态内容，比如根据 front matter 中的 tags，给每个 tag 生成 html div。如下 liquid 语法：</p><figure class="highlight html"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span>&gt;</span></span><br><span class="line">{% for tag in site.tags %}</span><br><span class="line">    <span class="tag">&lt;<span class="name">a</span> <span class="attr">style</span>=<span class="string">"background-color: blue;"</span>&gt;</span>#{{ tag[0] }}<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">{% endfor %}</span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></tbody></table></figure><span id="more"></span><p>会根据 post markdown 文件中的 front matter 中定义的 tags：</p><figure class="highlight markdown"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line"><span class="section">tags: some tags here</span></span><br><span class="line"><span class="section">---</span></span><br></pre></td></tr></tbody></table></figure><p>生成相应的 html div：</p><figure class="highlight html"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"post-tags"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"/tags/life"</span> <span class="attr">class</span>=<span class="string">"tag-card"</span>&gt;</span>life<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"/tags/update"</span> <span class="attr">class</span>=<span class="string">"tag-card"</span>&gt;</span>update<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"/tags/learn"</span> <span class="attr">class</span>=<span class="string">"tag-card"</span>&gt;</span>learn<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"/tags/important"</span> <span class="attr">class</span>=<span class="string">"tag-card"</span>&gt;</span>important<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"/tags/jekyll"</span> <span class="attr">class</span>=<span class="string">"tag-card"</span>&gt;</span>jekyll<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"/tags/hugo"</span> <span class="attr">class</span>=<span class="string">"tag-card"</span>&gt;</span>hugo<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"/tags/static-site-generator"</span> <span class="attr">class</span>=<span class="string">"tag-card"</span>&gt;</span>static-site-generator<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></tbody></table></figure><hr><p>不知道写啥，就写一个 Python 的二分搜索吧：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">bin_search</span>(<span class="params">arr: <span class="built_in">list</span>, target: <span class="type">Any</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Return the smallest index such that when target is inserted at that index,</span></span><br><span class="line"><span class="string">    the array will remain sorted.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    lo, hi = <span class="number">0</span>, <span class="built_in">len</span>(arr)</span><br><span class="line">    <span class="keyword">while</span> lo &lt; hi:</span><br><span class="line">        m = (lo + hi) // <span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> arr[m] &lt; target:</span><br><span class="line">            lo = m + <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            hi = m</span><br><span class="line">    <span class="keyword">return</span> lo</span><br></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> life </tag>
            
            <tag> 中文 </tag>
            
            <tag> test </tag>
            
            <tag> hugo </tag>
            
            <tag> jekyll </tag>
            
            <tag> static-site-generator </tag>
            
            <tag> html </tag>
            
            <tag> liquid </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
