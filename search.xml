<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>CFDBench: A Comprehensive Benchmark for Machine Learning Methods in Fluid Dynamics</title>
    <url>/2023/09/16/CFDBench/</url>
    <content><![CDATA[<p><a href="https://www.github.com/luo-yining/CFDBench">Code</a> | <a href="...">Paper (on hold by ArXiv)</a> | <a href="https://www.preprints.org/manuscript/202309.1550/v1">Paper (preprints.org)</a> | <a href="https://zhuanlan.zhihu.com/p/656033757">知乎</a></p>
<p>I did this work with my girlfriend, whose research direction is computational fluid dynamics (CFD). We observed that there are numerous research works in applying deep learning (DL) to solve CFD problems. E.g., <a href="https://github.com/198808xc/Pangu-Weather">Pangu-Weather</a> have shown that DL methods can not only be more accurate than the best numerical methods, but can also be multiple magnitudes faster.</p>
<span id="more"></span>
<p>However, there is no standard benchmark for evaluating the performance of different DL methods. Therefore, we constructed CFDBench.</p>
<hr>
<h2 id="Abstract">Abstract</h2>
<p>In recent years, applying deep learning to solve physics problems has attracted much attention. Data-driven deep learning methods produce operators that can learn solutions to the whole system of partial differential equations. However, the existing methods are only evaluated on simple flow equations (e.g., Burger's equation), and only consider the generalization ability on different initial conditions. In this paper, we construct CFDBench, a benchmark with four classic problems in computational fluid dynamics (CFD): lid-driven cavity flow, laminar boundary layer flow in circular tubes, dam flows through the steps, and periodic Karman vortex street. Each flow problem includes data with different boundary conditions, fluid physical properties, and domain geometry. Compared to existing datasets, the advantages of CFDBench are (1) comprehensive. It contains common physical parameters such as velocity, pressure, and cavity fraction. (2) realistic. It is very suitable for deep learning solutions of fluid mechanics equations. (3) challenging. It has a certain learning difficulty, prompting to find models with strong learning ability. (4) standardized. CFDBench facilitates a comprehensive and fair comparison of different deep learning methods for CFD. We make appropriate modifications to popular deep neural networks to apply them to CFDBench and enable the accommodation of more changing inputs. The evaluation on CFDBench reveals some new shortcomings of existing works and we propose possible directions for solving such problems.</p>
]]></content>
      <categories>
        <category>Research</category>
      </categories>
      <tags>
        <tag>research</tag>
        <tag>paper</tag>
        <tag>cfd</tag>
        <tag>dataset</tag>
        <tag>00</tag>
        <tag>english</tag>
        <tag>pinn</tag>
        <tag>fno</tag>
        <tag>physics</tag>
        <tag>machine-learning</tag>
        <tag>deep-learning</tag>
        <tag>deeponet</tag>
        <tag>ai4science</tag>
      </tags>
  </entry>
  <entry>
    <title>2023年中秋和国庆</title>
    <url>/2023/10/05/2023%E4%B8%AD%E7%A7%8B/</url>
    <content><![CDATA[<p>今年国庆 🇨🇳 和中秋 🥮 一起放假，我跟 00 一起回来应城参加她堂姐和初中同学的婚礼<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>，
住在她家里十个夜晚<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup>。第二次见家长，也算是挺顺利，但是每天都会见到陌生人，有点累，庆幸的是，感觉到 00 能接受跟我家人生活在一起。一号到三号我们去武汉玩了三天，超级开心，跟她在一起连逛商场都是开心的！</p>
<h2 id="小县城的氛围">小县城的氛围</h2>
<p>应城跟我想象中的小县城很像，也是很多远房亲戚，习俗也让人很烦。敬酒、随地扔垃圾、室内抽烟、八卦人家的私事、说话粗鄙、脏、说了不要还非要给人家……而且确实能明显感觉到，这里的人的素质的平均水平挺低的，尤其是上一辈。真的很讨厌吃席，00 也是，这些习俗的麻烦程度让 00 都不想结婚了……</p>
<span id="more"></span>
<p>但是无所谓了，之后能跟 00 在一起就好，除了回来过节应该也很少机会有联系。</p>
<h2 id="武汉">武汉</h2>
<p>一号到三号去了武汉旅游。早上五点多跟 00 的 ”二妈“（其实是婶，叔叔的老婆）坐车去武汉，坐了一个小时。他们这么早是因为要去谈婚礼的事情，然后害怕堵车。我们在酒店旁边下来，那时候“二妈”下地铁站上厕所，然后 00 非要给她买包子（为了礼貌），然后她最后还是拒绝了，导致我们得自己吃下包子。虽然包子没有不好吃，但是我就很讨厌这种明知人家不要还非要买的行为。</p>
<p>之后我们去酒店的时候，还没有房子，我们寄存了行李就直接去新天地买了杯霸王茶姬的奶茶，然后去了古德寺。网上说不可以穿着暴露，但是感觉路人穿着还是很暴露。</p>
<p><img src="/2023/10/05/2023%E4%B8%AD%E7%A7%8B/%E6%96%B0%E5%A4%A9%E5%9C%B0-%E9%9C%B8%E7%8E%8B%E8%8C%B6%E5%A7%AC.png" alt="" title="在武汉新天地买霸王茶姬。"></p>
<p>之后还去了解放公园和中山公园，都挺不错的。大城市就是好。里面看到了很好看的建筑物。在中山公园我们问了两个小孩借用羽毛球拍子来打了几下。之后在一个相亲角<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup>旁边跟她的高中同学，彭双，会合，然后逛了一下相亲角。之后我们还坐了一下过山车（公园里面有过山车还是第一次见）。</p>
<p><img src="/2023/10/05/2023%E4%B8%AD%E7%A7%8B/%E8%A7%A3%E6%94%BE%E5%85%AC%E5%9B%AD%E4%B8%AD%E9%97%B4.png" alt="" title="解放公园中间的一个很多塔的地方。"></p>
<p>晚上就去跟她的高中同学一起吃饭。</p>
<p>第二天我们先在地铁站剪了头发，然后去宝通寺，晚上去武商梦时代。这个商场规格超级高，还挺好玩的。第一次看到索尼专卖店，还有 Pico 专卖店。里面还有滑雪的地方，但是太贵的。我们还去了优衣库，买了一些衣服，发现还挺便宜的。以前都会觉得逛街购物很无聊，但是跟她在一起连连逛街买衣服都是开心的。</p>
<p>晚上我们跟她“大哥”（其实是堂哥）和他老婆一起吃饭，吃了魔宗烤肉，然后喝了茶颜悦色。总体来说也挺顺利的，感觉他们也不难相处。</p>
<p><img src="/2023/10/05/2023%E4%B8%AD%E7%A7%8B/%E6%AD%A6%E5%95%86%E6%A2%A6%E6%97%B6%E4%BB%A3.png" alt="" title="武商梦时代里面的美食街买鲜虾汤包"></p>
<p>第三天我们去了欢乐谷！是我们第一次一起去游乐场！玩了一个过山车，然后做了太阳飞车，00 就头晕想吐了，果然还是不行……但是没事，还是挺开心的。排队过程中还遇到了插队的人，好恶心！</p>
<p>晚上我们跟一些人（共七个人）一起拼车回来应城，居然比火车还便宜，不错。回来已经11点了，然后回家放下行李箱之后又出去找她初中同学一起吃宵夜。</p>
<p><img src="/2023/10/05/2023%E4%B8%AD%E7%A7%8B/%E6%AD%A6%E6%B1%89%E6%AC%A2%E4%B9%90%E8%B0%B7.png" alt="" title="在武汉欢乐谷玩耍。"></p>
<h2 id="公事">公事</h2>
<p>这个假期有点长，感觉有很多活都没有干。每天都很多事情，感觉这里的人太闲了，应该让他们多上班哈哈哈。古文字翻译的工作还没有干完，目前感觉效果不是很好，我也不想干这个了，感觉很浪费我的时间……至于对齐神经元，貌似现有方法都无法用在自回归模型上面，但是对齐问题好像之后自回归模型才会出现。不知道是不是我没有找到，目前还没有找到一篇研究神经元对生成结果的影响的工作。<a href="https://www.github.com/kmeng01/rome">ROME</a> 的 Causal Tracing 感觉可以用，这两天得赶紧做点东西出来。</p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>27号是初中同学（魏陈）的婚礼，5号是堂姐（骆卓颖）的婚礼。 <a href="#fnref1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn2" class="footnote-item"><p>九月二十六日回来，十月七日走。坐高铁到北京，然后做火车到应城。 <a href="#fnref2" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn3" class="footnote-item"><p>之前在上海都没找到。 <a href="#fnref3" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section>
]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>00</tag>
        <tag>life</tag>
        <tag>中秋</tag>
        <tag>中文</tag>
        <tag>wedding</tag>
        <tag>中秋-middle-autumn</tag>
        <tag>国庆-national-day</tag>
        <tag>应城</tag>
        <tag>武汉-wuhan</tag>
      </tags>
  </entry>
  <entry>
    <title>Activation Addition (ActAdd)</title>
    <url>/2023/10/07/actadd/</url>
    <content><![CDATA[<p><a href="https://arxiv.org/abs/2308.10248">Paper</a></p>
<p>TLDR: Propose <strong>ActAdd</strong>, a method for controlling model behavior during inference by modifying activations with a bias term that is learned from a pair of prompt.</p>
<p>Summary:</p>
<ul>
<li>Propose <strong>ActAdd</strong>, a method for controlling model behavior by modifying activations at inference time.</li>
<li>Steering vectors are computed by taking the activation differences that result from pairs of prompts. The vectors are added as bias during inference.</li>
<li>ActAdd provides control over high-level properties of the output, and preserves off-target model performance, and requires little computational and implementational costs.</li>
</ul>
<span id="more"></span>
<blockquote>
<p>The recently popular <a href="https://arxiv.org/abs/2310.01405">representation engineering paper</a> (RepE) seems to be largely inspired by this work.</p>
</blockquote>
<h2 id="Background">Background</h2>
<p>The authors propose to compute steering vectors to steer the model's behavior. They call such methods <em>activation engineering</em>. They make the following contributions:</p>
<ul>
<li>Find that combining forward passes works well in GPT-2, despite it was not trained for this.</li>
<li>The proposed method, <strong>ActAdd</strong>, is efficient, requiring no gradient descent or labeled data.</li>
</ul>
<p>The difference between ActAdd and existing steering vector methods is that they find the vectors via one of the following.</p>
<ul>
<li>Differences after fine-tuning</li>
<li>Per-query gradient-based search</li>
<li>Linear probes + differences in truthy attention heads</li>
</ul>
<p>In contrast, ActAdd uses the difference between prompt pairs instead.</p>
<h2 id="Method">Method</h2>
<p>The method is really, really simple. Simply manually contruct a pair of prompts, and compute the difference between the activations. Then, add the difference as a bias term to the activations during inference. The algorithm is as follows.</p>
<p><img src="/2023/10/07/actadd/alg.png" alt="" title="The algorithm of ActAdd"></p>
<p>As shown, this method has two hyperparameters, the amount of drift $c$, and the modified layer $l$, and requires two manually constructed prompts $(p_+, p_-)$. How to more effectively construct these prompts is not discussed in this paper.</p>
<h2 id="Result">Result</h2>
<p><img src="/2023/10/07/actadd/result.png" alt="" title="Main results."></p>
<h2 id="My-Thoughts">My Thoughts</h2>
<p>The effectiveness of this method is a strong evidence that input and output features are represented as linear directions in representational space, but we still have no explanation for why such linearity arises naturally in LLMs. The fact that this actually works is very thought-provoking. However, ActAdd start to see degraded performance on off-target inputs when we drive the activations to far off, and the steering sometimes simply fails, this may indicate that the optimal steering path is not linear, which I believe is reasonable. This is somewhat similar to neuron attribution methods, many features/skills/knowledge cannot be attributed to single neurons (they are distributed across neurons), but existing neuron attribution methods still work well because, by change, some features are primarily determined by the activity or state of a single neuron (or a small set of neurons).</p>
<p>We can also draw a parallel with <a href="https://arxiv.org/abs/2106.10199">BitFit</a>, which shows that tuning only (a subset of) the bias terms and the task-specific classifier head in a transformer model can achieve tuning performance comparable to full parameter finetuning. BitFit did only experiments on <strong>encoder models</strong>, in which case bias terms can be seen as steering vectors in the hidden representation space, therefore, ActAdd and BitFit differs only in the training signel. ActAdd uses the difference between the representation of a pair of (positive and negative) prompts, while BitFit propagates the human annotation from the classification head. For <strong>autoregressive models</strong>, ActAdd is more expressive because each token can be steered independently, while BitFit can only steer the whole sequence.</p>
<p>Interestingly, the fact that difference (or other arithmetics) in hidden representation are useful signal for some semantics has been shown in the era of learning word vectors, the fact that this can be used as a training signal is pretty neat.</p>
<p>The author also discussed the difference between activation engineering and adaptation methods, but the empirical results were not enough to show that ActAdd can be used as a substitute for adaptation. E.g., we cannot practically adapt the model into performing machine translation with ActAdd, because there is no negative prompt for translation. But perhaps upcoming works can apply this method to replace fine-tuning (or other adaptation methods). The realization may be a promising way to effiicently control any arbitrary model behavior without backpropagation<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>.</p>
<p>Nevertheless, this method is extremely interesting, and I feel like many existing methods can be improved by taking inspirations from this work.</p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p><a href="https://arxiv.org/abs/2305.17333">MeZO</a> is one alternative, but the training time of MeZO is almost the same as fine-tuning. <a href="#fnref1" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section>
]]></content>
      <categories>
        <category>Paper Note</category>
      </categories>
      <tags>
        <tag>llm</tag>
        <tag>english</tag>
        <tag>ai-alignment</tag>
        <tag>gpt</tag>
        <tag>activation-modification</tag>
        <tag>adaptation</tag>
        <tag>model-editing</tag>
        <tag>representation-engineering</tag>
        <tag>fine-tuning</tag>
        <tag>parameter-efficient-tuning</tag>
      </tags>
  </entry>
  <entry>
    <title>(EREN) Robust and Scalable Model Editing for Large Language Models</title>
    <url>/2024/03/14/eren/</url>
    <content><![CDATA[<p><a href="https://www.github.com/chen-yingfa/eren">GitHub</a> | <a href="...">Paper (upcoming)</a></p>
<p><strong>TL;DR</strong>: A reader is augmented with a growing notebook that caches all edits in natural texts, and the reader retrieves relevant edits and make inference based on them. This achieves SOTA in model editing in QA and fact-checking.</p>
<span id="more"></span>
<hr>
<h2 id="Introduction">Introduction</h2>
<p><img src="/2024/03/14/eren/framework.png" alt=""></p>
<p>This work introduces a model editing method that addresses two issues with existing model editors:</p>
<ol>
<li>They cannot handle multiple sequential edits.</li>
<li>They cannot be applied to black-box models.</li>
</ol>
<blockquote>
<p>The reason we want to apply multiple sequential edits is that in practical applications, new knowledge appear in an on-line manner and the user might want to update the model's knowledge immediately.</p>
</blockquote>
<h2 id="Method">Method</h2>
<p>In summary, the edited LLM is complemented with a notebook that caches all edits in natural text. For each question, the model first determines whether the input is relevant to any edit, if so, it makes a prediction based on the notebook. Else, it directly answers the question using its memorized knowledge.</p>
<p>We find that LLMs, even when instruction-tuned, are not readily controllable by their context, i.e. the notebook. In particular, they are not robust to irrelevant context<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>, resulting in changed predictions on unrelated inputs. Also, the number of edits may to too large to fit into the input context of the LLM. Addressing these two issues, we propose to (1) split inference into two steps, and (2) use a dual-encoder retrieval framework to perform rough relevance estimation. Figure 1 (above) shows the overall framework of our method.<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup></p>
<h2 id="Result">Result</h2>
<p>From the following results on CounterFact (QA editing task) and FEVER (fact-checking editing task), we conclude that our LLM editor significantly outperforms existing methods.</p>
<h3 id="Metrics">Metrics</h3>
<ul>
<li>ES (Edit Success): the percentage of examples where the model correctly answers the question after the edit.</li>
<li>BP (Behavior Preservation): the percentage of unrelated examples whose output were not changed by the edit.</li>
<li>EQ (Edit Quality): the harmonic mean of ES and BP.<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup></li>
</ul>
<p>A perfect model editor should have an EQ of 1.</p>
<p><img src="/2024/03/14/eren/results.png" alt=""></p>
<p>The reason MEND and ROME performs so bad is that after many <strong>sequential</strong> edits, the model parameters are changed to much that the models start to output unintelligible tokens. In the case of fact-checking, randomly guessing "yes" or "no" should result in a 50% accuracy. However, since we use exact match to compute the accuracy, both MEND and ROME have much lower accuracy than random guess.</p>
<h3 id="Different-Number-of-Edits">Different Number of Edits</h3>
<p>For methods that modify the model parameters, each edit will have detrimental effects on the model's performance. Moreover, for methods like ROME that depend on precomputing layer statistic of the unedited model, that statistics will be gradually more inaccurate after every edit (recomputing that statistics is too costly). Therefore, after many sequential edits, the model will start to spit out gibberish, as evident in the Figure below.</p>
<p><img src="/2024/03/14/eren/diff-edit-cnt.png" alt=""></p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>which has been concluded in several previous works <a href="#fnref1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn2" class="footnote-item"><p>For more details, please read the paper or contact me through e-mail. <a href="#fnref2" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn3" class="footnote-item"><p>Using harmonic mean because we want the model editor to have both high ES and BP. A naive editor that ignores all edits can have a BP of 1, but ES of 0. <a href="#fnref3" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section>
]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>llm</tag>
        <tag>research</tag>
        <tag>ai</tag>
        <tag>paper</tag>
        <tag>english</tag>
        <tag>arxiv</tag>
        <tag>knowledge</tag>
        <tag>emnlp</tag>
        <tag>model editing</tag>
        <tag>in-context-learning</tag>
        <tag>serac</tag>
        <tag>rome</tag>
        <tag>eren</tag>
        <tag>mend</tag>
      </tags>
  </entry>
  <entry>
    <title>InfiniteBench: Extending Long Context Evaluation Beyond 100K Tokens</title>
    <url>/2024/01/10/infinitebench/</url>
    <content><![CDATA[<p><a href="http://www.github.com/OpenBMB/InfiniteBench">Code</a> | <a href="https://arxiv.org/abs/2402.13718">Paper</a></p>
<p>The first benchmark for evaluating the effectiveness of LLMs in handling more than 100k tokens!</p>
<blockquote>
<p>In the paper, we name it $\infty$-Bench, but I will sometimes use "InfiniteBench" in this blog post for better readability.</p>
</blockquote>
<p>Finally got some time to write this blog, been so busy lately! I have been in a fairly long duration of research hiatus, meanwhile the field of NLP has been revolutionized by an overwhelming number of new LLMs. Finally, I was able to arrive at some productive and meaningful work in this new era of research, as a second author. In this blog post, I will introduce this work that I have been working on recently.</p>
<span id="more"></span>
<h2 id="Background">Background</h2>
<p>The advent of LLMs have shown many promising results, but many practice applications (e.g., agents, document/webpage reading, long text summarization, etc.) are greatly limited by the context length constraint. Therefore, many works have strived to increase the length of the context that LLMs can accept. However, current "long-sequence" benchmarks all fall below 100k tokens, and is therefore not able to evaluate the effectiveness of many long-context LLMs. Our work, $\infty$-Bench</p>
<h2 id="The-Data">The Data</h2>
<p>The data consists of language tasks from diverse domains (math, code, novels), two languages (English and Chinese). Half of the tasks are automatically generated, which is desirable for optionally further scaling the context lengths to any arbitrary lengths.</p>
<p>Following shows the statistics of the tasks in our benchmark.</p>
<p><img src="/2024/01/10/infinitebench/data-stat-pie.png" alt="" title="Data statistics. The angle of segments is proportional to the number of examples, and the radius is proportional to the average example lengths (sum of input and output tokens)."></p>
<h2 id="Results">Results</h2>
<p>We tested SOTA proprietary and open-source LLMs at the time of evaluation. The result is shown below. We can see that in most tasks, the performance is far from satisfactory in practical applications.</p>
<p><img src="/2024/01/10/infinitebench/results.png" alt="" title="Results of some SOTA long-context LLMs on our InfiniteBench"></p>
<h2 id="Thoughts-on-the-Future-of-Long-Context-Research">Thoughts on the Future of Long-Context Research</h2>
<p>Our lab have been investing much efforts in long-context LLMs lately. Particularly, we are interested in developing LLMs that can accept infinite input lengths, or what some of my colleagues call streaming language models (i.e., models that operate on streaming inputs). I believe that the transformer architecture is inherently incapable of processing infinite-length inputs. This is the research direction that I have been focusing on.</p>
<h3 id="Inherent-Limitations-of-Transformers">Inherent Limitations of Transformers</h3>
<p>The most obvious reason is the quadratic complexity. A large number of research papers have focused on reducing the computational cost of the self-attention mechanism. But most SOTA LLMs at the moment are still dense attention layers, and rely on using Flash-Attention to speedup the computation. However, through discussion with various researchers, I have found that many people now believe that the attention computation is fast enough for most practical applications. I strongly disagree with this view. Firstly, the computational cost translates to the operational cost and emission that results from the usage of LLMs, which is of great concern. Secondly, my experience with ChatGPT (especially when using GPT-4) is that it is often not fast enough. Especially when it tends to produce many irrelevant lead-up sentences, I often find that I can find the answer using search engines before ChatGPT gives me the answer. Thirdly, current LLMs typically only have less than 100k in context length, however, as we apply them on contexts with millions of tokens, the speed of processing these tokens becomes unacceptable in most applications. For instance, in our experiments with InfiniteBench, applying a 7B model on 128k tokens using one A100 GPU takes 8~11 minutes to simply read the input<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>.</p>
<blockquote>
<p>This is not the only drawback of the transformer architecture, but that is out of the scope of this discussion.</p>
</blockquote>
<h3 id="Possible-Paths">Possible Paths</h3>
<p>I do not believe making small tweaks to the self-attention mechanism will solve the problem. Yep, we need new model architectures. Two architectures that I find promising are <strong>linear attention</strong> and <strong>state-space models</strong> (SSMs). Since these architectures have been widely discussed in the research community, I will not describe them in detail here. Instead, I want to express my opinion on the future of these architectures.</p>
<p>I like to think of different language model architectures from the perspective of compressing the history<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup>. The way transformers work is that they feed the last $L$ tokens without any compression to the model in each step. This means that the model remembers everything perfectly up to $L$ previous tokens, and remembers <strong>nothing</strong> about the history before that.</p>
<p>In contrast, SSM and models with linear attention can learn to automatically choose what information retain about the past, which more closely resembles how humans memorize, and provides a smoother curve of forgettance (which is likely beneficial because I think that a blurry remembrance is much better than complete forgettal beyond $n$ tokens). I firmly believe that this is the right direction to go. We are very likely to see a surge of LLMs with $O(1)$ inference cost (for one token) in the upcoming five years, and this can drastically reduce the computational costs and increase their applicability in real-world applications.</p>
<p>Some people may want to refute by saying "but recurrent models are much weaker than transformers". The thing is, most of such comparison are done in settings where the input does not exceed the context window of the transformer models. In other words, we only evaluate transformers on cases where it has perfect memory. In fact, I believe that within the context windows of a transformer model, it should be the upper bounds for the performance of recurrent models, which holds a lossful compression of the window. Moreover, I think that further research down the line can drastically improve the performance of recurrent models (actually any possible linear language models) over self-attention-based language models.</p>
<p>Another thing to note is that, I have noticed that people like to align the parameter count of different LLMs during comparison, but for models with different architecture, this is a bad practice. In practice, we likely care more about the cost of maintenance, the speed of inference or training, and memory usage, etc. For instance, <a href="https://arxiv.org/abs/2307.08621">RetNet</a>'s training throughput is actually faster than a transformer with <a href="https://github.com/Dao-AILab/flash-attention">Flash-Attention</a>. Imagine how fast RetNet + Flash-Attention can be. For applications, if a model is 10x faster than ChatGPT, but just slightly underperforms it, it is very likely that I will choose that over ChatGPT.</p>
<h2 id="Additional-Notes">Additional Notes</h2>
<p>I am currently working on a linear attention model, but the field is changing so fast. I expect that this project will end within the next three months, because if not, my ideas will likely become obselete due to new works being released. Stay tuned.</p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>I know that this is much faster than humans, but we expect AI to be faster than humans, especially considering they cost so much power to run. <a href="#fnref1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn2" class="footnote-item"><p>This perspective is not new and has been discussed in many papers. <a href="#fnref2" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section>
]]></content>
      <categories>
        <category>Research</category>
      </categories>
      <tags>
        <tag>llm</tag>
        <tag>transformer</tag>
        <tag>nlp</tag>
        <tag>research</tag>
        <tag>long-context</tag>
        <tag>benchmark</tag>
        <tag>recurrence</tag>
        <tag>linear-attention</tag>
      </tags>
  </entry>
  <entry>
    <title>Some Binary Search</title>
    <url>/2023/09/14/some_binary_search/</url>
    <content><![CDATA[<p>A binary search with C++:</p>
<figure class="highlight c++"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> T&gt;</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">bin_search</span><span class="params">(vector&lt;T&gt;&amp; arr, T target)</span> </span>{</span><br><span class="line">    <span class="type">int</span> left = <span class="number">0</span>, right = arr.<span class="built_in">size</span>() - <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span> (left &lt;= right) {</span><br><span class="line">        <span class="type">int</span> mid = (left + right) / <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">if</span> (arr[mid] == target) {</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        } <span class="keyword">else</span> <span class="keyword">if</span> (arr[mid] &lt; target) {</span><br><span class="line">            left = mid + <span class="number">1</span>;</span><br><span class="line">        } <span class="keyword">else</span> {</span><br><span class="line">            right = mid - <span class="number">1</span>;</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">return</span> left;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>The same thing with Rust:</p>
<figure class="highlight rust"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">fn</span> <span class="title function_">bin_search</span>&lt;T: <span class="built_in">Ord</span>&gt;(arr: &amp;<span class="type">Vec</span>&lt;T&gt;, target: &amp;T) <span class="punctuation">-&gt;</span> <span class="type">usize</span> {</span><br><span class="line">    <span class="keyword">let</span> <span class="keyword">mut </span><span class="variable">left</span> = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">let</span> <span class="keyword">mut </span><span class="variable">right</span> = arr.<span class="title function_ invoke__">len</span>() - <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span> left &lt;= right {</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">mid</span> = (left + right) / <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">if</span> arr[mid] == *target {</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        } <span class="keyword">else</span> <span class="keyword">if</span> arr[mid] &lt; *target {</span><br><span class="line">            left = mid + <span class="number">1</span>;</span><br><span class="line">        } <span class="keyword">else</span> {</span><br><span class="line">            right = mid - <span class="number">1</span>;</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">    left</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>And with Python:</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">bin_search</span>(<span class="params">arr: <span class="built_in">list</span>, target</span>):</span><br><span class="line">    left = <span class="number">0</span></span><br><span class="line">    right = <span class="built_in">len</span>(arr) - <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> left &lt;= right:</span><br><span class="line">        mid = (left + right) // <span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> arr[mid] == target:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">elif</span> arr[mid] &lt; target:</span><br><span class="line">            left = mid + <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            right = mid - <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> left</span><br></pre></td></tr></tbody></table></figure>]]></content>
      <categories>
        <category>Test</category>
      </categories>
      <tags>
        <tag>english</tag>
        <tag>algorithm</tag>
        <tag>binary-search</tag>
        <tag>rust</tag>
        <tag>python</tag>
        <tag>c++</tag>
        <tag>test</tag>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title>Interpreting a Maze-Solving Network</title>
    <url>/2023/10/07/interpreting-a-maze-solving-network/</url>
    <content><![CDATA[<p><a href="https://www.lesswrong.com/s/sCGfFb5DPfjEmtEdn">The blog post</a></p>
<p>I can't believe I haven't read this until now. This is mind-provoking, and the result is an important step towards understanding neural networks.</p>
<span id="more"></span>
<p>The culmination of this blog post is the exciting work of <a href="/2023/10/07/actadd/">Activation Addition</a>, which I believe is one important work that inspired the recently <a href="https://arxiv.org/abs/2310.01405">Representation Engineering</a> work.</p>
]]></content>
      <categories>
        <category>Thoughts</category>
      </categories>
      <tags>
        <tag>llm</tag>
        <tag>english</tag>
        <tag>representation-engineering</tag>
        <tag>activation-engineering</tag>
        <tag>interpretability</tag>
        <tag>rl</tag>
        <tag>alignment</tag>
        <tag>maze</tag>
      </tags>
  </entry>
  <entry>
    <title>Safety and Ethical Concerns of Large Language Models</title>
    <url>/2023/09/19/llm-safety-and-ethics/</url>
    <content><![CDATA[<p>I will be holding a seminar at ModelBest (面壁智能) in Sep 20, 2023 in Beijing, Haidian, 科技园. The seminar will be in Chinese, and it's called "大模型安全与伦理问题" (translation: Safety and Ethical Concerns of Large Language Models). Below is a list of references.</p>
<span id="more"></span>
<h2 id="Introduction">Introduction</h2>
<ul>
<li>Galactica: A Large Language Model for Science</li>
<li><a href="https://openai.com/research/gpt-4">https://openai.com/research/gpt-4</a></li>
<li>SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions</li>
<li>Bias and Fairness in Large Language Models: A Survey</li>
<li>A Survey of Safety and Trustworthiness of Large Language Models through the Lens of Verification and Validation</li>
</ul>
<h2 id="Evaluation-Methods">Evaluation Methods</h2>
<ul>
<li>A General Language Assistant as a Laboratory for Alignment, Anthropic</li>
<li>Safety Assessment of Chinese Large Language Models</li>
<li>Semantics derived automatically from language corpora contain human-like biases</li>
<li>StereoSet: Measuring stereotypical bias in pretrained language models</li>
</ul>
<h3 id="Instruction-Attacks">Instruction Attacks</h3>
<ul>
<li>Toxicity in CHATGPT: Analyzing Persona-assigned Language Models ⭐️</li>
<li>Large Language Models are Zero-Shot Reasoners ⭐️</li>
<li>On Second Thought, Let’s Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning ⭐️</li>
<li>Prompting GPT-3 To Be Reliable</li>
<li>Universal and Transferable Adversarial Attacks on Aligned Language Models ⭐️</li>
<li>Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment ⭐️⭐️</li>
</ul>
<h3 id="Exaggerated-Safety">Exaggerated Safety</h3>
<ul>
<li>XSTEST: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models ⭐️</li>
<li>Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions ⭐️</li>
</ul>
<h2 id="Alignment-Methods">Alignment Methods</h2>
<ul>
<li>Aligning language models to follow instructions ⭐️</li>
<li>Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback ⭐️</li>
<li>SELF-INSTRUCT: Aligning Language Models with Self-Generated Instructions ⭐️⭐️</li>
<li>Pretraining Language Models with Human Preferences ⭐️</li>
<li>LIMA: Less Is More for Alignment</li>
<li><a href="https://openai.com/blog/our-approach-to-alignment-research">https://openai.com/blog/our-approach-to-alignment-research</a> (Aug 2022)</li>
<li><a href="https://openai.com/blog/our-approach-to-alignment-research">https://openai.com/blog/our-approach-to-alignment-research</a> (Jul 2023) ⭐️</li>
</ul>
<p>⭐️: important</p>
<p>⭐️⭐️: very important</p>
<h2 id="My-Thoughts">My Thoughts</h2>
<p>AI alignment is extremely important, and we know very little about it right now. In my everyday use of ChatGPT, it occasionally refuses to help me. This is presumably because it thinks that assisting me is harmful, while it's actually not. This is a problem of "exaggerated safety", and it is very similar to the overgeneralization model editing, which is a problem I have worked on previous (see my publication, <a href="../../../../2023/09/14/EREN/">EREN</a>). I think using classifier on top (a simple safe guard), along with prompting methods<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup> and currect alignment methods is a viable solution (seem to work fairly well in ChatGPT), but as we can see from the <a href="https://www.anthropic.com/index/claude-2">technical report of Claude 2</a>, the helpfulness of the model significantly drops after alignment. Therefore, I think minimizing the sacrifice in helpfulness will be an important direction of future research.</p>
<p>Another concern is that there is no concensus on the goal of alignment. In fact, many people think that the fact that role-playing can be used to jailbreak alignment is not a bad thing per se, especially regarding toxicity, because if the user explicitly tells the AI to role-play a person that slurs a lot, the user expects slurs (one kind of toxicity).</p>
<p>Moreover, the entire meaning of alignment research might be undermined by the fact that AI system can be unaligned pretty easily. This concern is specially severe for works that focus on reducing the cost of alignment, because the same techniques might be used to effectively unalign AI systems.<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup></p>
<p>All in all, AI alignment is a sub-field of better controllability of AI system, and I can foresee that it will be a hot research topic for the upcoming five years.</p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>Basically prepending an prefix that tells it what is unethical and unsafe. <a href="#fnref1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn2" class="footnote-item"><p>Does there exist a way to make AI impossible to unalign? This reminds me of the "mind stamping" (Chinese: 思想钢印) from the Three Body Problem, a novel by Liu Cixin. <a href="#fnref2" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section>
]]></content>
      <categories>
        <category>Thoughts</category>
      </categories>
      <tags>
        <tag>llm</tag>
        <tag>research</tag>
        <tag>english</tag>
        <tag>machine-learning</tag>
        <tag>life</tag>
        <tag>ai-alignment</tag>
        <tag>eren</tag>
        <tag>ethics</tag>
        <tag>safety</tag>
        <tag>面壁智能</tag>
        <tag>modelbest</tag>
        <tag>tutorial</tag>
        <tag>chatgpt</tag>
        <tag>claude</tag>
        <tag>三体</tag>
      </tags>
  </entry>
  <entry>
    <title>临近2023暑假，00师姐答辩，晚上打球</title>
    <url>/2023/05/18/%E4%B8%B4%E8%BF%912023%E6%9A%91%E5%81%87/</url>
    <content><![CDATA[<p>今天睡到九点才醒来，还是被00打电话叫醒的。去过了个早，然后去上《深度学习》。</p>
<p>今天00的师姐答辩，下午三点去了，当时我在睡午觉。感觉之后她有点emo，但是她不承认，不知道为什么。然后聊了很多关于未来，结婚、生孩子、找工作等事情。感觉也没有很大的问题，但是00总是把东西看得很灰暗，很焦虑。</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>00</tag>
        <tag>life</tag>
        <tag>中文</tag>
        <tag>school</tag>
        <tag>graduation</tag>
      </tags>
  </entry>
  <entry>
    <title>更新个人主页</title>
    <url>/2023/09/16/%E6%9B%B4%E6%96%B0%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5/</url>
    <content><![CDATA[<p>之前有过个人主页，但是一直没有弄好，更没有更新。最近我将自己的 GitHub 的用户名改了，导致之前的 GitHub Pages 失效了，就趁机重新搭建个人主页。</p>
<p>兜兜转转，还是决定使用 Hexo。以前用过 Jekyll，觉得还行，但是真的不想用 Ruby，Hugo 又太麻烦。</p>
<span id="more"></span>
<p>选了好久主题，Hexo 宣传说有很多主题，但是官网上不到 400 个主题，而且大部分都不符合我的审美或者要求。我想要的风格是简约，现代，需要同时支持黑暗和白亮模式，需要有代码高亮且是代码是等款字体。最接近我的要求就是<a href="https://www.github.com/xbmlz/hexo-theme-maple">Maple</a>主题。可是仍然无法满足我的要求，所以我修改了一些格式（原版甚至有一些颜色 bug），添加了自己的一些内容，结果是一个叫做<a href="https://www.github.com/chen-yingfa/hexo-theme-fengye">枫叶</a>的主题。</p>
<h2 id="日记">日记</h2>
<p>今天早上七点半起来 🛏，打电话 📱 叫醒00（终于有一次是我打电话了哈哈哈哈），然后去核研院俱乐部在综体打羽毛球 🏸，后来发现他们其实约了西体，但是我跟00自己在蹭一个空场就不管了，八点半左右有人来了我们就去过早，然后去我宿舍 🏡。</p>
<p>之后点了库迪，然后去了学校南边的一个超市，买了一大包薯片和一个榴莲！然后就在宿舍没有吃午饭，直接待到晚饭。中午的时候还拍了视频 📷，中间还差点说到00emo了，哈哈哈哈。</p>
<p>今天 00 下午四点和晚上七点都有直播课 👩🏻‍🏫，都是真正开课，下午的在我宿舍开的，好像很成功，虽然拖堂了一点点。晚上的在她自己宿舍，貌似也拖堂了，00 说有好多人。</p>
<p>晚上九点去打羽毛球了 🏸，带上相机录了打球的视频，然后回去洗澡，晚上去林大北路的家 🏡。</p>
<h2 id="最近">最近</h2>
<p>最近好忙，新学期马上就要开始了，这里总结一下暑假开始到此比较重要的事情吧。</p>
<p>这个暑假搬出校又搬回来了，折腾了又费钱 💰，学校真的好恶心，之前说了大概率是不会有宿舍，现在就有很多空的房间。</p>
<p>期末前跟导师确定了要读博了，我跟他我想要三年毕业，他说没有问题，希望真的是可以吧，我们实验室好像基本都是直博生，普博的应该都是四年吧。00 也确定了不会读博了，最近在投简历，Oppo 好像已经拿到了 offer，但是他们北京没有部门，所以 00 不想去，我也不想她去。好像互联网以外很多公司都不在北京……</p>
<p>我的论文 📃 EREN（以前叫做 EmoRen）投出去了，上周 rebuttal 结果出来了，不是很理想，本来 soundess 是 433，Excitement 323，rebuttal 结束后第一个审稿人将 soundness 调低了。学长说主会议估计没有机会了，Findings 还有希望，我其实无所谓是不是 Findings，感觉学长反而有点介意。</p>
<p>被实验室的学长学姐拉去面壁智能<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>去干活，跟公司的业务没啥关系，就是把我的工位搬了，可能不想占用隔壁实验室的位置吧 😂 但是我真的不想去 😭 不能跟 00 待在一起了。我现在就是一周可能去两三天 😂</p>
<p>最近还申请了签证，决定了寒假 00 跟我一起回家！在家待一整个月，好神奇，觉得我们的关系发展得好顺利。马上的国庆 🇨🇳 我会跟 00 回去武汉和应城参加她高中同学和表姐的婚礼 💑，顺便还会看她的外公外婆。</p>
<p><img src="/%E6%9B%B4%E6%96%B0%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5/%E4%B8%AD%E5%9B%BD%E6%8A%A4%E7%85%A7.jpg" alt=""></p>
<blockquote>
<p>00 的护照，是我向往的身份！</p>
</blockquote>
<p><img src="/%E6%9B%B4%E6%96%B0%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5/%E7%94%B3%E8%AF%B7%E7%AD%BE%E8%AF%81.jpg" alt=""></p>
<blockquote>
<p>签证中心门口，不是大使馆，很多个国家统一办理签证的地方。</p>
</blockquote>
<p><img src="/%E6%9B%B4%E6%96%B0%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5/%E5%90%83%E6%BE%B3%E9%97%A8%E8%8F%9C.jpg" alt=""></p>
<blockquote>
<p>在申请签证的地方旁边的一个很高级的商场里面吃澳门餐。</p>
</blockquote>
<hr>
<h2 id="一些-Markdown-渲染器测试">一些 Markdown 渲染器测试</h2>
<p>测试一下公式的渲染。<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup></p>
<p>$$
\theta_i \leftarrow  \frac{\partial}{\partial  \theta_i} \mathcal L( y, f(x; \theta))
$$</p>
<p>其中 $\mathcal L$ 是损失函数，$f(\cdot; \theta)$ 由 $\theta$ 参数化的模型。</p>
<p>那 <code>代码</code> 呢？<code>codell1i0oO</code> 可以吗？</p>
<p>一个表格：</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>0.1</td>
</tr>
<tr>
<td>B</td>
<td>0.2</td>
</tr>
<tr>
<td>C</td>
<td>0.3</td>
</tr>
</tbody>
</table>
<p>一个嵌套列表：</p>
<ul>
<li>
<p>a</p>
<ul>
<li>
<p>x</p>
</li>
<li>
<p>asdf</p>
<ul>
<li>懂得都懂</li>
</ul>
</li>
<li>
<p>好好</p>
<ol>
<li>
<p>item 1</p>
</li>
<li>
<p>item 2</p>
<ol>
<li>sdf</li>
<li>sdf</li>
</ol>
</li>
<li>
<p>item 3</p>
</li>
</ol>
</li>
</ul>
</li>
<li>
<p>asdf</p>
</li>
</ul>
<p>一个代办列表：</p>
<ul class="contains-task-list">
<li class="task-list-item"><input class="task-list-item-checkbox" disabled="" type="checkbox"> 买菜</li>
<li class="task-list-item"><input class="task-list-item-checkbox" disabled="" type="checkbox"> 做饭</li>
<li class="task-list-item"><input class="task-list-item-checkbox" checked="" disabled="" type="checkbox"> 跑步</li>
<li class="task-list-item"><input class="task-list-item-checkbox" checked="" disabled="" type="checkbox"> 打羽毛球</li>
<li class="task-list-item"><input class="task-list-item-checkbox" checked="" disabled="" type="checkbox"> 写论文</li>
<li class="task-list-item"><input class="task-list-item-checkbox" disabled="" type="checkbox"> 搞科研</li>
</ul>
<p>auto-links: <a href="http://www.hexo.io">www.hexo.io</a></p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>我导师和知乎孵化的的公司 <a href="#fnref1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn2" class="footnote-item"><p>我现在用了 <a href="https://github.com/hexojs/hexo-renderer-markdown-it">hexo-rendered-markdown-it</a> 来代替原来 Hexo 的渲染器。原来的渲染器是 Marked (hexo-renderer-marked) <a href="#fnref2" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section>
]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>research</tag>
        <tag>00</tag>
        <tag>life</tag>
        <tag>中文</tag>
        <tag>面壁智能</tag>
        <tag>blog</tag>
        <tag>签证</tag>
        <tag>hexo</tag>
        <tag>hugo</tag>
        <tag>jekyll</tag>
        <tag>static-site-generator</tag>
        <tag>羽毛球</tag>
        <tag>work</tag>
        <tag>枫叶</tag>
        <tag>中国</tag>
        <tag>挪威</tag>
        <tag>markdown</tag>
        <tag>宿舍</tag>
      </tags>
  </entry>
  <entry>
    <title>第一个帖子，瞎写点东西</title>
    <url>/2023/05/17/%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%B8%96%E5%AD%90%EF%BC%8C%E7%9E%8E%E5%86%99%E7%82%B9%E4%B8%9C%E8%A5%BF/</url>
    <content><![CDATA[<p>现在是 2023 年五月十七，马上硕士一年级就结束，在清华园已经快五年了，感觉对我人生的影响真的巨大。这一年认识了很可爱的 00，希望可以一直走下去。</p>
<p>我和 00 的孩子们：</p>
<span id="more"></span>
<ul>
<li>卧龙：调皮的肥猫 🐱</li>
<li>小绿：喜欢咬东西的鳄鱼 🐊</li>
<li>骆雁：超级大的土鸡！🐰</li>
<li>凤雏：不调皮的猫咪 🐱</li>
<li>黄帝：更大的巨兔 🐰</li>
<li>内存条：白色的熊熊 🐻</li>
<li>闪光灯：灰色的熊熊 🐻</li>
</ul>
<h2 id="现在要做的事情">现在要做的事情</h2>
<ul>
<li>
<p>把 EmoRen 投了</p>
<ul>
<li>能不能行啊</li>
</ul>
</li>
<li>
<p>跑 CFD 的丹炉调好</p>
<ul>
<li>好难呀</li>
</ul>
</li>
<li>
<p>写完作业</p>
<ul>
<li>NLP和DL的大作业！</li>
</ul>
</li>
<li>
<p>搞定去ACL的手续</p>
<ul>
<li>去加拿大，然后回挪威一两周，然后回来跟 00 去南京，我不用签证，但是还是有很多手续。</li>
</ul>
</li>
<li>
<p>写好开题报告</p>
<ul>
<li>还不知道做啥呢</li>
</ul>
</li>
</ul>
<h2 id="我的家乡-Lillesand">我的家乡 Lillesand</h2>
<p><img src="/2023/05/17/%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%B8%96%E5%AD%90%EF%BC%8C%E7%9E%8E%E5%86%99%E7%82%B9%E4%B8%9C%E8%A5%BF/lillesand0.jpg" alt=""></p>
<p><img src="/2023/05/17/%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%B8%96%E5%AD%90%EF%BC%8C%E7%9E%8E%E5%86%99%E7%82%B9%E4%B8%9C%E8%A5%BF/lillesand1.jpg" alt=""></p>
<p>好久没有回去了，上一次回挪威也没有回去</p>
]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>research</tag>
        <tag>cfd</tag>
        <tag>00</tag>
        <tag>life</tag>
        <tag>中文</tag>
        <tag>school</tag>
        <tag>孩子们</tag>
        <tag>卧龙</tag>
        <tag>凤雏</tag>
        <tag>骆雁</tag>
        <tag>黄帝</tag>
        <tag>小绿</tag>
        <tag>猫咪</tag>
        <tag>土鸡</tag>
        <tag>熊</tag>
        <tag>🐻</tag>
        <tag>🐱</tag>
        <tag>🐇</tag>
        <tag>🐰</tag>
        <tag>🐊</tag>
        <tag>emoren</tag>
        <tag>acl</tag>
        <tag>lillesand</tag>
        <tag>加拿大</tag>
      </tags>
  </entry>
  <entry>
    <title>第一篇</title>
    <url>/2022/10/27/%E7%AC%AC%E4%B8%80%E7%AF%87/</url>
    <content><![CDATA[<p>之前尝试用 Hugo 来部署，发现 Hugo 不仅挺复杂，而且还有很多小问题，可能这就是速度带来的代价吧。但是其实我也不是写很多内容，所以 Jekyll 的速度应该是够用的。</p>
<p>Jekyll 支持在 markdown 内容里面用 Liquid template tags 来生成动态内容，比如根据 front matter 中的 tags，给每个 tag 生成 html div。如下 liquid 语法：</p>

<figure class="highlight html"><table><tbody><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span>&gt;</span></span><br><span class="line">{% for tag in site.tags %}</span><br><span class="line">    <span class="tag">&lt;<span class="name">a</span> <span class="attr">style</span>=<span class="string">"background-color: blue;"</span>&gt;</span>#{{ tag[0] }}<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">{% endfor %}</span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></tbody></table></figure>

<span id="more"></span>
<p>会根据 post markdown 文件中的 front matter 中定义的 tags：</p>
<figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line">---</span><br><span class="line"><span class="section">tags: some tags here</span></span><br><span class="line"><span class="section">---</span></span><br></pre></td></tr></tbody></table></figure>
<p>生成相应的 html div：</p>
<figure class="highlight html"><table><tbody><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"post-tags"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"/tags/life"</span> <span class="attr">class</span>=<span class="string">"tag-card"</span>&gt;</span>life<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"/tags/update"</span> <span class="attr">class</span>=<span class="string">"tag-card"</span>&gt;</span>update<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"/tags/learn"</span> <span class="attr">class</span>=<span class="string">"tag-card"</span>&gt;</span>learn<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"/tags/important"</span> <span class="attr">class</span>=<span class="string">"tag-card"</span>&gt;</span>important<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"/tags/jekyll"</span> <span class="attr">class</span>=<span class="string">"tag-card"</span>&gt;</span>jekyll<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"/tags/hugo"</span> <span class="attr">class</span>=<span class="string">"tag-card"</span>&gt;</span>hugo<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"/tags/static-site-generator"</span> <span class="attr">class</span>=<span class="string">"tag-card"</span>&gt;</span>static-site-generator<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></tbody></table></figure>
<hr>
<p>不知道写啥，就写一个 Python 的二分搜索吧：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">bin_search</span>(<span class="params">arr: <span class="built_in">list</span>, target: <span class="type">Any</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Return the smallest index such that when target is inserted at that index,</span></span><br><span class="line"><span class="string">    the array will remain sorted.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    lo, hi = <span class="number">0</span>, <span class="built_in">len</span>(arr)</span><br><span class="line">    <span class="keyword">while</span> lo &lt; hi:</span><br><span class="line">        m = (lo + hi) // <span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> arr[m] &lt; target:</span><br><span class="line">            lo = m + <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            hi = m</span><br><span class="line">    <span class="keyword">return</span> lo</span><br></pre></td></tr></tbody></table></figure>]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>life</tag>
        <tag>中文</tag>
        <tag>test</tag>
        <tag>hugo</tag>
        <tag>jekyll</tag>
        <tag>static-site-generator</tag>
        <tag>html</tag>
        <tag>liquid</tag>
      </tags>
  </entry>
</search>
